{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "MODULE_PATH = ipynbname.path().parent.parent.parent.parent\n",
    "sys.path.append(str(MODULE_PATH))\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# This should always be `./src`\n",
    "print(f\"Current working directory [{os.getcwd()}]\")\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# freqtrade messes with stdout, so we cache the original object here\n",
    "stdout = sys.stdout\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# Setup utils\n",
    "#\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "from utils import config\n",
    "\n",
    "LOGGER = LoggerUtil(config.MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "\n",
    "from freqtrade.configuration import Configuration\n",
    "\n",
    "from utils import config\n",
    "\n",
    "# First prime the user_data_dir key. This will take priority when merged with config.json\n",
    "def load_ft_config():\n",
    "    freqtrade_config = Configuration({\"user_data_dir\": config.FREQTRADE_USER_DATA_DIR})\n",
    "    freqtrade_config = freqtrade_config.load_from_files([str(config.FREQTRADE_USER_DATA_DIR / \"config.json\")])\n",
    "    freqtrade_config[\"user_data_dir\"] = config.FREQTRADE_USER_DATA_DIR\n",
    "    freqtrade_config[\"datadir\"] = config.FREQTRADE_USER_DATA_DIR / \"data\" / \"binance\"\n",
    "    return freqtrade_config\n",
    "\n",
    "freqtrade_config = load_ft_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load data based on the freqtrade config\n",
    "#\n",
    "\n",
    "from freqtrade.data.history import load_pair_history\n",
    "\n",
    "candles = load_pair_history(\n",
    "    datadir=freqtrade_config[\"datadir\"],\n",
    "    timeframe=freqtrade_config[\"timeframe\"],\n",
    "    pair=config.CRYPTO_PAIR)\n",
    "\n",
    "LOGGER.log(f\"Loaded {str(len(candles))} rows of data for {config.CRYPTO_PAIR} from {freqtrade_config['datadir']}\")\n",
    "\n",
    "# candles_df = candles.tail(550).copy()\n",
    "candles_df = candles.copy()\n",
    "LOGGER.log(candles_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# You can use this cell for discovery and experimentation\n",
    "#\n",
    "\n",
    "import data_processing.marshal_features as mf\n",
    "df = mf.marshal_candle_metadata(candles_df, drop_date_column=True)\n",
    "# print(df[\"log_return_close\"].describe())\n",
    "# print(df[\"log_return_close_2\"].describe())\n",
    "LOGGER.log(df[\"close\"].head(10))\n",
    "# print(df[\"log_return_close\"].head(10))\n",
    "# print(df[\"log_return_close_2\"].head(10))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # df_plot = df.drop(['sell', 'buy', \\\"volume\\\", \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"hma\\\"], axis=1)\n",
    "# df_plot2 = df[[\"log_return_close\"]].tail(60).copy()\n",
    "# ax = df_plot2.plot(figsize=(12, 5))\n",
    "# plt.grid()\n",
    "# plt.legend(df_plot2.columns)\n",
    "# plt.show()\n",
    "\n",
    "# ax = df.plot(kind='scatter', x=\"volume\", y='close', figsize=(15, 15))\n",
    "# [plt.axvline(x) for x in mf.NATURAL_VOLUME_BREAKS]\n",
    "\n",
    "# # This takes some time to generate\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# scatter_matrix(df_plot, alpha=0.2, figsize=(12, 12), diagonal='kde')\n",
    "\n",
    "# df_plot[-499:].plot(figsize=(12, 5), linewidth=2)\n",
    "# plt.grid()\n",
    "# plt.legend(df_plot.columns)\n",
    "# plt.show()\n",
    "\n",
    "# ax = df_plot.plot(kind='scatter', x='volume_bin', y='pattern_detected', figsize=(15, 15))\n",
    "# ax = df_plot.plot(kind='scatter', x='pattern_count', y='pattern_detected', figsize=(15, 15))\n",
    "# ax = df_plot.plot(kind='scatter', x='volume', y='pattern_detected', figsize=(15, 1))\n",
    "\n",
    "\n",
    "# for (columnName, columnData) in df.iteritems():\n",
    "#     print(columnName)\n",
    "#     print(columnData.isnull().sum())\n",
    "\n",
    "# rows_with_nan = [index for index, row in df.iterrows() if row.isnull().any()]\n",
    "# print(rows_with_nan)\n",
    "\n",
    "# import numpy as np\n",
    "# nan_indices = np.where(df['volume_bin'].isnull())[0]\n",
    "# print(nan_indices)\n",
    "#\n",
    "# for idx in nan_indices:\n",
    "#     print(df.iloc[idx]['volume'])\n",
    "#     print(df.iloc[idx]['volume_bin'])\n",
    "#     print(\"-------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load strategy and generate indicators and predictions\n",
    "#\n",
    "\n",
    "from freqtrade.resolvers import StrategyResolver\n",
    "\n",
    "strategy = StrategyResolver.load_strategy(freqtrade_config)\n",
    "\n",
    "strategy_metadata = {\n",
    "    \"pair\": config.CRYPTO_PAIR,\n",
    "\n",
    "    # Useful to turn off when df was already generated in this cell with the candle metadata\n",
    "    \"marshal_candle_metadata\": False,\n",
    "\n",
    "    # Useful to turn off when doing feature engineering before needing to predict\n",
    "    \"run_inference\": True,\n",
    "\n",
    "    # Useful to turn on to retrieve a CSV cache of the predicted results\n",
    "    \"return_cached_dataframe\": True,\n",
    "}\n",
    "\n",
    "# Generate buy/sell signals using strategy\n",
    "if strategy_metadata.get('run_inference', False):\n",
    "    df = cached_df.copy()\n",
    "    df = df.drop(['sell', 'buy'], axis=1, errors='ignore')\n",
    "\n",
    "    # Feel free to uncomment/comment this line out or even change the range. It is meant to reduce how many predictions\n",
    "    # are made since they are computationally expensive\n",
    "    df = df[\"2021-05-15 00:00:00\":\"2021-05-31 00:00:00\"]\n",
    "\n",
    "    df = strategy.analyze_ticker(df, strategy_metadata)\n",
    "else:\n",
    "    cached_df = strategy.analyze_ticker(candles_df, strategy_metadata)\n",
    "    df = cached_df.copy()\n",
    "\n",
    "LOGGER.log(df[config.FREQTRADE_MAX_CONTEXT:config.FREQTRADE_MAX_CONTEXT + 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# Reload and rerun strategy to later back test on generated indicators and predictions\n",
    "#\n",
    "\n",
    "# Reload Config\n",
    "freqtrade_config = load_ft_config()\n",
    "\n",
    "# Reload strategy\n",
    "strategy = StrategyResolver.load_strategy(freqtrade_config)\n",
    "\n",
    "# Rerun buy and sell generation\n",
    "df = df.drop(['sell', 'buy'], axis=1, errors='ignore')\n",
    "# df.to_csv(\"../out/pred_cache.csv\")\n",
    "df = strategy.advise_buy(df, strategy_metadata)\n",
    "df = strategy.advise_sell(df, strategy_metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Back test the strategy based on the indicators generated above\n",
    "#\n",
    "\n",
    "# iPython, which is used in jupyter notebooks, are not compatible with the async event loops of freqtrade.\n",
    "# This is a workaround for that issue\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import freqtrade.optimize.backtesting as ft_backtest\n",
    "from freqtrade.data import history\n",
    "\n",
    "LOGGER.log(f\"Generated {df['buy'].sum()} buy signals\")\n",
    "LOGGER.log(f\"Generated {df['sell'].sum()} sell signals\")\n",
    "\n",
    "backtest_start_time = datetime.now(timezone.utc)\n",
    "backtesting = ft_backtest.Backtesting(freqtrade_config)\n",
    "\n",
    "backtesting._set_strategy(strategy)\n",
    "strategy_name = backtesting.strategy.get_strategy_name()\n",
    "\n",
    "preprocessed_data = {config.CRYPTO_PAIR: df}\n",
    "min_date, max_date = history.get_timerange(preprocessed_data)\n",
    "\n",
    "results = backtesting.backtest(\n",
    "    processed=preprocessed_data,\n",
    "    start_date=min_date,\n",
    "    end_date=max_date,\n",
    "    max_open_trades=freqtrade_config['max_open_trades'],\n",
    "    position_stacking=backtesting.config.get('position_stacking', False),\n",
    "    enable_protections=backtesting.config.get('enable_protections', False),\n",
    ")\n",
    "backtest_end_time = datetime.now(timezone.utc)\n",
    "\n",
    "results.update({\n",
    "    'backtest_start_time': int(backtest_start_time.timestamp()),\n",
    "    'backtest_end_time': int(backtest_end_time.timestamp()),\n",
    "})\n",
    "backtesting.all_results[strategy_name] = results\n",
    "\n",
    "stats = ft_backtest.generate_backtest_stats(preprocessed_data, backtesting.all_results, min_date=min_date, max_date=max_date)\n",
    "ft_backtest.show_backtest_results(backtesting.config, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load backtested trades as dataframe\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replicate relevant parts of freqtrade.data.btanalysis.load_backtest_data\n",
    "def load_backtest_data():\n",
    "    data = stats['strategy'][strategy_name]['trades']\n",
    "    trades_df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        trades_df['open_date'] = pd.to_datetime(\n",
    "            trades_df['open_date'],\n",
    "            utc=True,\n",
    "            infer_datetime_format=True\n",
    "        )\n",
    "        trades_df['close_date'] = pd.to_datetime(\n",
    "            trades_df['close_date'],\n",
    "            utc=True,\n",
    "            infer_datetime_format=True\n",
    "        )\n",
    "\n",
    "        if 'profit_ratio' not in trades_df.columns:\n",
    "            trades_df['profit_ratio'] = trades_df['profit_percent']\n",
    "        trades_df = trades_df.sort_values(\"open_date\").reset_index(drop=True)\n",
    "    return trades_df\n",
    "\n",
    "trades = load_backtest_data()\n",
    "\n",
    "# Show value-counts per pair\n",
    "trades.groupby(\"pair\")[\"sell_reason\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freqtrade.data.btanalysis import analyze_trade_parallelism\n",
    "\n",
    "# Analyze the above\n",
    "parallel_trades = analyze_trade_parallelism(trades, '5m')\n",
    "parallel_trades.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from freqtrade.plot.plotting import generate_candlestick_graph\n",
    "\n",
    "# Filter trades to one pair\n",
    "trades_red = trades.loc[trades['pair'] == config.CRYPTO_PAIR]\n",
    "\n",
    "# Generate candlestick graph\n",
    "graph = generate_candlestick_graph(\n",
    "    pair=config.CRYPTO_PAIR,\n",
    "    data=df,\n",
    "    trades=trades_red,\n",
    "    indicators1=['sma20', 'ema50', 'ema55'],\n",
    "    indicators2=['rsi', 'macd', 'macdsignal', 'macdhist']\n",
    ")\n",
    "\n",
    "graph.show()\n",
    "# graph.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# Hyperopt the stratergy\n",
    "#\n",
    "\n",
    "# Restore stdout each time this runs\n",
    "sys.stdout = stdout\n",
    "\n",
    "import freqtrade.optimize.hyperopt as ft_hyperopt\n",
    "\n",
    "# freqtrade_config[\"hyperopt_loss\"] = \"ShortTradeDurHyperOptLoss\"\n",
    "freqtrade_config[\"hyperopt_loss\"] = \"OnlyProfitHyperOptLoss\"\n",
    "# freqtrade_config[\"hyperopt_loss\"] = \"SharpeHyperOptLoss\"\n",
    "# freqtrade_config[\"hyperopt_loss\"] = \"SortinoHyperOptLoss\"\n",
    "freqtrade_config[\"spaces\"] = \"all\"\n",
    "\n",
    "# freqtrade_config[\"timerange\"] = \"20210515-20210518\"\n",
    "freqtrade_config[\"timerange\"] = \"20210515-20210522\"\n",
    "\n",
    "# 20210515, from 0000 to 1000\n",
    "# freqtrade_config[\"timerange\"] = \"1621036800-1621072800\"\n",
    "\n",
    "freqtrade_config[\"print_all\"] = True\n",
    "freqtrade_config[\"print_colorized\"] = False\n",
    "freqtrade_config[\"print_json\"] = False\n",
    "freqtrade_config[\"epochs\"] = 500\n",
    "freqtrade_config[\"hyperopt_min_trades\"] = 1\n",
    "\n",
    "LOGGER.log(\"Starting freqtrade hyperopt\")\n",
    "# Install the necessary dependencies first with `pip install -r src/freqtrade/requirements-hyperopt.txt`\n",
    "hyperopt = ft_hyperopt.Hyperopt(freqtrade_config)\n",
    "hyperopt.start()\n",
    "\n",
    "#{\"params\":{\"buy_pred_close_diff_1\":\"1\",\"sell_pred_close_diff_1\":\"-3\"},\"minimal_roi\":{\"0\":0.041999999999999996,\"2\":0.015,\"14\":0.005,\"32\":0},\"stoploss\":-0.022,\"trailing_stop\":\"True\",\"trailing_stop_positive\":0.261,\"trailing_stop_positive_offset\":0.303,\"trailing_only_offset_is_reached\":\"False\"}\n",
    "\n",
    "sys.stdout = stdout\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Capia (venv)",
   "language": "python",
   "name": "capia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}