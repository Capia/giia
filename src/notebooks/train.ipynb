{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "MODULE_PATH = ipynbname.path().parent.parent\n",
    "sys.path.append(str(MODULE_PATH))\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# This should always be `./src`\n",
    "print(f\"Current working directory [{os.getcwd()}]\")\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup utils\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "from utils import config\n",
    "\n",
    "LOGGER = LoggerUtil(config.MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()\n",
    "\n",
    "AWS_INSTANCE = 'ml.m5.large'\n",
    "LOCAL_INSTANCE = 'local'\n",
    "try:\n",
    "    if subprocess.call('nvidia-smi') == 0:\n",
    "        LOCAL_INSTANCE = 'local_gpu'\n",
    "except:\n",
    "    print(\"The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU \"\n",
    "          \"computation\")\n",
    "\n",
    "# Change this to your desired instance type\n",
    "INSTANCE_TYPE = LOCAL_INSTANCE\n",
    "IS_LOCAL = LOCAL_INSTANCE == INSTANCE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Parse dataset\n",
    "#\n",
    "\n",
    "from data_processing.parse import Parse\n",
    "\n",
    "PARSE = Parse(LOGGER)\n",
    "\n",
    "dataset_dir_path = local_artifact_dir / \"datasets\"\n",
    "\n",
    "# Creates train and test dataset CSVs\n",
    "PARSE.split_train_test_dataset(dataset_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup local/aws environment. If aws, upload the datasets to S3\n",
    "#\n",
    "\n",
    "from data_processing.upload import Upload\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "UPLOAD = Upload(LOGGER, config.MODEL_ID)\n",
    "\n",
    "sagemaker_session = None\n",
    "\n",
    "if IS_LOCAL:\n",
    "    LOGGER.log(\"Notebook is set to local mode, not uploading to S3\")\n",
    "    model_output_dir_path = local_artifact_dir / \"models\"\n",
    "    model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dataset_dir_uri = f\"file://{dataset_dir_path}\"\n",
    "    model_output_dir_uri = f\"file://{model_output_dir_path}\"\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {\n",
    "        'local': {\n",
    "            'local_code': True,\n",
    "            'container_root': str(model_output_dir_path)\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    sagemaker_session = UPLOAD.sagemaker_session\n",
    "\n",
    "    UPLOAD.upload_to_sagemaker_s3_bucket(dataset_dir_path, PARSE.TRAIN_DATASET_FILENAME)\n",
    "    UPLOAD.upload_to_sagemaker_s3_bucket(dataset_dir_path, PARSE.TEST_DATASET_FILENAME)\n",
    "    dataset_dir_uri = UPLOAD.s3_dataset_dir_uri\n",
    "\n",
    "    model_output_dir_uri = UPLOAD.s3_model_output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Configure sagemaker and estimator\n",
    "#\n",
    "\n",
    "from ml.train import Train\n",
    "\n",
    "TRAIN = Train(LOGGER)\n",
    "\n",
    "estimator = TRAIN.create_model(config.SM_ROLE, INSTANCE_TYPE, sagemaker_session)\n",
    "TRAIN.fit_model(estimator, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# Load model and define test data and variables to visually evaluate the model\n",
    "#\n",
    "\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "#TODO: Use deepar.model_fn?\n",
    "# model_output_dir_path is basically the same path as it was before, though sagemaker appends a random temp directory\n",
    "#  to the path. The path from TRAIN includes that random temp directory\n",
    "model_output_dir_path = TRAIN.model_dir_path / \"model\"\n",
    "predictor = Predictor.deserialize(Path(model_output_dir_path))\n",
    "\n",
    "test_dataset_filename = dataset_dir_path / config.TEST_DATASET_FILENAME\n",
    "df = pd.read_csv(filepath_or_buffer=test_dataset_filename, header=0, index_col=0)\n",
    "\n",
    "test_data = ListDataset(\n",
    "    [\n",
    "        # This is first so we can easily graph the entire test dataset below\n",
    "        {\"start\": df.index[0], \"target\": df[\"close\"][:]},\n",
    "        {\"start\": df.index[0], \"target\": df[\"close\"][:\"2020-11-20 12:20:00\"]},\n",
    "        {\"start\": df.index[0], \"target\": df[\"close\"][:\"2021-01-20 18:50:00\"]}\n",
    "    ],\n",
    "    freq=\"5min\"\n",
    ")\n",
    "\n",
    "to_pandas(next(iter(test_data)))[:200].plot(figsize=(12, 5), linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend([\"close\"])\n",
    "plt.show()\n",
    "\n",
    "def plot_prob_forecasts(ts_list, forecast_list, plot_length=100):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecasts)):\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "        ax = target[-plot_length:].plot(figsize=(10, 7), linewidth=2)\n",
    "        forecast.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "        plt.grid(which='both')\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Visually evaluate the model by graphing some prediction test results\n",
    "#\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_data,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "forecast_entry = forecasts[0]\n",
    "tss = list(ts_it)\n",
    "\n",
    "print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "print(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "\n",
    "plot_prob_forecasts(tss, forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: FURTHER CELLS ARE COMPATIBLE WITH AWS SAGEMAKER ONLY, LOCAL MODE WILL NOT WORK\n",
    "# Hyperparameter tune the model\n",
    "#\n",
    "\n",
    "from ml.tune import Tune\n",
    "\n",
    "TUNE = Tune(LOGGER)\n",
    "\n",
    "train_dataset_uri = f\"{dataset_dir_uri}/{PARSE.TRAIN_DATASET_FILENAME}\"\n",
    "test_dataset_uri = f\"{dataset_dir_uri}/{PARSE.TEST_DATASET_FILENAME}\"\n",
    "\n",
    "tuner = TUNE.create_tuner(estimator)\n",
    "TUNE.fit_tuner(tuner, train_dataset_uri, test_dataset_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get updates for Hyperparameter tune job. Ensure this is completed before going to the next cell\n",
    "#\n",
    "\n",
    "TUNE.get_tune_job_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the metrics of the tune job\n",
    "#\n",
    "\n",
    "TUNE.report_job_analytics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "capia",
   "language": "python",
   "display_name": "Capia (venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}