{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory [/Users/jbeckman/projects/capia/src]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "MODULE_PATH = ipynbname.path().parent.parent\n",
    "sys.path.append(str(MODULE_PATH))\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# This should always be `./src`\n",
    "print(f\"Current working directory [{os.getcwd()}]\")\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11 14:15:09.910933 Background logger started\n",
      "2021-05-11 14:15:09.911395 The model id is [giia-0.6.2]\n",
      "2021-05-11 14:15:09.911488 The MXNet version is [1.7.0]\n",
      "2021-05-11 14:15:09.911653 The GluonTS version is [0.7.1.dev3+gecb903f]\n",
      "2021-05-11 14:15:09.911896 The GPU count is [0]\n",
      "2021-05-11 14:15:09.917194 The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU computation\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup utils\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "from utils import config\n",
    "\n",
    "LOGGER = LoggerUtil(config.MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()\n",
    "\n",
    "# AWS instance specs can be found here https://aws.amazon.com/sagemaker/pricing/\n",
    "AWS_INSTANCE = 'ml.m5.large' # 2 vCPU, 0 GPU, 8 GB memory, $0.134/hour\n",
    "AWS_INSTANCE_2 = 'ml.m5.4xlarge' # 8 vCPU, 0 GPU, 32 GB memory, $0.538/hour\n",
    "AWS_GPU_INSTANCE = 'ml.g4dn.xlarge' # 4 vCPU, 1 GPU, 16 GB memory, $0.736/hour\n",
    "AWS_GPU_INSTANCE_2 = 'ml.g4dn.2xlarge' # 8 vCPU, 1 GPU, 32 GB memory, $1.053/hour\n",
    "LOCAL_INSTANCE = 'local'\n",
    "try:\n",
    "    if subprocess.call('nvidia-smi') == 0:\n",
    "        LOCAL_INSTANCE = 'local_gpu'\n",
    "except:\n",
    "    LOGGER.log(\"The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU \"\n",
    "               \"computation\")\n",
    "\n",
    "# Change this to your desired instance type\n",
    "INSTANCE_TYPE = AWS_GPU_INSTANCE_2\n",
    "IS_LOCAL = LOCAL_INSTANCE == INSTANCE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of momentum_indicator: 9\n",
      "Number of overlap_studies: 1\n",
      "Number of pattern_recognition: 2\n",
      "Number of volume_bin: 1\n",
      "2021-05-11 14:16:18.561781 First sample:\n",
      "2021-05-11 14:16:18.569640 \n",
      "              open    high     low   close     volume        mfi       roc  \\\n",
      "date                                                                         \n",
      "2021-01-01  736.42  737.09  735.94  737.02  647.71994  67.016654 -0.024417   \n",
      "\n",
      "                  adx        rsi      slowd      slowk     macd  macdsignal  \\\n",
      "date                                                                          \n",
      "2021-01-01  41.517721  48.978247  28.422228  36.793794 -0.39571   -0.524754   \n",
      "\n",
      "            macdhist         hma  pattern_count  pattern_detected volume_bin  \n",
      "date                                                                          \n",
      "2021-01-01  0.129044  736.669073              1                63          1  \n",
      "2021-05-11 14:16:18.569849 Last sample:\n",
      "2021-05-11 14:16:18.575791 \n",
      "                        open     high      low    close    volume        mfi  \\\n",
      "date                                                                           \n",
      "2021-05-09 14:14:00  3810.12  3810.97  3806.97  3807.36  605.6446  33.781165   \n",
      "\n",
      "                          roc        adx       rsi      slowd      slowk  \\\n",
      "date                                                                       \n",
      "2021-05-09 14:14:00  0.003152  25.055337  44.92111  25.136783  13.314242   \n",
      "\n",
      "                         macd  macdsignal  macdhist          hma  \\\n",
      "date                                                               \n",
      "2021-05-09 14:14:00 -1.411808    0.944717 -2.356525  3806.673724   \n",
      "\n",
      "                     pattern_count  pattern_detected volume_bin  \n",
      "date                                                             \n",
      "2021-05-09 14:14:00              3                74          1  \n",
      "2021-05-11 14:16:18.576085 Number of raw columns: 18\n",
      "2021-05-11 14:16:19.774093 Number of feature columns: 16\n",
      "2021-05-11 14:16:20.200705 Parsed train and test datasets can be found in [/Users/jbeckman/projects/capia/out/datasets]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Parse dataset\n",
    "#\n",
    "\n",
    "from data_processing.parse import Parse\n",
    "\n",
    "PARSE = Parse(LOGGER)\n",
    "\n",
    "dataset_dir_path = local_artifact_dir / \"datasets\"\n",
    "\n",
    "# Creates train and test dataset CSVs\n",
    "PARSE.split_train_test_dataset(dataset_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11 14:18:17.170105 Data will be uploaded to [sagemaker-us-east-2-941048668662]\n",
      "2021-05-11 14:18:17.469087 Uploaded metadata/metadata.json to s3://sagemaker-us-east-2-941048668662/giia-0.6.2/datasets\n",
      "2021-05-11 14:18:17.469499 Data will be uploaded to [sagemaker-us-east-2-941048668662]\n",
      "2021-05-11 14:18:28.222064 Uploaded train/data.json to s3://sagemaker-us-east-2-941048668662/giia-0.6.2/datasets\n",
      "2021-05-11 14:18:28.222466 Data will be uploaded to [sagemaker-us-east-2-941048668662]\n",
      "2021-05-11 14:18:31.520954 Uploaded test/data.json to s3://sagemaker-us-east-2-941048668662/giia-0.6.2/datasets\n",
      "2021-05-11 14:18:31.521408 Model output dir is [s3://sagemaker-us-east-2-941048668662/giia-0.6.2/models]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup local/aws environment. If aws, upload the datasets to S3\n",
    "#\n",
    "\n",
    "from data_processing.aws_handler import AWSHandler\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "AWS_HANDLER = AWSHandler(LOGGER, config.MODEL_ID)\n",
    "\n",
    "sagemaker_session = None\n",
    "\n",
    "model_output_dir_path = local_artifact_dir / config.MODEL_ID / \"models\"\n",
    "model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    LOGGER.log(\"Notebook is set to local mode, not uploading to S3\")\n",
    "\n",
    "    dataset_dir_uri = f\"file://{dataset_dir_path}\"\n",
    "    model_output_dir_uri = f\"file://{model_output_dir_path}\"\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {\n",
    "        'local': {\n",
    "            'local_code': True,\n",
    "            'container_root': str(model_output_dir_path)\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    sagemaker_session = AWS_HANDLER.sagemaker_session\n",
    "\n",
    "    AWS_HANDLER.upload_train_datasets(dataset_dir_path)\n",
    "    dataset_dir_uri = AWS_HANDLER.s3_dataset_dir_uri\n",
    "\n",
    "    model_output_dir_uri = AWS_HANDLER.s3_model_output_uri\n",
    "\n",
    "LOGGER.log(f\"Model output dir is [{model_output_dir_uri}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11 18:20:49 Starting - Starting the training job...\n",
      "2021-05-11 18:21:12 Starting - Launching requested ML instancesProfilerReport-1620757162: InProgress\n",
      "......\n",
      "2021-05-11 18:22:12 Starting - Preparing the instances for training.........\n",
      "2021-05-11 18:23:53 Downloading - Downloading input data...\n",
      "2021-05-11 18:24:23 Training - Training image download completed. Training in progress..\u001B[34m2021-05-11 18:24:23,642 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001B[0m\n",
      "\u001B[34m2021-05-11 18:24:23,665 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch_size\":128,\"context_length\":60,\"dropout_rate\":0.0528,\"epochs\":4,\"learning_rate\":0.003,\"num_cells\":96,\"num_layers\":4,\"prediction_length\":5}', 'SM_USER_ENTRY_POINT': 'deepar_dynamic_real.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"dataset\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'deepar_dynamic_real', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-941048668662/giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dataset\":\"/opt/ml/input/data/dataset\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"context_length\":60,\"dropout_rate\":0.0528,\"epochs\":4,\"learning_rate\":0.003,\"num_cells\":96,\"num_layers\":4,\"prediction_length\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-05-11-18-19-22-092\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-941048668662/giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/source/sourcedir.tar.gz\",\"module_name\":\"deepar_dynamic_real\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"deepar_dynamic_real.py\"}', 'SM_USER_ARGS': '[\"--batch_size\",\"128\",\"--context_length\",\"60\",\"--dropout_rate\",\"0.0528\",\"--epochs\",\"4\",\"--learning_rate\",\"0.003\",\"--num_cells\",\"96\",\"--num_layers\",\"4\",\"--prediction_length\",\"5\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_DATASET': '/opt/ml/input/data/dataset', 'SM_HP_PREDICTION_LENGTH': '5', 'SM_HP_DROPOUT_RATE': '0.0528', 'SM_HP_BATCH_SIZE': '128', 'SM_HP_CONTEXT_LENGTH': '60', 'SM_HP_NUM_CELLS': '96', 'SM_HP_NUM_LAYERS': '4', 'SM_HP_EPOCHS': '4', 'SM_HP_LEARNING_RATE': '0.003'}\u001B[0m\n",
      "\u001B[34m2021-05-11 18:24:26,606 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[34m/usr/local/bin/python3.6 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: PyYAML==5.4.1 in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (5.4.1)\u001B[0m\n",
      "\u001B[34mCollecting jupyter==1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting matplotlib==3.3.3\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/43/2bd63467490036697e7be71444fafc7b236923d614d4521979a200c6b559/matplotlib-3.3.3-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\u001B[0m\n",
      "\u001B[34mCollecting boto3==1.16.57\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/ef/8fd72f02fd605b5a9390c31def4f266978ca6d8e79f3ba740491e487044b/boto3-1.16.57-py2.py3-none-any.whl (130kB)\u001B[0m\n",
      "\u001B[34mCollecting ipynbname==2021.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/fd/ebf75a3f28a7ead2cff9aa0b763bb8308aa6a77b1f7bbc0a86d15e7217be/ipynbname-2021.3.2-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting numpy==1.19.5\n",
      "  Downloading https://files.pythonhosted.org/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8MB)\u001B[0m\n",
      "\u001B[34mCollecting pandas==1.1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\u001B[0m\n",
      "\u001B[34mCollecting s3fs==0.4.2\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/e4/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675/s3fs-0.4.2-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting sagemaker==2.33.0\n",
      "  Downloading https://files.pythonhosted.org/packages/31/cf/99caca0de7515c3710ddd7ed8946ad67965bc4645fd0020b132d8ab408b5/sagemaker-2.33.0.tar.gz (406kB)\u001B[0m\n",
      "\u001B[34mCollecting jenkspy==0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/cc/ccc6d904b40b5c40a66f226a43e3815ff667d8c963421f2b0c928ad12d37/jenkspy-0.2.0.tar.gz (55kB)\u001B[0m\n",
      "\u001B[34mCollecting gluonts\n",
      "  Cloning https://github.com/awslabs/gluon-ts (to revision master) to /tmp/pip-install-xs01o0nj/gluonts\n",
      "  Running command git clone -q https://github.com/awslabs/gluon-ts /tmp/pip-install-xs01o0nj/gluonts\u001B[0m\n",
      "\u001B[34m  Installing build dependencies: started\u001B[0m\n",
      "\u001B[34m  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\u001B[0m\n",
      "\u001B[34m  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\u001B[0m\n",
      "\u001B[34m    Preparing wheel metadata: finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting nbstripout==0.3.9\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/01/762e0daa1765358ca2a7e2176c7470d7a01c4a7b35bbd6a3d391ef019cd8/nbstripout-0.3.9-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting jupyter-console\n",
      "  Downloading https://files.pythonhosted.org/packages/59/cd/aa2670ffc99eb3e5bbe2294c71e4bf46a9804af4f378d09d7a8950996c9b/jupyter_console-6.4.0-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting ipywidgets\n",
      "  Downloading https://files.pythonhosted.org/packages/11/53/084940a83a8158364e630a664a30b03068c25ab75243224d6b488800d43a/ipywidgets-7.6.3-py2.py3-none-any.whl (121kB)\u001B[0m\n",
      "\u001B[34mCollecting nbconvert\n",
      "  Downloading https://files.pythonhosted.org/packages/13/2f/acbe7006548f3914456ee47f97a2033b1b2f3daf921b12ac94105d87c163/nbconvert-6.0.7-py3-none-any.whl (552kB)\u001B[0m\n",
      "\u001B[34mCollecting notebook\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/86/8f951abc6ac651a75a059d2b77fe99fa5df80bf4dc4700c126a0bee486b8/notebook-6.3.0-py3-none-any.whl (9.5MB)\u001B[0m\n",
      "\u001B[34mCollecting qtconsole\n",
      "  Downloading https://files.pythonhosted.org/packages/90/72/3f81d75611bcd4634ae1785efd9f5afbed06ce2b810e4dee28061a2c3895/qtconsole-5.1.0-py3-none-any.whl (119kB)\u001B[0m\n",
      "\u001B[34mCollecting ipykernel\n",
      "  Downloading https://files.pythonhosted.org/packages/89/2d/cff3898f2f58f41b1031c9f98501992ae0c8378919213b88884a1db064c2/ipykernel-5.5.4-py3-none-any.whl (120kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (0.10.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (8.1.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.8.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (1.3.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.4.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from boto3==1.16.57->-r requirements.txt (line 4)) (0.10.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/site-packages (from boto3==1.16.57->-r requirements.txt (line 4)) (0.3.4)\u001B[0m\n",
      "\u001B[34mCollecting botocore<1.20.0,>=1.19.57\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/05/0a955f0c92bec7da076fbbc73926dfb13fab8e2b88de7f8eb17c443f28f0/botocore-1.19.63-py2.py3-none-any.whl (7.2MB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 7)) (2021.1)\u001B[0m\n",
      "\u001B[34mCollecting fsspec>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: attrs in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (20.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: google-pasta in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (0.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf>=3.1 in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (3.15.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (0.1.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (1.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: importlib-metadata>=1.4.0 in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (3.7.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/site-packages (from sagemaker==2.33.0->-r requirements.txt (line 9)) (20.9)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.6/site-packages (from gluonts->-r requirements.txt (line 15)) (4.39.0)\u001B[0m\n",
      "\u001B[34mCollecting holidays>=0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/46/a471d6594325aeb5dc2a591d38eb5ae8b0703dda3bc8d9f656ec1ab00a07/holidays-0.11.1-py3-none-any.whl (133kB)\u001B[0m\n",
      "\u001B[34mCollecting toolz~=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f5/537e55f8ba664ff2a26f26913010fb0fcb98b6bbadc6158af888184fd0b7/toolz-0.11.1-py3-none-any.whl (55kB)\u001B[0m\n",
      "\u001B[34mCollecting pydantic~=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/db/3cd67f632e65fbae6d6c1be200263e684dc4d041078c2f1ad8838ff5adc3/pydantic-1.8.1-cp36-cp36m-manylinux2014_x86_64.whl (10.1MB)\u001B[0m\n",
      "\u001B[34mCollecting nbformat\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/c7/dd50978c637a7af8234909277c4e7ec1b71310c13fb3135f3c8f5b6e045f/nbformat-5.1.3-py3-none-any.whl (178kB)\u001B[0m\n",
      "\u001B[34mCollecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/e6/4b4ca4fa94462d4560ba2f4e62e62108ab07be2e16a92e594e43b12d3300/prompt_toolkit-3.0.18-py3-none-any.whl (367kB)\u001B[0m\n",
      "\u001B[34mCollecting pygments\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/c9/be11fce9810793676017f79ffab3c6cb18575844a6c7b8d4ed92f95de604/Pygments-2.9.0-py3-none-any.whl (1.0MB)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-client\n",
      "  Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB)\u001B[0m\n",
      "\u001B[34mCollecting ipython\n",
      "  Downloading https://files.pythonhosted.org/packages/23/6a/210816c943c9aeeb29e4e18a298f14bf0e118fe222a23e13bfcc2d41b0a4/ipython-7.16.1-py3-none-any.whl (785kB)\u001B[0m\n",
      "\u001B[34mCollecting widgetsnbextension~=3.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2MB)\u001B[0m\n",
      "\u001B[34mCollecting traitlets>=4.3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl (75kB)\u001B[0m\n",
      "\u001B[34mCollecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
      "  Downloading https://files.pythonhosted.org/packages/18/b5/3473d275e3b2359efdf5768e9df95537308b93a31ad94fa92814ac565826/jupyterlab_widgets-1.0.0-py3-none-any.whl (243kB)\u001B[0m\n",
      "\u001B[34mCollecting pandocfilters>=1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/28/78/bd59a9adb72fa139b1c9c186e6f65aebee52375a747e4b6a6dcf0880956f/pandocfilters-1.4.3.tar.gz\u001B[0m\n",
      "\u001B[34mCollecting bleach\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/46/2bbd92086a4c6f051214cb48df6d9132b5f32c5e881d3f4991b16ec7e499/bleach-3.3.0-py2.py3-none-any.whl (283kB)\u001B[0m\n",
      "\u001B[34mCollecting testpath\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl (163kB)\u001B[0m\n",
      "\u001B[34mCollecting defusedxml\n",
      "  Downloading https://files.pythonhosted.org/packages/07/6c/aa3f2f849e01cb6a001cd8554a88d4c77c5c1a31c95bdf1cf9301e6d9ef4/defusedxml-0.7.1-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting jupyter-core\n",
      "  Downloading https://files.pythonhosted.org/packages/53/40/5af36bffa0af3ac71d3a6fc6709de10e4f6ff7c01745b8bc4715372189c9/jupyter_core-4.7.1-py3-none-any.whl (82kB)\u001B[0m\n",
      "\u001B[34mCollecting mistune<2,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting jupyterlab-pygments\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/6f/c34288766797193b512c6508f5994b830fb06134fdc4ca8214daba0aa443/jupyterlab_pygments-0.1.2-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/22/a6/f3a01a5c1a0e72d1d064f33d4cd9c3a782010f48f48f47f256d0b438994a/nbclient-0.5.3-py3-none-any.whl (82kB)\u001B[0m\n",
      "\u001B[34mCollecting entrypoints>=0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting jinja2>=2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/c2/1eece8c95ddbc9b1aeb64f5783a9e07a286de42191b7204d67b7496ddf35/Jinja2-2.11.3-py2.py3-none-any.whl (125kB)\u001B[0m\n",
      "\u001B[34mCollecting ipython-genutils\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting tornado>=6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/85/26/e710295dcb4aac62b08f22d07efc899574476db37532159a7f71713cdaf2/tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427kB)\u001B[0m\n",
      "\u001B[34mCollecting pyzmq>=17\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/8f/e83fc0060a7626d3555b971a70a37a0d57f727ec7ec860e9aadf96fdd724/pyzmq-22.0.3-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\u001B[0m\n",
      "\u001B[34mCollecting Send2Trash>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting terminado>=0.8.3\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/48/af0a3ca6aacd6feccf7a59c45884aeaa10a3000a648a02a4740a9137c243/terminado-0.9.5-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting prometheus-client\n",
      "  Downloading https://files.pythonhosted.org/packages/22/f7/f6e1676521ce7e311d38563d2cf6594d09d3717d799ede7dab7b2520093e/prometheus_client-0.10.1-py2.py3-none-any.whl (55kB)\u001B[0m\n",
      "\u001B[34mCollecting argon2-cffi\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/d7/5da06217807106ed6d7b4f5ccb8ec5e3f9ec969217faad4b5d1af0b55101/argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97kB)\u001B[0m\n",
      "\u001B[34mCollecting qtpy\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/fd/9972948f02e967b691cc0ca1f26124826a3b88cb38f412a8b7935b8c3c72/QtPy-1.9.0-py2.py3-none-any.whl (54kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib==3.3.3->-r requirements.txt (line 3)) (1.15.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.57->boto3==1.16.57->-r requirements.txt (line 4)) (1.25.11)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.33.0->-r requirements.txt (line 9)) (3.4.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.33.0->-r requirements.txt (line 9)) (3.7.4.3)\u001B[0m\n",
      "\u001B[34mCollecting korean-lunar-calendar\n",
      "  Downloading https://files.pythonhosted.org/packages/15/41/aa426a4a9141afd8e7f5c8312bb59d5693274f3f7b34e73bdce4ee48b4c1/korean_lunar_calendar-0.2.1-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting hijri-converter\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/75/e6da96d4ea768c8e6fa9676cffce80e457b66c3beb5711189959582870d6/hijri_converter-2.1.1-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting convertdate>=2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/fa/b5f5d8b3a328c930a190540231fe79e854a416df62c57329630823f3941e/convertdate-2.3.2-py3-none-any.whl (47kB)\u001B[0m\n",
      "\u001B[34mCollecting dataclasses>=0.6; python_version < \"3.7\"\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\u001B[0m\n",
      "\u001B[34mCollecting wcwidth\n",
      "  Downloading https://files.pythonhosted.org/packages/59/7c/e39aca596badaf1b78e8f547c807b04dae603a433d3e7a7e04d67f2ef3e5/wcwidth-0.2.5-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting backcall\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/1c/ff6546b6c12603d8dd1070aa3c3d273ad4c07f5771689a7b69a550e8c951/backcall-0.2.0-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: decorator in /usr/local/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 2)) (4.4.2)\u001B[0m\n",
      "\u001B[34mCollecting pickleshare\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting pexpect; sys_platform != \"win32\"\n",
      "  Downloading https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl (59kB)\u001B[0m\n",
      "\u001B[34mCollecting jedi>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/36/7aa67ae2663025b49e8426ead0bad983fee1b73f472536e9790655da0277/jedi-0.18.0-py2.py3-none-any.whl (1.4MB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 2)) (54.1.1)\u001B[0m\n",
      "\u001B[34mCollecting webencodings\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting async-generator\n",
      "  Downloading https://files.pythonhosted.org/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting nest-asyncio\n",
      "  Downloading https://files.pythonhosted.org/packages/52/e2/9b37da54e6e9094d2f558ae643d1954a0fa8215dfee4fa261f31c5439796/nest_asyncio-1.5.1-py3-none-any.whl\u001B[0m\n",
      "\u001B[34mCollecting MarkupSafe>=0.23\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/d3/75cddfad6ca1d1bb3a017cece499a65e54ceb4583800f1256b8ad07bb57f/MarkupSafe-1.1.1-cp36-cp36m-manylinux2010_x86_64.whl\u001B[0m\n",
      "\u001B[34mCollecting ptyprocess; os_name != \"nt\"\n",
      "  Downloading https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/site-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.14.5)\u001B[0m\n",
      "\u001B[34mCollecting pymeeus<=1,>=0.3.13\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/ff/0f0a0becf088281c6bc6c75b7d7c03a2481d486ef6cc7c8899bbcab0a88d/PyMeeus-0.5.11.tar.gz (5.4MB)\u001B[0m\n",
      "\u001B[34mCollecting pyrsistent>=0.14.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/70/fd441df751ba8b620e03fd2d2d9ca902103119616f0f6cc42e6405035062/pyrsistent-0.17.3.tar.gz (106kB)\u001B[0m\n",
      "\u001B[34mCollecting parso<0.9.0,>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/c4/d5476373088c120ffed82f34c74b266ccae31a68d665b837354d4d8dc8be/parso-0.8.2-py2.py3-none-any.whl (94kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pycparser in /usr/local/lib/python3.6/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.20)\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: gluonts\n",
      "  Building wheel for gluonts (PEP 517): started\u001B[0m\n",
      "\u001B[34m  Building wheel for gluonts (PEP 517): finished with status 'done'\n",
      "  Created wheel for gluonts: filename=gluonts-0.7.0.dev16+g973e43c-cp36-none-any.whl size=2060195 sha256=8cf6b2c5415dfb87a330d332361c15dba03488b398d867e9c02ad87d3a877626\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zd8k6pdd/wheels/9a/c8/a1/3e031a3ab929ce56ea808f874ee0c44b90fcf4ffc588d7d1fb\u001B[0m\n",
      "\u001B[34mSuccessfully built gluonts\u001B[0m\n",
      "\u001B[34mERROR: awscli 1.19.24 has requirement botocore==1.20.24, but you'll have botocore 1.19.63 which is incompatible.\u001B[0m\n",
      "\u001B[34mInstalling collected packages: wcwidth, prompt-toolkit, pygments, ipython-genutils, traitlets, jupyter-core, tornado, pyzmq, jupyter-client, backcall, pickleshare, ptyprocess, pexpect, parso, jedi, ipython, ipykernel, jupyter-console, pyrsistent, jsonschema, nbformat, Send2Trash, terminado, pandocfilters, webencodings, bleach, testpath, defusedxml, mistune, jupyterlab-pygments, async-generator, nest-asyncio, nbclient, entrypoints, MarkupSafe, jinja2, nbconvert, prometheus-client, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, ipywidgets, qtpy, qtconsole, jupyter, numpy, matplotlib, botocore, boto3, ipynbname, pandas, fsspec, s3fs, sagemaker, jenkspy, korean-lunar-calendar, hijri-converter, pymeeus, convertdate, holidays, toolz, dataclasses, pydantic, gluonts, nbstripout\u001B[0m\n",
      "\u001B[34m    Running setup.py install for pyrsistent: started\u001B[0m\n",
      "\u001B[34m    Running setup.py install for pyrsistent: finished with status 'done'\n",
      "    Running setup.py install for pandocfilters: started\u001B[0m\n",
      "\u001B[34m    Running setup.py install for pandocfilters: finished with status 'done'\u001B[0m\n",
      "\u001B[34m  Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\u001B[0m\n",
      "\u001B[34m  Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\u001B[0m\n",
      "\u001B[34m      Successfully uninstalled matplotlib-3.3.4\u001B[0m\n",
      "\u001B[34m  Found existing installation: botocore 1.20.24\n",
      "    Uninstalling botocore-1.20.24:\n",
      "      Successfully uninstalled botocore-1.20.24\n",
      "  Found existing installation: boto3 1.17.24\n",
      "    Uninstalling boto3-1.17.24:\n",
      "      Successfully uninstalled boto3-1.17.24\n",
      "  Found existing installation: pandas 0.25.1\u001B[0m\n",
      "\u001B[34m    Uninstalling pandas-0.25.1:\n",
      "      Successfully uninstalled pandas-0.25.1\u001B[0m\n",
      "\u001B[34m  Found existing installation: sagemaker 2.29.1\n",
      "    Uninstalling sagemaker-2.29.1:\n",
      "      Successfully uninstalled sagemaker-2.29.1\n",
      "    Running setup.py install for sagemaker: started\u001B[0m\n",
      "\u001B[34m    Running setup.py install for sagemaker: finished with status 'done'\n",
      "    Running setup.py install for jenkspy: started\u001B[0m\n",
      "\u001B[34m    Running setup.py install for jenkspy: finished with status 'done'\n",
      "    Running setup.py install for pymeeus: started\n",
      "    Running setup.py install for pymeeus: finished with status 'done'\u001B[0m\n",
      "\u001B[34mSuccessfully installed MarkupSafe-1.1.1 Send2Trash-1.5.0 argon2-cffi-20.1.0 async-generator-1.10 backcall-0.2.0 bleach-3.3.0 boto3-1.16.57 botocore-1.19.63 convertdate-2.3.2 dataclasses-0.8 defusedxml-0.7.1 entrypoints-0.3 fsspec-2021.4.0 gluonts-0.7.0.dev16+g973e43c hijri-converter-2.1.1 holidays-0.11.1 ipykernel-5.5.4 ipynbname-2021.3.2 ipython-7.16.1 ipython-genutils-0.2.0 ipywidgets-7.6.3 jedi-0.18.0 jenkspy-0.2.0 jinja2-2.11.3 jsonschema-3.2.0 jupyter-1.0.0 jupyter-client-6.1.12 jupyter-console-6.4.0 jupyter-core-4.7.1 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.0 korean-lunar-calendar-0.2.1 matplotlib-3.3.3 mistune-0.8.4 nbclient-0.5.3 nbconvert-6.0.7 nbformat-5.1.3 nbstripout-0.3.9 nest-asyncio-1.5.1 notebook-6.3.0 numpy-1.19.5 pandas-1.1.5 pandocfilters-1.4.3 parso-0.8.2 pexpect-4.8.0 pickleshare-0.7.5 prometheus-client-0.10.1 prompt-toolkit-3.0.18 ptyprocess-0.7.0 pydantic-1.8.1 pygments-2.9.0 pymeeus-0.5.11 pyrsistent-0.17.3 pyzmq-22.0.3 qtconsole-5.1.0 qtpy-1.9.0 s3fs-0.4.2 sagemaker-2.33.0 terminado-0.9.5 testpath-0.4.4 toolz-0.11.1 tornado-6.1 traitlets-4.3.3 wcwidth-0.2.5 webencodings-0.5.1 widgetsnbextension-3.5.1\u001B[0m\n",
      "\u001B[34mWARNING: You are using pip version 19.3.1; however, version 21.1.1 is available.\u001B[0m\n",
      "\u001B[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001B[0m\n",
      "\u001B[34m2021-05-11 18:25:10,327 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001B[0m\n",
      "\u001B[34mTraining Env:\n",
      "\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dataset\": \"/opt/ml/input/data/dataset\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"prediction_length\": 5,\n",
      "        \"dropout_rate\": 0.0528,\n",
      "        \"batch_size\": 128,\n",
      "        \"context_length\": 60,\n",
      "        \"num_cells\": 96,\n",
      "        \"num_layers\": 4,\n",
      "        \"epochs\": 4,\n",
      "        \"learning_rate\": 0.003\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dataset\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2021-05-11-18-19-22-092\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-941048668662/giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"deepar_dynamic_real\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"deepar_dynamic_real.py\"\u001B[0m\n",
      "\u001B[34m}\n",
      "\u001B[0m\n",
      "\u001B[34mEnvironment variables:\n",
      "\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"batch_size\":128,\"context_length\":60,\"dropout_rate\":0.0528,\"epochs\":4,\"learning_rate\":0.003,\"num_cells\":96,\"num_layers\":4,\"prediction_length\":5}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=deepar_dynamic_real.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"dataset\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=deepar_dynamic_real\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=8\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=1\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-941048668662/giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dataset\":\"/opt/ml/input/data/dataset\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"context_length\":60,\"dropout_rate\":0.0528,\"epochs\":4,\"learning_rate\":0.003,\"num_cells\":96,\"num_layers\":4,\"prediction_length\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-05-11-18-19-22-092\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-941048668662/giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/source/sourcedir.tar.gz\",\"module_name\":\"deepar_dynamic_real\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"deepar_dynamic_real.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--context_length\",\"60\",\"--dropout_rate\",\"0.0528\",\"--epochs\",\"4\",\"--learning_rate\",\"0.003\",\"--num_cells\",\"96\",\"--num_layers\",\"4\",\"--prediction_length\",\"5\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_DATASET=/opt/ml/input/data/dataset\u001B[0m\n",
      "\u001B[34mSM_HP_PREDICTION_LENGTH=5\u001B[0m\n",
      "\u001B[34mSM_HP_DROPOUT_RATE=0.0528\u001B[0m\n",
      "\u001B[34mSM_HP_BATCH_SIZE=128\u001B[0m\n",
      "\u001B[34mSM_HP_CONTEXT_LENGTH=60\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_CELLS=96\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_LAYERS=4\u001B[0m\n",
      "\u001B[34mSM_HP_EPOCHS=4\u001B[0m\n",
      "\u001B[34mSM_HP_LEARNING_RATE=0.003\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\n",
      "\u001B[0m\n",
      "\u001B[34m/usr/local/bin/python3.6 deepar_dynamic_real.py --batch_size 128 --context_length 60 --dropout_rate 0.0528 --epochs 4 --learning_rate 0.003 --num_cells 96 --num_layers 4 --prediction_length 5\n",
      "\n",
      "\u001B[0m\n",
      "\u001B[34m/usr/local/lib/python3.6/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  \"Using `json`-module for json-handling. \"\u001B[0m\n",
      "\u001B[34mUsing the follow arguments: [Namespace(batch_size=128, context_length=60, dataset_dir='/opt/ml/input/data/dataset', dropout_rate=0.0528, epochs=4, learning_rate=0.003, model_dir='/opt/ml/model', num_batches_per_epoch=None, num_cells=96, num_layers=4, prediction_length=5)]\u001B[0m\n",
      "\u001B[34mThe model id is [giia-0.6.2]\u001B[0m\n",
      "\u001B[34mThe MXNet version is [1.7.0]\u001B[0m\n",
      "\u001B[34mThe GluonTS version is [0.7.0.dev16+g973e43c]\u001B[0m\n",
      "\u001B[34mThe GPU count is [1]\u001B[0m\n",
      "\u001B[34m[✔ CUDA, ✔ CUDNN, ✔ NCCL, ✔ CUDA_RTC, ✖ TENSORRT, ✔ CPU_SSE, ✔ CPU_SSE2, ✔ CPU_SSE3, ✔ CPU_SSE4_1, ✔ CPU_SSE4_2, ✖ CPU_SSE4A, ✔ CPU_AVX, ✖ CPU_AVX2, ✔ OPENMP, ✖ SSE, ✔ F16C, ✖ JEMALLOC, ✔ BLAS_OPEN, ✖ BLAS_ATLAS, ✖ BLAS_MKL, ✖ BLAS_APPLE, ✔ LAPACK, ✔ MKLDNN, ✔ OPENCV, ✖ CAFFE, ✖ PROFILER, ✔ DIST_KVSTORE, ✖ CXX14, ✖ INT64_TENSOR_SIZE, ✔ SIGNAL_HANDLER, ✖ DEBUG, ✖ TVM_OP]\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\u001B[0m\n",
      "\u001B[34mTrain dataset stats: DatasetStatistics(integer_dataset=False, max_target=2039.510009765625, mean_abs_target=1529.6377466788044, mean_target=1529.6377466788044, mean_target_length=129622.0, max_target_length=129622, min_target=715.219970703125, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=16, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=129622, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts={10: 1}, empty_target_count=0))\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\u001B[0m\n",
      "\u001B[34mTest dataset stats: DatasetStatistics(integer_dataset=False, max_target=3979.570068359375, mean_abs_target=2507.467823519882, mean_target=2507.467823519882, mean_target_length=55553.0, max_target_length=55553, min_target=1886.219970703125, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=16, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=55553, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts={11: 1}, empty_target_count=0))\u001B[0m\n",
      "\u001B[34mDefaulting num_batches_per_epoch to: [1012] = (length of train dataset [129622]) / (batch size [128])\u001B[0m\n",
      "\u001B[34mUsing GPU context\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.417 ip-10-0-130-30.us-east-2.compute.internal:145 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.417 ip-10-0-130-30.us-east-2.compute.internal:145 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.417 ip-10-0-130-30.us-east-2.compute.internal:145 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.418 ip-10-0-130-30.us-east-2.compute.internal:145 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.435 ip-10-0-130-30.us-east-2.compute.internal:145 INFO hook.py:398] Monitoring the collections: losses\u001B[0m\n",
      "\u001B[34m[2021-05-11 18:25:41.435 ip-10-0-130-30.us-east-2.compute.internal:145 INFO hook.py:459] Hook is writing from the hook with pid: 145\n",
      "\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1012 [00:00<?, ?it/s]INFO:gluonts.trainer:Number of parameters in DeepARTrainingNetwork: 279460\u001B[0m\n",
      "\n",
      "2021-05-11 18:38:56 Stopping - Stopping the training job\u001B[34m#015  0%|          | 1/1012 [00:44<12:36:08, 44.88s/it, epoch=1/4, avg_epoch_loss=7.97]#015  0%|          | 2/1012 [00:58<9:57:23, 35.49s/it, epoch=1/4, avg_epoch_loss=10.6] #015  0%|          | 3/1012 [01:09<7:51:20, 28.03s/it, epoch=1/4, avg_epoch_loss=10.4]#015  0%|          | 4/1012 [01:23<6:44:18, 24.07s/it, epoch=1/4, avg_epoch_loss=10.1]#015  0%|          | 5/1012 [01:34<5:33:57, 19.90s/it, epoch=1/4, avg_epoch_loss=9.94]#015  1%|          | 6/1012 [01:49<5:13:11, 18.68s/it, epoch=1/4, avg_epoch_loss=9.82]#015  1%|          | 7/1012 [02:01<4:34:46, 16.40s/it, epoch=1/4, avg_epoch_loss=9.73]#015  1%|          | 9/1012 [02:22<4:05:18, 14.67s/it, epoch=1/4, avg_epoch_loss=9.58]#015  1%|          | 10/1012 [02:38<4:14:35, 15.25s/it, epoch=1/4, avg_epoch_loss=9.5]#015  1%|          | 11/1012 [02:53<4:12:57, 15.16s/it, epoch=1/4, avg_epoch_loss=9.43]#015  1%|          | 12/1012 [03:10<4:19:49, 15.59s/it, epoch=1/4, avg_epoch_loss=9.35]#015  1%|▏         | 14/1012 [03:28<3:46:02, 13.59s/it, epoch=1/4, avg_epoch_loss=9.18]#015  1%|▏         | 14/1012 [03:43<3:46:02, 13.59s/it, epoch=1/4, avg_epoch_loss=9.08]#015  2%|▏         | 16/1012 [03:50<3:32:07, 12.78s/it, epoch=1/4, avg_epoch_loss=8.99]#015  2%|▏         | 17/1012 [04:00<3:21:12, 12.13s/it, epoch=1/4, avg_epoch_loss=8.89]#015  2%|▏         | 18/1012 [04:15<3:36:34, 13.07s/it, epoch=1/4, avg_epoch_loss=8.77]#015  2%|▏         | 19/1012 [04:30<3:43:26, 13.50s/it, epoch=1/4, avg_epoch_loss=8.66]#015  2%|▏         | 20/1012 [04:42<3:37:30, 13.16s/it, epoch=1/4, avg_epoch_loss=8.66]#015  2%|▏         | 21/1012 [04:58<3:52:07, 14.05s/it, epoch=1/4, avg_epoch_loss=8.6] #015  2%|▏         | 22/1012 [05:13<3:56:07, 14.31s/it, epoch=1/4, avg_epoch_loss=8.54]#015  2%|▏         | 23/1012 [05:26<3:45:55, 13.71s/it, epoch=1/4, avg_epoch_loss=8.49]#015  2%|▏         | 24/1012 [05:40<3:49:36, 13.94s/it, epoch=1/4, avg_epoch_loss=8.41]#015  2%|▏         | 25/1012 [05:53<3:41:43, 13.48s/it, epoch=1/4, avg_epoch_loss=8.34]#015  3%|▎         | 26/1012 [06:07<3:44:08, 13.64s/it, epoch=1/4, avg_epoch_loss=8.3] #015  3%|▎         | 27/1012 [06:18<3:33:12, 12.99s/it, epoch=1/4, avg_epoch_loss=8.25]#015  3%|▎         | 28/1012 [06:29<3:23:25, 12.40s/it, epoch=1/4, avg_epoch_loss=8.2] #015  3%|▎         | 29/1012 [06:43<3:29:33, 12.79s/it, epoch=1/4, avg_epoch_loss=8.15]#015  3%|▎         | 30/1012 [06:57<3:37:27, 13.29s/it, epoch=1/4, avg_epoch_loss=8.1] #015  3%|▎         | 31/1012 [07:11<3:40:46, 13.50s/it, epoch=1/4, avg_epoch_loss=8.06]#015  3%|▎         | 32/1012 [07:24<3:34:48, 13.15s/it, epoch=1/4, avg_epoch_loss=8.02]#015  3%|▎         | 33/1012 [07:35<3:28:44, 12.79s/it, epoch=1/4, avg_epoch_loss=7.97]#015  3%|▎         | 34/1012 [07:47<3:22:32, 12.43s/it, epoch=1/4, avg_epoch_loss=7.93]#015  3%|▎         | 35/1012 [07:57<3:11:41, 11.77s/it, epoch=1/4, avg_epoch_loss=7.89]#015  4%|▎         | 36/1012 [08:12<3:24:27, 12.57s/it, epoch=1/4, avg_epoch_loss=7.84]#015  4%|▎         | 37/1012 [08:28<3:41:35, 13.64s/it, epoch=1/4, avg_epoch_loss=7.79]#015  4%|▍         | 38/1012 [08:41<3:41:02, 13.62s/it, epoch=1/4, avg_epoch_loss=7.75]#015  4%|▍         | 39/1012 [08:55<3:38:54, 13.50s/it, epoch=1/4, avg_epoch_loss=7.7] #015  4%|▍         | 40/1012 [09:09<3:43:07, 13.77s/it, epoch=1/4, avg_epoch_loss=7.66]#015  4%|▍         | 41/1012 [09:22<3:40:06, 13.60s/it, epoch=1/4, avg_epoch_loss=7.63]#015  4%|▍         | 42/1012 [09:34<3:31:37, 13.09s/it, epoch=1/4, avg_epoch_loss=7.58]#015  4%|▍         | 43/1012 [09:50<3:42:28, 13.78s/it, epoch=1/4, avg_epoch_loss=7.53]#015  4%|▍         | 44/1012 [10:04<3:43:27, 13.85s/it, epoch=1/4, avg_epoch_loss=7.52]#015  4%|▍         | 45/1012 [10:21<3:58:22, 14.79s/it, epoch=1/4, avg_epoch_loss=7.47]#015  5%|▍         | 46/1012 [10:36<4:00:23, 14.93s/it, epoch=1/4, avg_epoch_loss=7.42]#015  5%|▍         | 47/1012 [10:49<3:52:20, 14.45s/it, epoch=1/4, avg_epoch_loss=7.39]#015  5%|▍         | 48/1012 [11:04<3:52:10, 14.45s/it, epoch=1/4, avg_epoch_loss=7.38]#015  5%|▍         | 49/1012 [11:15<3:39:44, 13.69s/it, epoch=1/4, avg_epoch_loss=7.35]#015  5%|▍         | 50/1012 [11:29<3:39:00, 13.66s/it, epoch=1/4, avg_epoch_loss=7.37]#015  5%|▌         | 51/1012 [11:40<3:26:20, 12.88s/it, epoch=1/4, avg_epoch_loss=7.39]#015  5%|▌         | 52/1012 [11:53<3:25:48, 12.86s/it, epoch=1/4, avg_epoch_loss=7.39]#015  5%|▌         | 53/1012 [12:06<3:27:02, 12.95s/it, epoch=1/4, avg_epoch_loss=7.36]#015  5%|▌         | 54/1012 [12:18<3:19:43, 12.51s/it, epoch=1/4, avg_epoch_loss=7.34]#015  5%|▌         | 55/1012 [12:31<3:24:45, 12.84s/it, epoch=1/4, avg_epoch_loss=7.33]#015  6%|▌         | 56/1012 [12:46<3:33:03, 13.37s/it, epoch=1/4, avg_epoch_loss=7.32]#015  6%|▌         | 57/1012 [12:59<3:31:53, 13.31s/it, epoch=1/4, avg_epoch_loss=7.3] #015  6%|▌         | 58/1012 [13:10<3:22:58, 12.77s/it, epoch=1/4, avg_epoch_loss=7.28]#015  6%|▌         | 59/1012 [13:21<3:10:32, 12.00s/it, epoch=1/4, avg_epoch_loss=7.27]#015  6%|▌         | 60/1012 [13:35<3:21:43, 12.71s/it, epoch=1/4, avg_epoch_loss=7.26]#015  6%|▌         | 61/1012 [13:46<3:11:57, 12.11s/it, epoch=1/4, avg_epoch_loss=7.25]#015  6%|▌         | 62/1012 [13:59<3:18:46, 12.55s/it, epoch=1/4, avg_epoch_loss=7.23]#015  6%|▌         | 63/1012 [14:19<3:53:42, 14.78s/it, epoch=1/4, avg_epoch_loss=7.22]#015  6%|▋         | 64/1012 [14:35<3:55:47, 14.92s/it, epoch=1/4, avg_epoch_loss=7.2] #015  6%|▋         | 65/1012 [14:47<3:45:37, 14.30s/it, epoch=1/4, avg_epoch_loss=7.19]#015  7%|▋         | 66/1012 [15:04<3:56:11, 14.98s/it, epoch=1/4, avg_epoch_loss=7.18]\u001B[0m\n",
      "\n",
      "2021-05-11 18:40:59 Uploading - Uploading generated training model\n",
      "2021-05-11 18:40:59 Stopped - Training job stopped\n",
      "ProfilerReport-1620757162: Stopping\n",
      "Training seconds: 889\n",
      "Billable seconds: 267\n",
      "Managed Spot Training savings: 70.0%\n",
      "2021-05-11 14:41:15.922367 Training job name: mxnet-training-2021-05-11-18-19-22-092\n",
      "2021-05-11 14:41:16.127897 Model is saved in: /giia-0.6.2/models/mxnet-training-2021-05-11-18-19-22-092/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Configure sagemaker and estimator\n",
    "#\n",
    "\n",
    "from ml.train import Train\n",
    "\n",
    "TRAIN = Train(LOGGER)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    train_kwargs = {}\n",
    "else:\n",
    "    train_kwargs = {\n",
    "        # 'checkpoint_s3_uri': model_output_dir_uri,\n",
    "        'output_path': model_output_dir_uri,\n",
    "        'code_location': model_output_dir_uri,\n",
    "        'use_spot_instances': True,\n",
    "        'max_wait': 18 * 60 * 60, # 18 hours\n",
    "        'max_run': 18 * 60 * 60, # 18 hours\n",
    "    }\n",
    "\n",
    "estimator = TRAIN.create_model(config.SM_ROLE, INSTANCE_TYPE, sagemaker_session, train_kwargs)\n",
    "TRAIN.fit_model(estimator, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Load model\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from gluonts.model.predictor import Predictor\n",
    "\n",
    "if IS_LOCAL:\n",
    "    # model_output_dir_path is basically the same path as it was before, though sagemaker appends a random temp\n",
    "    # directory to the path. The path from TRAIN includes that random temp directory\n",
    "    # model_dir_path = TRAIN.model_data_path.parent.parent / \"model\"\n",
    "    model_dir_path = local_artifact_dir / \"local_cli\" / \"model\"\n",
    "else:\n",
    "    model_dir_path = AWS_HANDLER.download_model_from_s3(str(TRAIN.model_data_path), local_artifact_dir)\n",
    "\n",
    "LOGGER.log(f\"Model dir is [{model_dir_path}]\")\n",
    "predictor = Predictor.deserialize(model_dir_path)\n",
    "LOGGER.log(f\"Predictor metadata [{predictor.__dict__}]\")\n",
    "\n",
    "\n",
    "def plot_prob_forecasts(ts_list, forecast_list, plot_length=100):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "        ax = target[-plot_length:].plot(figsize=(10, 7), linewidth=2)\n",
    "        forecast.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()\n",
    "    \n",
    "def plot_prob_forecasts_multi(ts_list, forecast_list, plot_length=60):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        for i in range(5):\n",
    "            prediction_intervals = (50.0, 90.0)\n",
    "            legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "            target[i][-plot_length:].plot(ax=ax)  # plot the time series\n",
    "            forecast.copy_dim(i).plot(prediction_intervals=prediction_intervals, color='g')\n",
    "            plt.grid(which=\"both\")\n",
    "            plt.legend(legend, loc=\"upper left\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define test data and make a prediction\n",
    "#\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.common import ListDataset, FileDataset\n",
    "from utils.splitter import DateSplitter\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.stat import calculate_dataset_statistics\n",
    "\n",
    "# test_file_dataset = FileDataset(path=(dataset_dir_path / config.TEST_DATASET_FILENAME).parent, freq=\"min\")\n",
    "test_file_dataset = FileDataset(path=(dataset_dir_path / config.TEST_DATASET_FILENAME).parent, freq=config.DATASET_FREQ)\n",
    "\n",
    "# pd.Timedelta(config.HYPER_PARAMETERS[\"prediction_length\"], unit=\"5min\")\n",
    "# for data in iter(test_file_dataset):\n",
    "#     data.start.freqstr = \"min\"\n",
    "\n",
    "test_datasets = []\n",
    "test_dates = [\"2021-04-20 12:50:00\", \"2021-04-20 15:55:00\", \"2021-04-21 16:00:00\", \"2021-04-22 17:00:00\"]\n",
    "for idx, date in enumerate(test_dates):\n",
    "    # 1) Get splice of dataset for different dates with ample history\n",
    "    splitter = DateSplitter(\n",
    "        prediction_length=-config.HYPER_PARAMETERS[\"prediction_length\"],\n",
    "        split_date=date,\n",
    "        max_history=config.FREQTRADE_MAX_CONTEXT\n",
    "    )\n",
    "    (_, train_dataset), (_, test_dataset) = splitter.split(test_file_dataset)\n",
    "\n",
    "    # # 2) Remove other time-series as we only want to predict\n",
    "    # for data in iter(test_dataset):\n",
    "    #     if data['item_id'] == \"close\":\n",
    "    #         test_dataset = ListDataset([{\n",
    "    #             FieldName.START: data[FieldName.START],\n",
    "    #             FieldName.TARGET: data[FieldName.TARGET],\n",
    "    #             FieldName.FEAT_STATIC_CAT: data[FieldName.FEAT_STATIC_CAT],\n",
    "    #             FieldName.ITEM_ID: data[FieldName.ITEM_ID],\n",
    "    #         }], freq=config.DATASET_FREQ)\n",
    "    #         break\n",
    "\n",
    "    LOGGER.log(f\"Test dataset [{idx}] stats: {calculate_dataset_statistics(test_dataset)}\")\n",
    "    test_datasets.append(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate and visualize the prediction\n",
    "#\n",
    "import json\n",
    "\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "\n",
    "for test_dataset in test_datasets:\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_dataset,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    forecast_entry = forecasts[0]\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    # LOGGER.log(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "    # LOGGER.log(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "    # LOGGER.log(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "    # LOGGER.log(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "\n",
    "    evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_dataset))\n",
    "\n",
    "    LOGGER.log(json.dumps(agg_metrics, indent=4))\n",
    "    item_metrics.head()\n",
    "\n",
    "    plot_prob_forecasts(tss, forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: FURTHER CELLS ARE COMPATIBLE WITH AWS SAGEMAKER ONLY, LOCAL MODE WILL NOT WORK\n",
    "# Hyperparameter tune the model\n",
    "#\n",
    "\n",
    "from ml.tune import Tune\n",
    "\n",
    "TUNE = Tune(UTILS, LOGGER)\n",
    "\n",
    "train_dataset_uri = f\"{dataset_dir_uri}/{config.TRAIN_DATASET_FILENAME}\"\n",
    "test_dataset_uri = f\"{dataset_dir_uri}/{config.TEST_DATASET_FILENAME}\"\n",
    "\n",
    "# Note: Feel free to tune the tuner, i.e. update max number of jobs and hyperparameters. Default is 10 jobs, but you\n",
    "# may want to change this as you refine the model. Additionally, if you find the best model has a parameter at the\n",
    "# end of the range you gave it, then you should look to move that range to determine if the model performs better\n",
    "# along that vector\n",
    "tuner = TUNE.create_tuner(estimator)\n",
    "TUNE.fit_tuner(tuner, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get updates for Hyperparameter tune job. Ensure this is completed before going to the next cell\n",
    "#\n",
    "\n",
    "TUNE.get_tune_job_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the metrics of the tune job\n",
    "#\n",
    "\n",
    "TUNE.report_job_analytics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capia (venv)",
   "language": "python",
   "name": "capia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}