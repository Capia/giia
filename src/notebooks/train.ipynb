{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory [/Users/jbeckman/projects/capia/giia/src]\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "MODULE_PATH = ipynbname.path().parent.parent\n",
    "sys.path.append(str(MODULE_PATH))\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# This should always be `./src`\n",
    "print(f\"Current working directory [{os.getcwd()}]\")\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 13:57:22.520296 The model id is [giia-tft_torch-1.1.2]\n",
      "2023-03-05 13:57:22.521198 The MXNet version is [1.9.1]\n",
      "2023-03-05 13:57:22.521418 The GluonTS version is [0.12.3]\n",
      "2023-03-05 13:57:22.521567 The SageMaker version is [2.111.0]\n",
      "2023-03-05 13:57:22.521848 The GPU count is [0]\n",
      "2023-03-05 13:57:22.549110 The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU computation\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup utils\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "from utils import config\n",
    "\n",
    "LOGGER = LoggerUtil(config.MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()\n",
    "\n",
    "# AWS instance specs can be found here https://aws.amazon.com/sagemaker/pricing/\n",
    "# AWS_INSTANCE = 'ml.m5.large'        # 2 vCPU,   0 GPU,  8 GB memory,     $0.134/hour\n",
    "# AWS_INSTANCE = 'ml.m5.4xlarge'      # 16 vCPU,  0 GPU,  64 GB memory,    $0.922/hour\n",
    "# AWS_INSTANCE = 'ml.g4dn.xlarge'     # 4 vCPU,   1 GPU,  16 GB memory,    $0.736/hour\n",
    "AWS_INSTANCE = 'ml.g4dn.8xlarge'    # 32 vCPU,  1 GPU,  128 GB memory,   $2.72/hour\n",
    "# AWS_INSTANCE = 'ml.p2.xlarge'       # 4 vCPU,   1 GPU,  61 GB memory,    $0.900/hour\n",
    "# AWS_INSTANCE = 'ml.p3.2xlarge'      # 8 vCPU,   1 GPU,  61 GB memory,    $3.825/hour\n",
    "LOCAL_INSTANCE = 'local'\n",
    "\n",
    "try:\n",
    "    if subprocess.call('nvidia-smi') == 0:\n",
    "        LOCAL_INSTANCE = 'local_gpu'\n",
    "except:\n",
    "    LOGGER.log(\"The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU \"\n",
    "               \"computation\")\n",
    "\n",
    "# Change this to your desired instance type\n",
    "INSTANCE_TYPE = AWS_INSTANCE\n",
    "# INSTANCE_TYPE = LOCAL_INSTANCE\n",
    "IS_LOCAL = LOCAL_INSTANCE == INSTANCE_TYPE\n",
    "\n",
    "# Does the model use filedataset or CSVs\n",
    "FILEDATASET_BASED = True\n",
    "\n",
    "# Is the model univariate\n",
    "ONE_DIM_TARGET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 13:57:31.090373 First sample:\n",
      "2023-03-05 13:57:31.093775 \n",
      "              open    high     low   close  volume\n",
      "date                                              \n",
      "2020-01-01  128.66  128.66  128.66  128.66     0.0\n",
      "2023-03-05 13:57:31.093947 Last sample:\n",
      "2023-03-05 13:57:31.096300 \n",
      "                        open    high      low   close     volume\n",
      "date                                                            \n",
      "2023-02-07 01:43:00  1624.49  1624.6  1624.49  1624.6  23.077475\n",
      "2023-03-05 13:57:31.096442 Number of raw columns: 5\n",
      "2023-03-05 13:57:31.096533 Number of rows: 1631624\n",
      "covariate_blacklist = [['volume', 'close']]\n",
      "Feature columns: ['open', 'high', 'low']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset stats: DatasetStatistics(integer_dataset=False, max_target=4863.81, mean_abs_target=1681.1041273361004, mean_target=1681.1041273361004, mean_target_length=1631619.0, max_target_length=1631619, min_target=87.57, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=3, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=1631619, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts=defaultdict(<class 'int'>, {10: 1}), empty_target_count=0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset stats: DatasetStatistics(integer_dataset=False, max_target=4863.81, mean_abs_target=1681.1039545262884, mean_target=1681.1039545262884, mean_target_length=1631624.0, max_target_length=1631624, min_target=87.57, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=3, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=1631624, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts=defaultdict(<class 'int'>, {10: 1}), empty_target_count=0))\n",
      "2023-03-05 13:57:31.871474 Parsed train and test datasets can be found in [/Users/jbeckman/projects/capia/giia/out/datasets]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Parse dataset\n",
    "#\n",
    "\n",
    "from data_processing.parse import Parse\n",
    "\n",
    "PARSE = Parse(LOGGER)\n",
    "\n",
    "dataset_dir_path = local_artifact_dir / \"datasets\"\n",
    "\n",
    "# Creates train and test dataset\n",
    "PARSE.create_train_test_dataset(\n",
    "    dataset_dir_path,\n",
    "    filedataset_based=FILEDATASET_BASED,\n",
    "    one_dim_target=ONE_DIM_TARGET,\n",
    "    starting_date_truncate=\"2020-01-01 00:00:00\"\n",
    "    # starting_date_truncate=\"2021-03-01 00:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 13:57:40.602314 Data will be uploaded to [sagemaker-us-east-1-941048668662]\n",
      "2023-03-05 13:57:40.794672 File s3://sagemaker-us-east-1-941048668662/s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/datasets/metadata.json already exists.\n",
      "Set override to `True` to upload anyway.\n",
      "\n",
      "2023-03-05 13:57:40.794893 Data will be uploaded to [sagemaker-us-east-1-941048668662]\n",
      "2023-03-05 13:57:40.834826 File s3://sagemaker-us-east-1-941048668662/s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/datasets/train/data.feather already exists.\n",
      "Set override to `True` to upload anyway.\n",
      "\n",
      "2023-03-05 13:57:40.835135 Data will be uploaded to [sagemaker-us-east-1-941048668662]\n",
      "2023-03-05 13:57:40.874995 File s3://sagemaker-us-east-1-941048668662/s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/datasets/test/data.feather already exists.\n",
      "Set override to `True` to upload anyway.\n",
      "\n",
      "2023-03-05 13:57:40.875252 Model output dir is [s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/models]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup local/aws environment. If aws, upload the datasets to S3\n",
    "#\n",
    "\n",
    "from data_processing.aws_handler import AWSHandler\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "AWS_HANDLER = AWSHandler(LOGGER, config.MODEL_ID)\n",
    "\n",
    "sagemaker_session = None\n",
    "\n",
    "model_output_dir_path = local_artifact_dir / config.MODEL_ID / \"models\"\n",
    "model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    LOGGER.log(\"Notebook is set to local mode, not uploading to S3\")\n",
    "\n",
    "    dataset_dir_uri = f\"file://{dataset_dir_path}\"\n",
    "    model_output_dir_uri = f\"file://{model_output_dir_path}\"\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {\n",
    "        'local': {\n",
    "            'local_code': True,\n",
    "            'container_root': str(model_output_dir_path)\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    sagemaker_session = AWS_HANDLER.sagemaker_session\n",
    "\n",
    "    AWS_HANDLER.upload_train_datasets(dataset_dir_path, override=False, filedataset_based=FILEDATASET_BASED)\n",
    "    dataset_dir_uri = AWS_HANDLER.s3_dataset_dir_uri\n",
    "\n",
    "    model_output_dir_uri = AWS_HANDLER.s3_model_output_uri\n",
    "\n",
    "LOGGER.log(f\"Model output dir is [{model_output_dir_uri}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 19:23:19 Starting - Starting the training job...\n",
      "2023-03-05 19:23:43 Starting - Preparing the instances for trainingProfilerReport-1678044198: InProgress\n",
      ".........\n",
      "2023-03-05 19:25:19 Downloading - Downloading input data\n",
      "2023-03-05 19:25:19 Training - Downloading the training image..................\n",
      "2023-03-05 19:28:24 Training - Training image download completed. Training in progress.....\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2023-03-05 19:28:53,756 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2023-03-05 19:28:53,776 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2023-03-05 19:28:53,787 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2023-03-05 19:28:53,789 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:04,414 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[34mCollecting PyYAML==6.0\u001B[0m\n",
      "\u001B[34mDownloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 701.2/701.2 kB 28.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting jupyterlab==3.6.1\u001B[0m\n",
      "\u001B[34mDownloading jupyterlab-3.6.1-py3-none-any.whl (8.9 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 30.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting matplotlib==3.6.3\u001B[0m\n",
      "\u001B[34mDownloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/9.4 MB 119.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting boto3==1.24.59\u001B[0m\n",
      "\u001B[34mDownloading boto3-1.24.59-py3-none-any.whl (132 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 30.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting ipynbname==2021.3.2\u001B[0m\n",
      "\u001B[34mDownloading ipynbname-2021.3.2-py3-none-any.whl (4.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy==1.23.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.23.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas==1.5.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.5.3)\u001B[0m\n",
      "\u001B[34mCollecting s3fs==2023.1.0\u001B[0m\n",
      "\u001B[34mDownloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\u001B[0m\n",
      "\u001B[34mCollecting setuptools==67.1.0\u001B[0m\n",
      "\u001B[34mDownloading setuptools-67.1.0-py3-none-any.whl (1.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 84.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: wheel==0.38.4 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.38.4)\u001B[0m\n",
      "\u001B[34mCollecting sagemaker==2.111.0\u001B[0m\n",
      "\u001B[34mDownloading sagemaker-2.111.0.tar.gz (577 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 577.4/577.4 kB 74.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting jenkspy==0.3.2\u001B[0m\n",
      "\u001B[34mDownloading jenkspy-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (527 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.1/527.1 kB 59.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm==4.64.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (4.64.1)\u001B[0m\n",
      "\u001B[34mCollecting nbstripout==0.6.1\u001B[0m\n",
      "\u001B[34mDownloading nbstripout-0.6.1-py2.py3-none-any.whl (15 kB)\u001B[0m\n",
      "\u001B[34mCollecting typed-argument-parser==1.7.2\u001B[0m\n",
      "\u001B[34mDownloading typed-argument-parser-1.7.2.tar.gz (27 kB)\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting gluonts[pro,torch]==0.12.3\u001B[0m\n",
      "\u001B[34mDownloading gluonts-0.12.3-py3-none-any.whl (1.2 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 88.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: torch==1.12.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (1.12.1+cu113)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jinja2>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyterlab==3.6.1->-r requirements.txt (line 2)) (3.1.2)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-ydoc~=0.2.2\u001B[0m\n",
      "\u001B[34mDownloading jupyter_ydoc-0.2.2-py3-none-any.whl (5.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting nbclassic\u001B[0m\n",
      "\u001B[34mDownloading nbclassic-0.5.2-py3-none-any.whl (10.0 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 115.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from jupyterlab==3.6.1->-r requirements.txt (line 2)) (23.0)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-core\u001B[0m\n",
      "\u001B[34mDownloading jupyter_core-5.2.0-py3-none-any.whl (94 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.3/94.3 kB 8.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting notebook<7\u001B[0m\n",
      "\u001B[34mDownloading notebook-6.5.2-py3-none-any.whl (439 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.1/439.1 kB 34.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting jupyter-server-ydoc<0.7.0,>=0.6.0\u001B[0m\n",
      "\u001B[34mDownloading jupyter_server_ydoc-0.6.1-py3-none-any.whl (11 kB)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-server<3,>=1.16.0\u001B[0m\n",
      "\u001B[34mDownloading jupyter_server-2.3.0-py3-none-any.whl (365 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.8/365.8 kB 64.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: ipython in /opt/conda/lib/python3.8/site-packages (from jupyterlab==3.6.1->-r requirements.txt (line 2)) (8.10.0)\u001B[0m\n",
      "\u001B[34mCollecting jupyterlab-server~=2.19\u001B[0m\n",
      "\u001B[34mDownloading jupyterlab_server-2.19.0-py3-none-any.whl (56 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.4/56.4 kB 17.3 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting tomli\u001B[0m\n",
      "\u001B[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tornado>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from jupyterlab==3.6.1->-r requirements.txt (line 2)) (6.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (1.4.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (2.8.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (0.11.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (1.0.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (3.0.9)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (4.38.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.6.3->-r requirements.txt (line 3)) (9.4.0)\u001B[0m\n",
      "\u001B[34mCollecting botocore<1.28.0,>=1.27.59\u001B[0m\n",
      "\u001B[34mDownloading botocore-1.27.96-py3-none-any.whl (9.3 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 115.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3==1.24.59->-r requirements.txt (line 4)) (0.6.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3==1.24.59->-r requirements.txt (line 4)) (1.0.1)\u001B[0m\n",
      "\u001B[34mCollecting ipykernel\u001B[0m\n",
      "\u001B[34mDownloading ipykernel-6.21.2-py3-none-any.whl (149 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.7/149.7 kB 36.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.5.3->-r requirements.txt (line 7)) (2022.7.1)\u001B[0m\n",
      "\u001B[34mCollecting aiobotocore~=2.4.2\u001B[0m\n",
      "\u001B[34mDownloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 19.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\u001B[0m\n",
      "\u001B[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 82.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fsspec==2023.1.0 in /opt/conda/lib/python3.8/site-packages (from s3fs==2023.1.0->-r requirements.txt (line 8)) (2023.1.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (22.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (0.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (3.20.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (0.1.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (1.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (4.13.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (0.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker==2.111.0->-r requirements.txt (line 11)) (0.7.5)\u001B[0m\n",
      "\u001B[34mCollecting nbformat\u001B[0m\n",
      "\u001B[34mDownloading nbformat-5.7.3-py3-none-any.whl (78 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.1/78.1 kB 20.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing_extensions>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from typed-argument-parser==1.7.2->-r requirements.txt (line 15)) (4.4.0)\u001B[0m\n",
      "\u001B[34mCollecting typing-inspect>=0.7.1\u001B[0m\n",
      "\u001B[34mDownloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.8/site-packages (from gluonts[pro,torch]==0.12.3->-r requirements.txt (line 18)) (0.12.0)\u001B[0m\n",
      "\u001B[34mCollecting pydantic~=1.7\u001B[0m\n",
      "\u001B[34mDownloading pydantic-1.10.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 123.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting orjson\u001B[0m\n",
      "\u001B[34mDownloading orjson-3.8.7-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 kB 23.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting pyarrow~=8.0\u001B[0m\n",
      "\u001B[34mDownloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 64.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting pytorch-lightning~=1.5\u001B[0m\n",
      "\u001B[34mDownloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 827.8/827.8 kB 85.3 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting protobuf<4.0,>=3.1\u001B[0m\n",
      "\u001B[34mDownloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 98.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting botocore<1.28.0,>=1.27.59\u001B[0m\n",
      "\u001B[34mDownloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 128.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting wrapt>=1.10.10\u001B[0m\n",
      "\u001B[34mDownloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.5/81.5 kB 23.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting aioitertools>=0.5.1\u001B[0m\n",
      "\u001B[34mDownloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.1.0->-r requirements.txt (line 8)) (2.1.1)\u001B[0m\n",
      "\u001B[34mCollecting aiosignal>=1.1.2\u001B[0m\n",
      "\u001B[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting frozenlist>=1.1.1\u001B[0m\n",
      "\u001B[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 33.5 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting yarl<2.0,>=1.0\u001B[0m\n",
      "\u001B[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 44.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting async-timeout<5.0,>=4.0.0a3\u001B[0m\n",
      "\u001B[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001B[0m\n",
      "\u001B[34mCollecting multidict<7.0,>=4.5\u001B[0m\n",
      "\u001B[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 22.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.59->boto3==1.24.59->-r requirements.txt (line 4)) (1.26.14)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.111.0->-r requirements.txt (line 11)) (3.13.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.1->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.1.2)\u001B[0m\n",
      "\u001B[34mCollecting anyio>=3.1.0\u001B[0m\n",
      "\u001B[34mDownloading anyio-3.6.2-py3-none-any.whl (80 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 19.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting jupyter-server-terminals\u001B[0m\n",
      "\u001B[34mDownloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\u001B[0m\n",
      "\u001B[34mCollecting argon2-cffi\u001B[0m\n",
      "\u001B[34mDownloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\u001B[0m\n",
      "\u001B[34mCollecting nbconvert>=6.4.4\u001B[0m\n",
      "\u001B[34mDownloading nbconvert-7.2.9-py3-none-any.whl (274 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.9/274.9 kB 60.3 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting prometheus-client\u001B[0m\n",
      "\u001B[34mDownloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.5/122.5 kB 33.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting jupyter-events>=0.4.0\u001B[0m\n",
      "\u001B[34mDownloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\u001B[0m\n",
      "\u001B[34mCollecting pyzmq>=24\u001B[0m\n",
      "\u001B[34mDownloading pyzmq-25.0.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 102.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting send2trash\u001B[0m\n",
      "\u001B[34mDownloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\u001B[0m\n",
      "\u001B[34mCollecting terminado>=0.8.3\u001B[0m\n",
      "\u001B[34mDownloading terminado-0.17.1-py3-none-any.whl (17 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: traitlets>=5.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=1.16.0->jupyterlab==3.6.1->-r requirements.txt (line 2)) (5.9.0)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-client>=7.4.4\u001B[0m\n",
      "\u001B[34mDownloading jupyter_client-8.0.3-py3-none-any.whl (102 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.7/102.7 kB 24.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from jupyter-server<3,>=1.16.0->jupyterlab==3.6.1->-r requirements.txt (line 2)) (1.5.1)\u001B[0m\n",
      "\u001B[34mCollecting platformdirs>=2.5\u001B[0m\n",
      "\u001B[34mDownloading platformdirs-3.1.0-py3-none-any.whl (14 kB)\u001B[0m\n",
      "\u001B[34mCollecting jupyter-server-fileid<1,>=0.6.0\u001B[0m\n",
      "\u001B[34mDownloading jupyter_server_fileid-0.8.0-py3-none-any.whl (15 kB)\u001B[0m\n",
      "\u001B[34mCollecting ypy-websocket<0.9.0,>=0.8.2\u001B[0m\n",
      "\u001B[34mDownloading ypy_websocket-0.8.4-py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mCollecting y-py<0.6.0,>=0.5.3\u001B[0m\n",
      "\u001B[34mDownloading y_py-0.5.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 82.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting json5>=0.9.0\u001B[0m\n",
      "\u001B[34mDownloading json5-0.9.11-py2.py3-none-any.whl (19 kB)\u001B[0m\n",
      "\u001B[34mCollecting jsonschema>=4.17.3\u001B[0m\n",
      "\u001B[34mDownloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 3.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.8/site-packages (from jupyterlab-server~=2.19->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.28.2)\u001B[0m\n",
      "\u001B[34mCollecting babel>=2.10\u001B[0m\n",
      "\u001B[34mDownloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 36.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting fastjsonschema\u001B[0m\n",
      "\u001B[34mDownloading fastjsonschema-2.16.3-py3-none-any.whl (23 kB)\u001B[0m\n",
      "\u001B[34mCollecting ipython-genutils\u001B[0m\n",
      "\u001B[34mDownloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\u001B[0m\n",
      "\u001B[34mCollecting nest-asyncio>=1.5\u001B[0m\n",
      "\u001B[34mDownloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting notebook-shim>=0.1.0\u001B[0m\n",
      "\u001B[34mDownloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.111.0->-r requirements.txt (line 11)) (1.16.0)\u001B[0m\n",
      "\u001B[34mCollecting torchmetrics>=0.7.0\u001B[0m\n",
      "\u001B[34mDownloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 9.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting lightning-utilities>=0.6.0.post0\u001B[0m\n",
      "\u001B[34mDownloading lightning_utilities-0.7.1-py3-none-any.whl (18 kB)\u001B[0m\n",
      "\u001B[34mCollecting mypy-extensions>=0.3.0\u001B[0m\n",
      "\u001B[34mDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\u001B[0m\n",
      "\u001B[34mCollecting comm>=0.1.1\u001B[0m\n",
      "\u001B[34mDownloading comm-0.1.2-py3-none-any.whl (6.5 kB)\u001B[0m\n",
      "\u001B[34mCollecting debugpy>=1.6.5\u001B[0m\n",
      "\u001B[34mDownloading debugpy-1.6.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 29.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->ipynbname==2021.3.2->-r requirements.txt (line 5)) (5.9.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->ipynbname==2021.3.2->-r requirements.txt (line 5)) (0.1.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.18.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (3.0.36)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.6.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (4.8.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.7.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.14.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (5.1.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker==2.111.0->-r requirements.txt (line 11)) (1.7.6.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker==2.111.0->-r requirements.txt (line 11)) (0.3.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker==2.111.0->-r requirements.txt (line 11)) (0.3.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker==2.111.0->-r requirements.txt (line 11)) (0.70.14)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker==2.111.0->-r requirements.txt (line 11)) (21.6.0)\u001B[0m\n",
      "\u001B[34mCollecting sniffio>=1.1\u001B[0m\n",
      "\u001B[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab==3.6.1->-r requirements.txt (line 2)) (3.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.8.3)\u001B[0m\n",
      "\u001B[34mCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\u001B[0m\n",
      "\u001B[34mDownloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 1.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting pkgutil-resolve-name>=1.3.10\u001B[0m\n",
      "\u001B[34mDownloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab==3.6.1->-r requirements.txt (line 2)) (5.10.2)\u001B[0m\n",
      "\u001B[34mCollecting python-json-logger>=2.0.4\u001B[0m\n",
      "\u001B[34mDownloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\u001B[0m\n",
      "\u001B[34mCollecting rfc3986-validator>=0.1.1\u001B[0m\n",
      "\u001B[34mDownloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting rfc3339-validator\u001B[0m\n",
      "\u001B[34mDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\u001B[0m\n",
      "\u001B[34mCollecting defusedxml\u001B[0m\n",
      "\u001B[34mDownloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001B[0m\n",
      "\u001B[34mCollecting beautifulsoup4\u001B[0m\n",
      "\u001B[34mDownloading beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.4/129.4 kB 2.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting mistune<3,>=2.0.3\u001B[0m\n",
      "\u001B[34mDownloading mistune-2.0.5-py2.py3-none-any.whl (24 kB)\u001B[0m\n",
      "\u001B[34mCollecting pandocfilters>=1.4.1\u001B[0m\n",
      "\u001B[34mDownloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\u001B[0m\n",
      "\u001B[34mCollecting jupyterlab-pygments\u001B[0m\n",
      "\u001B[34mDownloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\u001B[0m\n",
      "\u001B[34mCollecting bleach\u001B[0m\n",
      "\u001B[34mDownloading bleach-6.0.0-py3-none-any.whl (162 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 3.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting tinycss2\u001B[0m\n",
      "\u001B[34mDownloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\u001B[0m\n",
      "\u001B[34mCollecting nbclient>=0.5.0\u001B[0m\n",
      "\u001B[34mDownloading nbclient-0.7.2-py3-none-any.whl (71 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 1.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.7.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.2.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.28->jupyterlab-server~=2.19->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2022.12.7)\u001B[0m\n",
      "\u001B[34mCollecting aiofiles<23,>=22.1.0\u001B[0m\n",
      "\u001B[34mDownloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\u001B[0m\n",
      "\u001B[34mCollecting aiosqlite<1,>=0.17.0\u001B[0m\n",
      "\u001B[34mDownloading aiosqlite-0.18.0-py3-none-any.whl (15 kB)\u001B[0m\n",
      "\u001B[34mCollecting ypy-websocket<0.9.0,>=0.8.2\u001B[0m\n",
      "\u001B[34mDownloading ypy_websocket-0.8.3-py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mDownloading ypy_websocket-0.8.2-py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mCollecting argon2-cffi-bindings\u001B[0m\n",
      "\u001B[34mDownloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 1.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (1.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.2.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython->jupyterlab==3.6.1->-r requirements.txt (line 2)) (0.2.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.3)\u001B[0m\n",
      "\u001B[34mCollecting fqdn\u001B[0m\n",
      "\u001B[34mDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\u001B[0m\n",
      "\u001B[34mCollecting uri-template\u001B[0m\n",
      "\u001B[34mDownloading uri_template-1.2.0-py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mCollecting webcolors>=1.11\u001B[0m\n",
      "\u001B[34mDownloading webcolors-1.12-py3-none-any.whl (9.9 kB)\u001B[0m\n",
      "\u001B[34mCollecting isoduration\u001B[0m\n",
      "\u001B[34mDownloading isoduration-20.11.0-py3-none-any.whl (11 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab==3.6.1->-r requirements.txt (line 2)) (1.15.1)\u001B[0m\n",
      "\u001B[34mCollecting soupsieve>1.2\u001B[0m\n",
      "\u001B[34mDownloading soupsieve-2.4-py3-none-any.whl (37 kB)\u001B[0m\n",
      "\u001B[34mCollecting webencodings\u001B[0m\n",
      "\u001B[34mDownloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab==3.6.1->-r requirements.txt (line 2)) (2.21)\u001B[0m\n",
      "\u001B[34mCollecting arrow>=0.15.0\u001B[0m\n",
      "\u001B[34mDownloading arrow-1.2.3-py3-none-any.whl (66 kB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 1.3 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: sagemaker, typed-argument-parser\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker (setup.py): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for sagemaker: filename=sagemaker-2.111.0-py2.py3-none-any.whl size=793028 sha256=f06b73518d2a9b837ff0cc53688f5e4012eaca1c4621fd6850d89176c7504d79\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/45/89/ba/395399028fac032ce574184ddf7fc648f7bb1aafc04e48d592\u001B[0m\n",
      "\u001B[34mBuilding wheel for typed-argument-parser (setup.py): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for typed-argument-parser (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for typed-argument-parser: filename=typed_argument_parser-1.7.2-py3-none-any.whl size=22685 sha256=756eabe43e6285b8e407082e9339866d4fc29e92c9a0102a53a7dddba420f39e\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/9e/b1/0b/8644d4fd3b0c5cec2005ec7c5217466f5429c629cf300f72be\u001B[0m\n",
      "\u001B[34mSuccessfully built sagemaker typed-argument-parser\u001B[0m\n",
      "\u001B[34mInstalling collected packages: y-py, webencodings, send2trash, mistune, json5, ipython-genutils, fastjsonschema, wrapt, webcolors, uri-template, tomli, tinycss2, terminado, soupsieve, sniffio, setuptools, rfc3986-validator, rfc3339-validator, pyzmq, PyYAML, python-json-logger, pyrsistent, pydantic, pyarrow, protobuf, prometheus-client, platformdirs, pkgutil-resolve-name, pandocfilters, orjson, nest-asyncio, mypy-extensions, multidict, lightning-utilities, jupyterlab-pygments, jenkspy, frozenlist, fqdn, defusedxml, debugpy, comm, bleach, babel, async-timeout, aiosqlite, aioitertools, aiofiles, ypy-websocket, yarl, typing-inspect, torchmetrics, matplotlib, jupyter-ydoc, jupyter-server-terminals, jupyter-core, jsonschema, botocore, beautifulsoup4, arrow, argon2-cffi-bindings, anyio, aiosignal, typed-argument-parser, nbformat, jupyter-client, isoduration, gluonts, argon2-cffi, aiohttp, nbstripout, nbclient, ipykernel, boto3, aiobotocore, sagemaker, s3fs, pytorch-lightning, nbconvert, jupyter-events, ipynbname, jupyter-server, notebook-shim, jupyterlab-server, jupyter-server-fileid, nbclassic, jupyter-server-ydoc, notebook, jupyterlab\u001B[0m\n",
      "\u001B[34mAttempting uninstall: setuptools\u001B[0m\n",
      "\u001B[34mFound existing installation: setuptools 65.6.3\u001B[0m\n",
      "\u001B[34mUninstalling setuptools-65.6.3:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled setuptools-65.6.3\u001B[0m\n",
      "\u001B[34mAttempting uninstall: PyYAML\u001B[0m\n",
      "\u001B[34mFound existing installation: PyYAML 5.4.1\u001B[0m\n",
      "\u001B[34mUninstalling PyYAML-5.4.1:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled PyYAML-5.4.1\u001B[0m\n",
      "\u001B[34mAttempting uninstall: pyarrow\u001B[0m\n",
      "\u001B[34mFound existing installation: pyarrow 11.0.0\u001B[0m\n",
      "\u001B[34mUninstalling pyarrow-11.0.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled pyarrow-11.0.0\u001B[0m\n",
      "\u001B[34mAttempting uninstall: protobuf\u001B[0m\n",
      "\u001B[34mFound existing installation: protobuf 3.20.2\u001B[0m\n",
      "\u001B[34mUninstalling protobuf-3.20.2:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled protobuf-3.20.2\u001B[0m\n",
      "\u001B[34mAttempting uninstall: matplotlib\u001B[0m\n",
      "\u001B[34mFound existing installation: matplotlib 3.7.0\u001B[0m\n",
      "\u001B[34mUninstalling matplotlib-3.7.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled matplotlib-3.7.0\u001B[0m\n",
      "\u001B[34mAttempting uninstall: botocore\u001B[0m\n",
      "\u001B[34mFound existing installation: botocore 1.29.70\u001B[0m\n",
      "\u001B[34mUninstalling botocore-1.29.70:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled botocore-1.29.70\u001B[0m\n",
      "\u001B[34mAttempting uninstall: boto3\u001B[0m\n",
      "\u001B[34mFound existing installation: boto3 1.26.70\u001B[0m\n",
      "\u001B[34mUninstalling boto3-1.26.70:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled boto3-1.26.70\u001B[0m\n",
      "\u001B[34mAttempting uninstall: sagemaker\u001B[0m\n",
      "\u001B[34mFound existing installation: sagemaker 2.132.0\u001B[0m\n",
      "\u001B[34mUninstalling sagemaker-2.132.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled sagemaker-2.132.0\u001B[0m\n",
      "\u001B[34mAttempting uninstall: s3fs\u001B[0m\n",
      "\u001B[34mFound existing installation: s3fs 0.4.2\u001B[0m\n",
      "\u001B[34mUninstalling s3fs-0.4.2:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled s3fs-0.4.2\u001B[0m\n",
      "\u001B[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001B[0m\n",
      "\u001B[34mawscli 1.27.70 requires botocore==1.29.70, but you have botocore 1.27.59 which is incompatible.\u001B[0m\n",
      "\u001B[34mawscli 1.27.70 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001B[0m\n",
      "\u001B[34mSuccessfully installed PyYAML-6.0 aiobotocore-2.4.2 aiofiles-22.1.0 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 aiosqlite-0.18.0 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 async-timeout-4.0.2 babel-2.12.1 beautifulsoup4-4.11.2 bleach-6.0.0 boto3-1.24.59 botocore-1.27.59 comm-0.1.2 debugpy-1.6.6 defusedxml-0.7.1 fastjsonschema-2.16.3 fqdn-1.5.1 frozenlist-1.3.3 gluonts-0.12.3 ipykernel-6.21.2 ipynbname-2021.3.2 ipython-genutils-0.2.0 isoduration-20.11.0 jenkspy-0.3.2 json5-0.9.11 jsonschema-4.17.3 jupyter-client-8.0.3 jupyter-core-5.2.0 jupyter-events-0.6.3 jupyter-server-2.3.0 jupyter-server-fileid-0.8.0 jupyter-server-terminals-0.4.4 jupyter-server-ydoc-0.6.1 jupyter-ydoc-0.2.2 jupyterlab-3.6.1 jupyterlab-pygments-0.2.2 jupyterlab-server-2.19.0 lightning-utilities-0.7.1 matplotlib-3.6.3 mistune-2.0.5 multidict-6.0.4 mypy-extensions-1.0.0 nbclassic-0.5.2 nbclient-0.7.2 nbconvert-7.2.9 nbformat-5.7.3 nbstripout-0.6.1 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 orjson-3.8.7 pandocfilters-1.5.0 pkgutil-resolve-name-1.3.10 platformdirs-3.1.0 prometheus-client-0.16.0 protobuf-3.19.6 pyarrow-8.0.0 pydantic-1.10.5 pyrsistent-0.19.3 python-json-logger-2.0.7 pytorch-lightning-1.9.4 pyzmq-25.0.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3fs-2023.1.0 sagemaker-2.111.0 send2trash-1.8.0 setuptools-67.1.0 sniffio-1.3.0 soupsieve-2.4 terminado-0.17.1 tinycss2-1.2.1 tomli-2.0.1 torchmetrics-0.11.3 typed-argument-parser-1.7.2 typing-inspect-0.8.0 uri-template-1.2.0 webcolors-1.12 webencodings-0.5.1 wrapt-1.15.0 y-py-0.5.9 yarl-1.8.2 ypy-websocket-0.8.2\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001B[0m\n",
      "\u001B[34m[notice] To update, run: pip install --upgrade pip\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,648 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,648 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,671 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,703 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,735 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:34,746 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dataset\": \"/opt/ml/input/data/dataset\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.8xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dataset\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.8xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-03-05-19-21-31-227\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/models/pytorch-training-2023-03-05-19-21-31-227/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_entry_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_entry_train.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=sm_entry_train.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"dataset\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.8xlarge\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.8xlarge\"}}\u001B[0m\n",
      "\u001B[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001B[0m\n",
      "\u001B[34mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=sm_entry_train\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=32\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=1\u001B[0m\n",
      "\u001B[34mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/models/pytorch-training-2023-03-05-19-21-31-227/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dataset\":\"/opt/ml/input/data/dataset\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.8xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.8xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-05-19-21-31-227\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941048668662/giia-tft_torch-1.1.2/models/pytorch-training-2023-03-05-19-21-31-227/source/sourcedir.tar.gz\",\"module_name\":\"sm_entry_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_entry_train.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_DATASET=/opt/ml/input/data/dataset\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.8 sm_entry_train.py\u001B[0m\n",
      "\u001B[34m2023-03-05 19:29:37,126 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[34mUsing model [<module 'ml.models.tft_torch' from '/opt/ml/code/ml/models/tft_torch.py'>]\u001B[0m\n",
      "\u001B[34mUsing the follow arguments: [{'ar_window': 32,\n",
      " 'batch_size': 64,\n",
      " 'channels': 128,\n",
      " 'context_length': 60,\n",
      " 'dataset_dir': '/opt/ml/input/data/dataset',\n",
      " 'distr_output': 'StudentTOutput',\n",
      " 'dropout_rate': 0.1525,\n",
      " 'epochs': 3,\n",
      " 'hidden_dim': 256,\n",
      " 'kernel_size': 6,\n",
      " 'learning_rate': 0.001,\n",
      " 'model_dim': 2048,\n",
      " 'model_dir': '/opt/ml/model',\n",
      " 'model_type': 'tft_torch',\n",
      " 'n_hidden_layer': 10,\n",
      " 'n_neurons_per_layer': 512,\n",
      " 'num_batches_per_epoch': 500,\n",
      " 'num_cells': 256,\n",
      " 'num_heads': 32,\n",
      " 'num_layers': 8,\n",
      " 'prediction_length': 5,\n",
      " 'rnn_num_layers': 128,\n",
      " 'skip_rnn_num_layers': 4,\n",
      " 'skip_size': 32,\n",
      " 'variable_dim': 256}]\u001B[0m\n",
      "\u001B[34mThe model id is [giia-tft_torch-1.1.2]\u001B[0m\n",
      "\u001B[34mThe PyTorch version is [1.12.1+cu113]\u001B[0m\n",
      "\u001B[34mThe GluonTS version is [0.12.3]\u001B[0m\n",
      "\u001B[34mThe GPU count is [1]\u001B[0m\n",
      "\u001B[34mUsing GPU context\u001B[0m\n",
      "\u001B[34mnvidia-smi\u001B[0m\n",
      "\u001B[34mSun Mar  5 19:29:39 2023       \u001B[0m\n",
      "\u001B[34m+-----------------------------------------------------------------------------+\u001B[0m\n",
      "\u001B[34m| NVIDIA-SMI 515.65.07    Driver Version: 515.65.07    CUDA Version: 11.7     |\u001B[0m\n",
      "\u001B[34m|-------------------------------+----------------------+----------------------+\u001B[0m\n",
      "\u001B[34m| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\u001B[0m\n",
      "\u001B[34m| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\u001B[0m\n",
      "\u001B[34m|                               |                      |               MIG M. |\u001B[0m\n",
      "\u001B[34m|===============================+======================+======================|\u001B[0m\n",
      "\u001B[34m|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\u001B[0m\n",
      "\u001B[34m| N/A   27C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\u001B[0m\n",
      "\u001B[34m|                               |                      |                  N/A |\u001B[0m\n",
      "\u001B[34m+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \u001B[0m\n",
      "\u001B[34m+-----------------------------------------------------------------------------+\u001B[0m\n",
      "\u001B[34m| Processes:                                                                  |\u001B[0m\n",
      "\u001B[34m|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\u001B[0m\n",
      "\u001B[34m|        ID   ID                                                   Usage      |\u001B[0m\n",
      "\u001B[34m|=============================================================================|\u001B[0m\n",
      "\u001B[34m|  No running processes found                                                 |\u001B[0m\n",
      "\u001B[34m+-----------------------------------------------------------------------------+\u001B[0m\n",
      "\u001B[34mnvcc --version\u001B[0m\n",
      "\u001B[34mnvcc: NVIDIA (R) Cuda compiler driver\u001B[0m\n",
      "\u001B[34mCopyright (c) 2005-2021 NVIDIA Corporation\u001B[0m\n",
      "\u001B[34mBuilt on Mon_May__3_19:15:13_PDT_2021\u001B[0m\n",
      "\u001B[34mCuda compilation tools, release 11.3, V11.3.109\u001B[0m\n",
      "\u001B[34mBuild cuda_11.3.r11.3/compiler.29920130_0\u001B[0m\n",
      "\u001B[34mDataset directory and files:\u001B[0m\n",
      "\u001B[34m/opt/ml/input/data/dataset/test\u001B[0m\n",
      "\u001B[34m/opt/ml/input/data/dataset/metadata.json\u001B[0m\n",
      "\u001B[34m/opt/ml/input/data/dataset/train\u001B[0m\n",
      "\u001B[34m/opt/ml/input/data/dataset/test/data.feather\u001B[0m\n",
      "\u001B[34m/opt/ml/input/data/dataset/train/data.feather\u001B[0m\n",
      "\u001B[34m0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\u001B[0m\n",
      "\u001B[34mTrain dataset stats: DatasetStatistics(integer_dataset=False, max_target=4863.81005859375, mean_abs_target=1681.10383612841, mean_target=1681.10383612841, mean_target_length=1631619.0, max_target_length=1631619, min_target=87.56999969482422, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=3, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=1631619, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts=defaultdict(<class 'int'>, {10: 1}), empty_target_count=0))\u001B[0m\n",
      "\u001B[34m0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\u001B[0m\n",
      "\u001B[34mTest dataset stats: DatasetStatistics(integer_dataset=False, max_target=4863.81005859375, mean_abs_target=1681.1037052654287, mean_target=1681.1037052654287, mean_target_length=1631624.0, max_target_length=1631624, min_target=87.56999969482422, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=3, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=1631624, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts=defaultdict(<class 'int'>, {10: 1}), empty_target_count=0))\u001B[0m\n",
      "\u001B[34mNote that the max number of samples from the training dataset that can be used is [96000]. This should be larger than the number of samples in the training dataset [1631619] or you risk not using the full dataset.\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.438 algo-1:143 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.584 algo-1:143 INFO profiler_config_parser.py:111] User has disabled profiler.\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.585 algo-1:143 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.585 algo-1:143 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.586 algo-1:143 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001B[0m\n",
      "\u001B[34m[2023-03-05 19:29:41.586 algo-1:143 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\u001B[0m\n",
      "\u001B[34mGPU available: True (cuda), used: True\u001B[0m\n",
      "\u001B[34mTPU available: False, using: 0 TPU cores\u001B[0m\n",
      "\u001B[34mIPU available: False, using: 0 IPUs\u001B[0m\n",
      "\u001B[34mHPU available: False, using: 0 HPUs\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\u001B[0m\n",
      "\u001B[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001B[0m\n",
      "\u001B[34m| Name  | Type                           | Params | In sizes                                                                           | Out sizes\u001B[0m\n",
      "\u001B[34m----------------------------------------------------------------------------------------------------------------------------------------------------------\u001B[0m\n",
      "\u001B[34m0 | model | TemporalFusionTransformerModel | 10.0 M | [[1, 60], [1, 60], [1, 1], [1, 1], [1, 65, 8], [1, 65, 0], [1, 60, 0], [1, 60, 0]] | [1, 3, 5]\u001B[0m\n",
      "\u001B[34m----------------------------------------------------------------------------------------------------------------------------------------------------------\u001B[0m\n",
      "\u001B[34m10.0 M    Trainable params\u001B[0m\n",
      "\u001B[34m0         Non-trainable params\u001B[0m\n",
      "\u001B[34m10.0 M    Total params\u001B[0m\n",
      "\u001B[34m39.993    Total estimated model params size (MB)\u001B[0m\n",
      "\u001B[34mSanity Checking: 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mSanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 22.22it/s]\u001B[0m\n",
      "\u001B[34mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 26.34it/s]\u001B[0m\n",
      "\u001B[34mTraining: 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining: 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 1it [00:02,  2.44s/it]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 1it [00:02,  2.44s/it, loss=11.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 2it [00:02,  1.28s/it, loss=11.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 2it [00:02,  1.28s/it, loss=12.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 3it [00:05,  1.86s/it, loss=12.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 3it [00:05,  1.86s/it, loss=13.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 4it [00:05,  1.42s/it, loss=13.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 4it [00:05,  1.42s/it, loss=12.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 5it [00:05,  1.16s/it, loss=12.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 5it [00:05,  1.16s/it, loss=13.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 6it [00:05,  1.01it/s, loss=13.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 6it [00:05,  1.01it/s, loss=14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 7it [00:06,  1.16it/s, loss=14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 7it [00:06,  1.16it/s, loss=14.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 8it [00:06,  1.30it/s, loss=14.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 8it [00:06,  1.30it/s, loss=14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 9it [00:06,  1.43it/s, loss=14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 9it [00:06,  1.43it/s, loss=13.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 10it [00:06,  1.56it/s, loss=13.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 10it [00:06,  1.56it/s, loss=13.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 11it [00:06,  1.68it/s, loss=13.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 11it [00:06,  1.68it/s, loss=13.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 12it [00:06,  1.80it/s, loss=13.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 12it [00:06,  1.80it/s, loss=13.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 13it [00:06,  1.91it/s, loss=13.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 13it [00:06,  1.91it/s, loss=13.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 14it [00:06,  2.02it/s, loss=13.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 14it [00:06,  2.02it/s, loss=13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 15it [00:07,  2.13it/s, loss=13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 15it [00:07,  2.13it/s, loss=12.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 16it [00:07,  2.23it/s, loss=12.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 16it [00:07,  2.23it/s, loss=12.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 17it [00:07,  2.33it/s, loss=12.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 17it [00:07,  2.33it/s, loss=12.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 18it [00:07,  2.42it/s, loss=12.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 18it [00:07,  2.42it/s, loss=12.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 19it [00:07,  2.51it/s, loss=12.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 19it [00:07,  2.51it/s, loss=12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 20it [00:07,  2.61it/s, loss=12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 20it [00:07,  2.61it/s, loss=11.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 21it [00:07,  2.71it/s, loss=11.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 21it [00:07,  2.71it/s, loss=11.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 22it [00:07,  2.78it/s, loss=11.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 22it [00:07,  2.78it/s, loss=11.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 23it [00:08,  2.87it/s, loss=11.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 23it [00:08,  2.87it/s, loss=11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 24it [00:08,  2.95it/s, loss=11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 24it [00:08,  2.95it/s, loss=11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 25it [00:08,  3.03it/s, loss=11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 25it [00:08,  3.03it/s, loss=10.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 26it [00:08,  3.10it/s, loss=10.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 26it [00:08,  3.10it/s, loss=10.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 27it [00:08,  3.17it/s, loss=10.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 27it [00:08,  3.17it/s, loss=9.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 28it [00:08,  3.25it/s, loss=9.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 28it [00:08,  3.25it/s, loss=9.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 29it [00:08,  3.32it/s, loss=9.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 29it [00:08,  3.32it/s, loss=9.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 30it [00:08,  3.40it/s, loss=9.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 30it [00:08,  3.40it/s, loss=9.16, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 31it [00:08,  3.47it/s, loss=9.16, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 31it [00:08,  3.47it/s, loss=8.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 32it [00:36,  1.14s/it, loss=8.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 32it [00:36,  1.14s/it, loss=8.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 33it [00:36,  1.11s/it, loss=8.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 33it [00:36,  1.11s/it, loss=8.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 34it [00:46,  1.38s/it, loss=8.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 34it [00:46,  1.38s/it, loss=7.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 35it [00:47,  1.36s/it, loss=7.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 35it [00:47,  1.36s/it, loss=7.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 36it [00:47,  1.33s/it, loss=7.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 36it [00:47,  1.33s/it, loss=7.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 37it [00:47,  1.30s/it, loss=7.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 37it [00:47,  1.30s/it, loss=7.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 38it [00:48,  1.27s/it, loss=7.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 38it [00:48,  1.27s/it, loss=7.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 39it [00:48,  1.24s/it, loss=7.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 39it [00:48,  1.24s/it, loss=7.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 40it [01:09,  1.73s/it, loss=7.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 40it [01:09,  1.73s/it, loss=7.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 41it [01:09,  1.70s/it, loss=7.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 41it [01:09,  1.70s/it, loss=7.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 42it [01:09,  1.66s/it, loss=7.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 42it [01:09,  1.66s/it, loss=7.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 43it [01:09,  1.63s/it, loss=7.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 43it [01:09,  1.63s/it, loss=6.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 44it [01:10,  1.59s/it, loss=6.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 44it [01:10,  1.59s/it, loss=6.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 45it [01:18,  1.75s/it, loss=6.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 45it [01:18,  1.75s/it, loss=6.77, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 46it [01:18,  1.72s/it, loss=6.77, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 46it [01:18,  1.72s/it, loss=6.64, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 47it [01:19,  1.68s/it, loss=6.64, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 47it [01:19,  1.68s/it, loss=6.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 48it [01:19,  1.65s/it, loss=6.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 48it [01:19,  1.65s/it, loss=6.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 49it [01:19,  1.62s/it, loss=6.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 49it [01:19,  1.62s/it, loss=6.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 50it [01:19,  1.59s/it, loss=6.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 50it [01:19,  1.59s/it, loss=6.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 51it [01:19,  1.57s/it, loss=6.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 51it [01:19,  1.57s/it, loss=6.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 52it [01:20,  1.54s/it, loss=6.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 52it [01:20,  1.54s/it, loss=6.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 53it [01:20,  1.51s/it, loss=6.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 53it [01:20,  1.51s/it, loss=6.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 54it [01:20,  1.49s/it, loss=6.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 54it [01:20,  1.49s/it, loss=6.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 55it [01:20,  1.46s/it, loss=6.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 55it [01:20,  1.46s/it, loss=6.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 56it [01:20,  1.44s/it, loss=6.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 56it [01:20,  1.44s/it, loss=6.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 57it [01:20,  1.42s/it, loss=6.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 57it [01:20,  1.42s/it, loss=6.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 58it [01:21,  1.40s/it, loss=6.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 58it [01:21,  1.40s/it, loss=6.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 59it [01:21,  1.38s/it, loss=6.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 59it [01:21,  1.38s/it, loss=6.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 60it [01:21,  1.36s/it, loss=6.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 60it [01:21,  1.36s/it, loss=6.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 61it [01:21,  1.34s/it, loss=6.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 61it [01:21,  1.34s/it, loss=6.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 62it [01:21,  1.32s/it, loss=6.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 62it [01:21,  1.32s/it, loss=6.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 63it [01:21,  1.30s/it, loss=6.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 63it [01:21,  1.30s/it, loss=6.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 64it [02:51,  2.68s/it, loss=6.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 64it [02:51,  2.68s/it, loss=6.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 65it [02:51,  2.64s/it, loss=6.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 65it [02:51,  2.64s/it, loss=6.17, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 66it [03:26,  3.13s/it, loss=6.17, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 66it [03:26,  3.13s/it, loss=6.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 67it [03:26,  3.08s/it, loss=6.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 67it [03:26,  3.08s/it, loss=6.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 68it [03:26,  3.04s/it, loss=6.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 68it [03:26,  3.04s/it, loss=6.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 69it [03:26,  3.00s/it, loss=6.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 69it [03:26,  3.00s/it, loss=6.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 70it [03:27,  2.96s/it, loss=6.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 70it [03:27,  2.96s/it, loss=5.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 71it [03:27,  2.92s/it, loss=5.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 71it [03:27,  2.92s/it, loss=5.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 72it [03:32,  2.95s/it, loss=5.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 72it [03:32,  2.95s/it, loss=6.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 73it [03:32,  2.92s/it, loss=6.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 73it [03:32,  2.92s/it, loss=5.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 74it [03:33,  2.88s/it, loss=5.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 74it [03:33,  2.88s/it, loss=5.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 75it [03:33,  2.84s/it, loss=5.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 75it [03:33,  2.84s/it, loss=5.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 76it [03:33,  2.81s/it, loss=5.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 76it [03:33,  2.81s/it, loss=5.85, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 77it [03:33,  2.77s/it, loss=5.85, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 77it [03:33,  2.77s/it, loss=5.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 78it [03:33,  2.74s/it, loss=5.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 78it [03:33,  2.74s/it, loss=5.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 79it [03:33,  2.71s/it, loss=5.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 79it [03:33,  2.71s/it, loss=5.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 80it [03:34,  2.68s/it, loss=5.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 80it [03:34,  2.68s/it, loss=5.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 81it [03:34,  2.64s/it, loss=5.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 81it [03:34,  2.64s/it, loss=5.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 82it [03:34,  2.61s/it, loss=5.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 82it [03:34,  2.61s/it, loss=5.72, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 83it [03:34,  2.58s/it, loss=5.72, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 83it [03:34,  2.58s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 84it [03:34,  2.55s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 84it [03:34,  2.55s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 85it [03:34,  2.53s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 85it [03:34,  2.53s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 86it [03:34,  2.50s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 86it [03:34,  2.50s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 87it [03:34,  2.47s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 87it [03:34,  2.47s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 88it [03:34,  2.44s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 88it [03:34,  2.44s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 89it [03:35,  2.42s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 89it [03:35,  2.42s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 90it [03:35,  2.39s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 90it [03:35,  2.39s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 91it [03:35,  2.37s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 91it [03:35,  2.37s/it, loss=5.73, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 92it [03:35,  2.34s/it, loss=5.73, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 92it [03:35,  2.34s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 93it [03:35,  2.32s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 93it [03:35,  2.32s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 94it [03:35,  2.29s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 94it [03:35,  2.29s/it, loss=5.57, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 95it [03:35,  2.27s/it, loss=5.57, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 95it [03:35,  2.27s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 96it [04:33,  2.84s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 96it [04:33,  2.84s/it, loss=5.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 97it [04:33,  2.82s/it, loss=5.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 97it [04:33,  2.82s/it, loss=5.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 98it [05:13,  3.20s/it, loss=5.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 98it [05:13,  3.20s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 99it [05:13,  3.17s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 99it [05:13,  3.17s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 100it [05:13,  3.14s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 100it [05:13,  3.14s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 101it [05:14,  3.11s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 101it [05:14,  3.11s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 102it [05:14,  3.08s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 102it [05:14,  3.08s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 103it [05:17,  3.09s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 103it [05:18,  3.09s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 104it [05:18,  3.06s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 104it [05:18,  3.06s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 105it [05:18,  3.03s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 105it [05:18,  3.03s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 106it [05:18,  3.00s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 106it [05:18,  3.00s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 107it [05:18,  2.98s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 107it [05:18,  2.98s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 108it [05:18,  2.95s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 108it [05:18,  2.95s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 109it [05:19,  2.93s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 109it [05:19,  2.93s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 110it [05:19,  2.91s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 110it [05:19,  2.91s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 111it [05:19,  2.88s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 111it [05:19,  2.88s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 112it [05:20,  2.86s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 112it [05:20,  2.86s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 113it [05:20,  2.83s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 113it [05:20,  2.83s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 114it [05:21,  2.82s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 114it [05:21,  2.82s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 115it [05:21,  2.79s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 115it [05:21,  2.79s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 116it [05:21,  2.77s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 116it [05:21,  2.77s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 117it [05:21,  2.75s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 117it [05:21,  2.75s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 118it [05:21,  2.73s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 118it [05:21,  2.73s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 119it [05:22,  2.71s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 119it [05:22,  2.71s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 120it [05:22,  2.69s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 120it [05:22,  2.69s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 121it [05:22,  2.66s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 121it [05:22,  2.66s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 122it [05:22,  2.64s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 122it [05:22,  2.64s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 123it [05:22,  2.62s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 123it [05:22,  2.62s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 124it [05:22,  2.60s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 124it [05:22,  2.60s/it, loss=5.47, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 125it [05:23,  2.58s/it, loss=5.47, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 125it [05:23,  2.58s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 126it [05:23,  2.56s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 126it [05:23,  2.56s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 127it [05:23,  2.55s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 127it [05:23,  2.55s/it, loss=5.57, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 128it [06:26,  3.02s/it, loss=5.57, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 128it [06:26,  3.02s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 129it [06:27,  3.00s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 129it [06:27,  3.00s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 130it [07:16,  3.36s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 130it [07:16,  3.36s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 131it [07:17,  3.34s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 131it [07:17,  3.34s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 132it [07:17,  3.31s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 132it [07:17,  3.31s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 133it [07:17,  3.29s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 133it [07:17,  3.29s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 134it [07:17,  3.27s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 134it [07:17,  3.27s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 135it [07:19,  3.26s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 135it [07:19,  3.26s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 136it [07:19,  3.23s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 136it [07:19,  3.23s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 137it [07:19,  3.21s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 137it [07:19,  3.21s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 138it [07:19,  3.19s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 138it [07:19,  3.19s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 139it [07:20,  3.17s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 139it [07:20,  3.17s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 140it [07:20,  3.15s/it, loss=5.54, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 140it [07:20,  3.15s/it, loss=5.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 141it [07:20,  3.12s/it, loss=5.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 141it [07:20,  3.12s/it, loss=5.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 142it [07:20,  3.10s/it, loss=5.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 142it [07:20,  3.10s/it, loss=5.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 143it [07:20,  3.08s/it, loss=5.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 143it [07:20,  3.08s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 144it [07:21,  3.06s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 144it [07:21,  3.06s/it, loss=5.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 145it [07:21,  3.04s/it, loss=5.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 145it [07:21,  3.04s/it, loss=5.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 146it [07:21,  3.02s/it, loss=5.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 146it [07:21,  3.02s/it, loss=5.75, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 147it [07:21,  3.00s/it, loss=5.75, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 147it [07:21,  3.00s/it, loss=5.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 148it [07:21,  2.98s/it, loss=5.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 148it [07:21,  2.98s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 149it [07:21,  2.97s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 149it [07:21,  2.97s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 150it [07:21,  2.95s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 150it [07:21,  2.95s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 151it [07:22,  2.93s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 151it [07:22,  2.93s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 152it [07:22,  2.91s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 152it [07:22,  2.91s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 153it [07:22,  2.89s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 153it [07:22,  2.89s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 154it [07:22,  2.87s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 154it [07:22,  2.87s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 155it [07:22,  2.86s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 155it [07:22,  2.86s/it, loss=5.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 156it [07:22,  2.84s/it, loss=5.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 156it [07:22,  2.84s/it, loss=5.64, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 157it [07:22,  2.82s/it, loss=5.64, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 157it [07:22,  2.82s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 158it [07:22,  2.80s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 158it [07:22,  2.80s/it, loss=5.75, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 159it [07:23,  2.79s/it, loss=5.75, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 159it [07:23,  2.79s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 160it [08:41,  3.26s/it, loss=5.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 160it [08:41,  3.26s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 161it [08:41,  3.24s/it, loss=5.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 161it [08:41,  3.24s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 162it [10:00,  3.71s/it, loss=5.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 162it [10:00,  3.71s/it, loss=5.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 163it [10:00,  3.68s/it, loss=5.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 163it [10:00,  3.68s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 164it [10:00,  3.66s/it, loss=5.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 164it [10:00,  3.66s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 165it [10:00,  3.64s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 165it [10:00,  3.64s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 166it [10:00,  3.62s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 166it [10:00,  3.62s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 167it [10:00,  3.60s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 167it [10:00,  3.60s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 168it [10:00,  3.58s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 168it [10:00,  3.58s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 169it [10:00,  3.56s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 169it [10:00,  3.56s/it, loss=5.33, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 170it [10:01,  3.54s/it, loss=5.33, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 170it [10:01,  3.54s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 171it [10:01,  3.52s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 171it [10:01,  3.52s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 172it [10:01,  3.50s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 172it [10:01,  3.50s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 173it [10:01,  3.48s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 173it [10:01,  3.48s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 174it [10:01,  3.46s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 174it [10:01,  3.46s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 175it [10:01,  3.44s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 175it [10:01,  3.44s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 176it [10:01,  3.42s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 176it [10:01,  3.42s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 177it [10:01,  3.40s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 177it [10:01,  3.40s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 178it [10:01,  3.38s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 178it [10:01,  3.38s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 179it [10:02,  3.36s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 179it [10:02,  3.36s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 180it [10:02,  3.35s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 180it [10:02,  3.35s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 181it [10:02,  3.33s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 181it [10:02,  3.33s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 182it [10:02,  3.31s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 182it [10:02,  3.31s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 183it [10:02,  3.29s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 183it [10:02,  3.29s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 184it [10:02,  3.27s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 184it [10:02,  3.27s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 185it [10:02,  3.26s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 185it [10:02,  3.26s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 186it [10:02,  3.24s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 186it [10:02,  3.24s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 187it [10:02,  3.22s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 187it [10:02,  3.22s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 188it [10:02,  3.21s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 188it [10:02,  3.21s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 189it [10:03,  3.19s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 189it [10:03,  3.19s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 190it [10:03,  3.17s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 190it [10:03,  3.17s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 191it [10:03,  3.16s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 191it [10:03,  3.16s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 192it [10:55,  3.41s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 192it [10:55,  3.41s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 193it [10:55,  3.40s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 193it [10:55,  3.40s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 194it [11:26,  3.54s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 194it [11:26,  3.54s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 195it [11:26,  3.52s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 195it [11:26,  3.52s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 196it [11:26,  3.50s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 196it [11:26,  3.50s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 197it [11:26,  3.49s/it, loss=5.53, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 197it [11:26,  3.49s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 198it [11:27,  3.47s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 198it [11:27,  3.47s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 199it [11:27,  3.45s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 199it [11:27,  3.45s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 200it [11:27,  3.44s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 200it [11:27,  3.44s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 201it [11:27,  3.42s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 201it [11:27,  3.42s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 202it [11:27,  3.40s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 202it [11:27,  3.40s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 203it [11:27,  3.39s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 203it [11:27,  3.39s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 204it [11:27,  3.37s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 204it [11:27,  3.37s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 205it [11:39,  3.41s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 205it [11:39,  3.41s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 206it [11:39,  3.40s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 206it [11:39,  3.40s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 207it [11:39,  3.38s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 207it [11:39,  3.38s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 208it [11:40,  3.37s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 208it [11:40,  3.37s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 209it [11:40,  3.35s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 209it [11:40,  3.35s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 210it [11:40,  3.33s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 210it [11:40,  3.33s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 211it [11:40,  3.32s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 211it [11:40,  3.32s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 212it [11:40,  3.30s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 212it [11:40,  3.30s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 213it [11:40,  3.29s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 213it [11:40,  3.29s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 214it [11:40,  3.27s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 214it [11:40,  3.27s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 215it [11:40,  3.26s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 215it [11:40,  3.26s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 216it [11:41,  3.25s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 216it [11:41,  3.25s/it, loss=5.03, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 217it [11:41,  3.23s/it, loss=5.03, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 217it [11:41,  3.23s/it, loss=5.04, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 218it [11:41,  3.22s/it, loss=5.04, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 218it [11:41,  3.22s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 219it [11:41,  3.20s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 219it [11:41,  3.20s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 220it [11:41,  3.19s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 220it [11:41,  3.19s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 221it [11:41,  3.18s/it, loss=4.92, v_num=10]#015Epoch 0: : 221it [11:41,  3.18s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 222it [11:41,  3.16s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 222it [11:41,  3.16s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 223it [11:42,  3.15s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 223it [11:42,  3.15s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 224it [12:45,  3.42s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 224it [12:45,  3.42s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 225it [12:45,  3.40s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 225it [12:45,  3.40s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 226it [14:03,  3.73s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 226it [14:03,  3.73s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 227it [14:03,  3.71s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 227it [14:03,  3.71s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 228it [14:03,  3.70s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 228it [14:03,  3.70s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 229it [14:03,  3.68s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 229it [14:03,  3.68s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 230it [14:03,  3.67s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 230it [14:03,  3.67s/it, loss=4.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 231it [14:03,  3.65s/it, loss=4.81, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 231it [14:03,  3.65s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 232it [14:03,  3.64s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 232it [14:03,  3.64s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 233it [14:03,  3.62s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 233it [14:03,  3.62s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 234it [14:03,  3.61s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 234it [14:03,  3.61s/it, loss=4.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 235it [14:03,  3.59s/it, loss=4.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 235it [14:03,  3.59s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 236it [14:04,  3.58s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 236it [14:04,  3.58s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 237it [14:04,  3.56s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 237it [14:04,  3.56s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 238it [14:04,  3.55s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 238it [14:04,  3.55s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 239it [14:04,  3.53s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 239it [14:04,  3.53s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 240it [14:04,  3.52s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 240it [14:04,  3.52s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 241it [14:04,  3.50s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 241it [14:04,  3.50s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 242it [14:04,  3.49s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 242it [14:04,  3.49s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 243it [14:04,  3.48s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 243it [14:04,  3.48s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 244it [14:04,  3.46s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 244it [14:04,  3.46s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 245it [14:05,  3.45s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 245it [14:05,  3.45s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 246it [14:05,  3.44s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 246it [14:05,  3.44s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 247it [14:05,  3.42s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 247it [14:05,  3.42s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 248it [14:05,  3.41s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 248it [14:05,  3.41s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 249it [14:05,  3.40s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 249it [14:05,  3.40s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 250it [14:05,  3.38s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 250it [14:05,  3.38s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 251it [14:05,  3.37s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 251it [14:05,  3.37s/it, loss=4.96, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 252it [14:05,  3.36s/it, loss=4.96, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 252it [14:05,  3.36s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 253it [14:05,  3.34s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 253it [14:05,  3.34s/it, loss=5.14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 254it [14:06,  3.33s/it, loss=5.14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 254it [14:06,  3.33s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 255it [14:06,  3.32s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 255it [14:06,  3.32s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 256it [14:29,  3.40s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 256it [14:29,  3.40s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 257it [14:29,  3.38s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 257it [14:29,  3.38s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 258it [16:06,  3.75s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 258it [16:06,  3.75s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 259it [16:06,  3.73s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 259it [16:06,  3.73s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 260it [16:06,  3.72s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 260it [16:06,  3.72s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 261it [16:06,  3.70s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 261it [16:06,  3.70s/it, loss=4.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 262it [16:06,  3.69s/it, loss=4.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 262it [16:06,  3.69s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 263it [16:06,  3.68s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 263it [16:06,  3.68s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 264it [16:06,  3.66s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 264it [16:06,  3.66s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 265it [16:07,  3.65s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 265it [16:07,  3.65s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 266it [16:07,  3.64s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 266it [16:07,  3.64s/it, loss=4.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 267it [16:07,  3.62s/it, loss=4.95, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 267it [16:07,  3.62s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 268it [16:07,  3.61s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 268it [16:07,  3.61s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 269it [16:07,  3.60s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 269it [16:07,  3.60s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 270it [16:07,  3.58s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 270it [16:07,  3.58s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 271it [16:07,  3.57s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 271it [16:07,  3.57s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 272it [16:07,  3.56s/it, loss=4.93, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 272it [16:07,  3.56s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 273it [16:07,  3.55s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 273it [16:07,  3.55s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 274it [16:08,  3.53s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 274it [16:08,  3.53s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 275it [16:08,  3.52s/it, loss=5.1, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 275it [16:08,  3.52s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 276it [16:08,  3.51s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 276it [16:08,  3.51s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 277it [16:08,  3.50s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 277it [16:08,  3.50s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 278it [16:08,  3.48s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 278it [16:08,  3.48s/it, loss=4.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 279it [16:08,  3.47s/it, loss=4.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 279it [16:08,  3.47s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 280it [16:08,  3.46s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 280it [16:08,  3.46s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 281it [16:08,  3.45s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 281it [16:08,  3.45s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 282it [16:08,  3.44s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 282it [16:08,  3.44s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 283it [16:09,  3.42s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 283it [16:09,  3.42s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 284it [16:09,  3.41s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 284it [16:09,  3.41s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 285it [16:09,  3.40s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 285it [16:09,  3.40s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 286it [16:09,  3.39s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 286it [16:09,  3.39s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 287it [16:09,  3.38s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 287it [16:09,  3.38s/it, loss=4.96, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 288it [16:10,  3.37s/it, loss=4.96, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 288it [16:10,  3.37s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 289it [16:10,  3.36s/it, loss=4.94, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 289it [16:10,  3.36s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 290it [17:35,  3.64s/it, loss=4.92, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 290it [17:35,  3.64s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 291it [17:35,  3.63s/it, loss=4.9, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 291it [17:35,  3.63s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 292it [17:35,  3.61s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 292it [17:35,  3.61s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 293it [17:35,  3.60s/it, loss=4.87, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 293it [17:35,  3.60s/it, loss=4.78, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 294it [17:35,  3.59s/it, loss=4.78, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 294it [17:35,  3.59s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 295it [17:35,  3.58s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 295it [17:35,  3.58s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 296it [17:35,  3.57s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 296it [17:35,  3.57s/it, loss=4.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 297it [17:36,  3.56s/it, loss=4.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 297it [17:36,  3.56s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 298it [17:36,  3.54s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 298it [17:36,  3.54s/it, loss=4.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 299it [17:36,  3.53s/it, loss=4.69, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 299it [17:36,  3.53s/it, loss=4.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 300it [17:36,  3.52s/it, loss=4.58, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 300it [17:36,  3.52s/it, loss=4.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 301it [17:36,  3.51s/it, loss=4.7, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 301it [17:36,  3.51s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 302it [17:36,  3.50s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 302it [17:36,  3.50s/it, loss=4.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 303it [17:36,  3.49s/it, loss=4.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 303it [17:36,  3.49s/it, loss=4.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 304it [17:36,  3.48s/it, loss=4.6, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 304it [17:36,  3.48s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 305it [17:36,  3.47s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 305it [17:36,  3.47s/it, loss=4.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 306it [17:37,  3.45s/it, loss=4.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 306it [17:37,  3.45s/it, loss=4.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 307it [17:37,  3.44s/it, loss=4.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 307it [17:37,  3.44s/it, loss=4.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 308it [17:37,  3.43s/it, loss=4.56, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 308it [17:37,  3.43s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 309it [17:37,  3.42s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 309it [17:37,  3.42s/it, loss=4.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 310it [17:37,  3.41s/it, loss=4.61, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 310it [17:37,  3.41s/it, loss=4.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 311it [17:37,  3.40s/it, loss=4.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 311it [17:37,  3.40s/it, loss=4.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 312it [17:37,  3.39s/it, loss=4.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 312it [17:37,  3.39s/it, loss=4.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 313it [17:38,  3.38s/it, loss=4.65, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 313it [17:38,  3.38s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 314it [17:38,  3.37s/it, loss=4.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 314it [17:38,  3.37s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 315it [17:38,  3.36s/it, loss=4.66, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 315it [17:38,  3.36s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 316it [17:38,  3.35s/it, loss=4.74, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 316it [17:38,  3.35s/it, loss=4.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 317it [17:38,  3.34s/it, loss=4.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 317it [17:38,  3.34s/it, loss=4.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 318it [17:38,  3.33s/it, loss=4.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 318it [17:38,  3.33s/it, loss=4.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 319it [17:38,  3.32s/it, loss=4.71, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 319it [17:38,  3.32s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 320it [18:03,  3.39s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 320it [18:03,  3.39s/it, loss=4.79, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 321it [18:22,  3.43s/it, loss=4.79, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 321it [18:22,  3.43s/it, loss=4.76, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 322it [19:23,  3.61s/it, loss=4.76, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 322it [19:23,  3.61s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 323it [19:23,  3.60s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 323it [19:23,  3.60s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 324it [19:23,  3.59s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 324it [19:23,  3.59s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 325it [19:23,  3.58s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 325it [19:23,  3.58s/it, loss=4.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 326it [19:23,  3.57s/it, loss=4.98, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 326it [19:23,  3.57s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 327it [19:24,  3.56s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 327it [19:24,  3.56s/it, loss=5.03, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 328it [19:24,  3.55s/it, loss=5.03, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 328it [19:24,  3.55s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 329it [19:24,  3.54s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 329it [19:24,  3.54s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 330it [19:24,  3.53s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 330it [19:24,  3.53s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 331it [19:24,  3.52s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 331it [19:24,  3.52s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 332it [19:24,  3.51s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 332it [19:24,  3.51s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 333it [19:25,  3.50s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 333it [19:25,  3.50s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 334it [19:25,  3.49s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 334it [19:25,  3.49s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 335it [19:25,  3.48s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 335it [19:25,  3.48s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 336it [19:25,  3.47s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 336it [19:25,  3.47s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 337it [19:25,  3.46s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 337it [19:25,  3.46s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 338it [19:25,  3.45s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 338it [19:25,  3.45s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 339it [19:25,  3.44s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 339it [19:25,  3.44s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 340it [19:25,  3.43s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 340it [19:25,  3.43s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 341it [19:26,  3.42s/it, loss=4.99, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 341it [19:26,  3.42s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 342it [19:26,  3.41s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 342it [19:26,  3.41s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 343it [19:26,  3.40s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 343it [19:26,  3.40s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 344it [19:26,  3.39s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 344it [19:26,  3.39s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 345it [19:26,  3.38s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 345it [19:26,  3.38s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 346it [19:26,  3.37s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 346it [19:26,  3.37s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 347it [19:27,  3.36s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 347it [19:27,  3.36s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 348it [19:27,  3.35s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 348it [19:27,  3.35s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 349it [19:27,  3.35s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 349it [19:27,  3.35s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 350it [19:27,  3.34s/it, loss=5.07, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 350it [19:27,  3.34s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 351it [19:27,  3.33s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 351it [19:27,  3.33s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 352it [20:03,  3.42s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 352it [20:03,  3.42s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 353it [20:44,  3.53s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 353it [20:44,  3.53s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 354it [21:03,  3.57s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 354it [21:03,  3.57s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 355it [21:03,  3.56s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 355it [21:03,  3.56s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 356it [21:03,  3.55s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 356it [21:03,  3.55s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 357it [21:03,  3.54s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 357it [21:03,  3.54s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 358it [21:03,  3.53s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 358it [21:03,  3.53s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 359it [21:03,  3.52s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 359it [21:03,  3.52s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 360it [21:03,  3.51s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 360it [21:03,  3.51s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 361it [21:04,  3.50s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 361it [21:04,  3.50s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 362it [21:15,  3.52s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 362it [21:15,  3.52s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 363it [21:16,  3.52s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 363it [21:16,  3.52s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 364it [21:16,  3.51s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 364it [21:16,  3.51s/it, loss=5.16, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 365it [21:16,  3.50s/it, loss=5.16, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 365it [21:16,  3.50s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 366it [21:16,  3.49s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 366it [21:16,  3.49s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 367it [21:16,  3.48s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 367it [21:16,  3.48s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 368it [21:16,  3.47s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 368it [21:16,  3.47s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 369it [21:16,  3.46s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 369it [21:16,  3.46s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 370it [21:16,  3.45s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 370it [21:16,  3.45s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 371it [21:17,  3.44s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 371it [21:17,  3.44s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 372it [21:17,  3.43s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 372it [21:17,  3.43s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 373it [21:17,  3.42s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 373it [21:17,  3.42s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 374it [21:17,  3.42s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 374it [21:17,  3.42s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 375it [21:17,  3.41s/it, loss=5.13, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 375it [21:17,  3.41s/it, loss=5.17, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 376it [21:17,  3.40s/it, loss=5.17, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 376it [21:17,  3.40s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 377it [21:18,  3.39s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 377it [21:18,  3.39s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 378it [21:18,  3.38s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 378it [21:18,  3.38s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 379it [21:33,  3.41s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 379it [21:33,  3.41s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 380it [21:33,  3.40s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 380it [21:33,  3.40s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 381it [21:33,  3.40s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 381it [21:33,  3.40s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 382it [21:34,  3.39s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 382it [21:34,  3.39s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 383it [21:35,  3.38s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 383it [21:35,  3.38s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 384it [21:58,  3.43s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 384it [21:58,  3.43s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 385it [22:29,  3.51s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 385it [22:29,  3.51s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 386it [22:57,  3.57s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 386it [22:57,  3.57s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 387it [22:57,  3.56s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 387it [22:57,  3.56s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 388it [22:57,  3.55s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 388it [22:57,  3.55s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 389it [22:57,  3.54s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 389it [22:57,  3.54s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 390it [22:57,  3.53s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 390it [22:57,  3.53s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 391it [22:58,  3.52s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 391it [22:58,  3.52s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 392it [22:58,  3.52s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 392it [22:58,  3.52s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 393it [22:58,  3.51s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 393it [22:58,  3.51s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 394it [22:58,  3.50s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 394it [22:58,  3.50s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 395it [22:58,  3.49s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 395it [22:58,  3.49s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 396it [22:58,  3.48s/it, loss=5.39, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 396it [22:58,  3.48s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 397it [22:58,  3.47s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 397it [22:58,  3.47s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 398it [22:59,  3.47s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 398it [22:59,  3.47s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 399it [22:59,  3.46s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 399it [22:59,  3.46s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 400it [22:59,  3.45s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 400it [22:59,  3.45s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 401it [22:59,  3.44s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 401it [22:59,  3.44s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 402it [22:59,  3.43s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 402it [22:59,  3.43s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 403it [22:59,  3.42s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 403it [22:59,  3.42s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 404it [23:00,  3.42s/it, loss=5.25, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 404it [23:00,  3.42s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 405it [23:00,  3.41s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 405it [23:00,  3.41s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 406it [23:09,  3.42s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 406it [23:09,  3.42s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 407it [23:09,  3.41s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 407it [23:09,  3.41s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 408it [23:09,  3.41s/it, loss=5.21, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 408it [23:09,  3.41s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 409it [23:09,  3.40s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 409it [23:09,  3.40s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 410it [23:10,  3.39s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 410it [23:10,  3.39s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 411it [23:27,  3.42s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 411it [23:27,  3.42s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 412it [23:27,  3.42s/it, loss=5.29, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 412it [23:27,  3.42s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 413it [23:27,  3.41s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 413it [23:27,  3.41s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 414it [23:27,  3.40s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 414it [23:27,  3.40s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 415it [24:03,  3.48s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 415it [24:03,  3.48s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 416it [24:03,  3.47s/it, loss=5.15, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 416it [24:03,  3.47s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 417it [24:08,  3.47s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 417it [24:08,  3.47s/it, loss=5.04, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 418it [24:51,  3.57s/it, loss=5.04, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 418it [24:51,  3.57s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 419it [24:51,  3.56s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 419it [24:51,  3.56s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 420it [24:51,  3.55s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 420it [24:51,  3.55s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 421it [24:51,  3.54s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 421it [24:51,  3.54s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 422it [24:51,  3.53s/it, loss=5.09, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 422it [24:51,  3.53s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 423it [24:51,  3.53s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 423it [24:51,  3.53s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 424it [24:52,  3.52s/it, loss=5.05, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 424it [24:52,  3.52s/it, loss=5.14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 425it [24:52,  3.51s/it, loss=5.14, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 425it [24:52,  3.51s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 426it [24:57,  3.51s/it, loss=5.06, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 426it [24:57,  3.51s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 427it [24:57,  3.51s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 427it [24:57,  3.51s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 428it [24:57,  3.50s/it, loss=5.11, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 428it [24:57,  3.50s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 429it [25:10,  3.52s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 429it [25:10,  3.52s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 430it [25:11,  3.51s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 430it [25:11,  3.51s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 431it [25:11,  3.51s/it, loss=5.31, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 431it [25:11,  3.51s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 432it [25:11,  3.50s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 432it [25:11,  3.50s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 433it [25:11,  3.49s/it, loss=5.19, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 433it [25:11,  3.49s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 434it [25:11,  3.48s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 434it [25:11,  3.48s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 435it [25:11,  3.48s/it, loss=5.35, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 435it [25:11,  3.48s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 436it [25:12,  3.47s/it, loss=5.41, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 436it [25:12,  3.47s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 437it [25:12,  3.46s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 437it [25:12,  3.46s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 438it [25:26,  3.49s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 438it [25:26,  3.49s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 439it [25:27,  3.48s/it, loss=5.46, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 439it [25:27,  3.48s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 440it [25:27,  3.47s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 440it [25:27,  3.47s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 441it [25:27,  3.46s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 441it [25:27,  3.46s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 442it [25:27,  3.46s/it, loss=5.36, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 442it [25:27,  3.46s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 443it [25:27,  3.45s/it, loss=5.3, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 443it [25:27,  3.45s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 444it [25:27,  3.44s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 444it [25:27,  3.44s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 445it [25:27,  3.43s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 445it [25:27,  3.43s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 446it [25:27,  3.43s/it, loss=5.2, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 446it [25:27,  3.43s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 447it [25:56,  3.48s/it, loss=5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 447it [25:56,  3.48s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 448it [26:01,  3.49s/it, loss=5.02, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 448it [26:01,  3.49s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 449it [26:02,  3.48s/it, loss=5.01, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 449it [26:02,  3.48s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 450it [26:55,  3.59s/it, loss=4.91, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 450it [26:55,  3.59s/it, loss=4.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 451it [26:55,  3.58s/it, loss=4.8, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 451it [26:55,  3.58s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 452it [26:55,  3.57s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 452it [26:55,  3.57s/it, loss=4.82, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 453it [26:55,  3.57s/it, loss=4.82, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 453it [26:55,  3.57s/it, loss=4.72, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 454it [26:55,  3.56s/it, loss=4.72, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 454it [26:55,  3.56s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 455it [26:55,  3.55s/it, loss=4.86, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 455it [26:55,  3.55s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 456it [26:55,  3.54s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 456it [26:55,  3.54s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 457it [26:55,  3.54s/it, loss=4.83, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 457it [26:55,  3.54s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 458it [26:55,  3.53s/it, loss=4.89, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 458it [26:55,  3.53s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 459it [27:07,  3.55s/it, loss=4.88, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 459it [27:07,  3.55s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 460it [27:07,  3.54s/it, loss=4.97, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 460it [27:07,  3.54s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 461it [27:17,  3.55s/it, loss=5.08, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 461it [27:17,  3.55s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 462it [27:18,  3.55s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 462it [27:18,  3.55s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 463it [27:18,  3.54s/it, loss=5.22, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 463it [27:18,  3.54s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 464it [27:18,  3.53s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 464it [27:18,  3.53s/it, loss=5.33, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 465it [27:18,  3.52s/it, loss=5.33, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 465it [27:18,  3.52s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 466it [27:18,  3.52s/it, loss=5.12, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 466it [27:18,  3.52s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 467it [27:18,  3.51s/it, loss=5.23, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 467it [27:18,  3.51s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 468it [27:19,  3.50s/it, loss=5.28, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 468it [27:19,  3.50s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 469it [27:19,  3.50s/it, loss=5.27, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 469it [27:19,  3.50s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 470it [27:24,  3.50s/it, loss=5.18, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 470it [27:24,  3.50s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 471it [27:24,  3.49s/it, loss=5.26, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 471it [27:24,  3.49s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 472it [27:25,  3.49s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 472it [27:25,  3.49s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 473it [27:25,  3.48s/it, loss=5.43, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 473it [27:25,  3.48s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 474it [27:25,  3.47s/it, loss=5.38, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 474it [27:25,  3.47s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 475it [27:25,  3.46s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 475it [27:25,  3.46s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 476it [27:25,  3.46s/it, loss=5.24, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 476it [27:25,  3.46s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 477it [27:25,  3.45s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 477it [27:25,  3.45s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 478it [27:25,  3.44s/it, loss=5.4, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 478it [27:25,  3.44s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 479it [27:42,  3.47s/it, loss=5.44, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 479it [27:42,  3.47s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 480it [27:42,  3.46s/it, loss=5.45, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 480it [27:42,  3.46s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 481it [27:42,  3.46s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 481it [27:42,  3.46s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 482it [29:04,  3.62s/it, loss=5.32, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 482it [29:04,  3.62s/it, loss=5.47, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 483it [29:04,  3.61s/it, loss=5.47, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 483it [29:04,  3.61s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 484it [29:04,  3.60s/it, loss=5.52, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 484it [29:04,  3.60s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 485it [29:04,  3.60s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 485it [29:04,  3.60s/it, loss=5.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 486it [29:05,  3.59s/it, loss=5.68, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 486it [29:05,  3.59s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 487it [29:05,  3.58s/it, loss=5.62, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 487it [29:05,  3.58s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 488it [29:05,  3.58s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 488it [29:05,  3.58s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 489it [29:05,  3.57s/it, loss=5.51, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 489it [29:05,  3.57s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 490it [29:05,  3.56s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 490it [29:05,  3.56s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 491it [29:05,  3.56s/it, loss=5.55, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 491it [29:05,  3.56s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 492it [29:05,  3.55s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 492it [29:05,  3.55s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 493it [29:05,  3.54s/it, loss=5.49, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 493it [29:05,  3.54s/it, loss=5.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 494it [29:05,  3.53s/it, loss=5.67, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 494it [29:05,  3.53s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 495it [29:06,  3.53s/it, loss=5.59, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 495it [29:06,  3.53s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 496it [29:06,  3.52s/it, loss=5.63, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 496it [29:06,  3.52s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 497it [29:06,  3.51s/it, loss=5.5, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 497it [29:06,  3.51s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 498it [29:06,  3.51s/it, loss=5.48, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 498it [29:06,  3.51s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 499it [29:06,  3.50s/it, loss=5.34, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 499it [29:06,  3.50s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 500it [29:06,  3.49s/it, loss=5.42, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 500it [29:06,  3.49s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 1it [00:00, 26.31it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 501it [29:12,  3.50s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 2it [00:00, 27.88it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 502it [29:12,  3.49s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 3it [00:00, 31.22it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 503it [29:12,  3.48s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 4it [00:00, 27.48it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 504it [29:12,  3.48s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 5it [00:00, 24.15it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 505it [29:12,  3.47s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 6it [00:00, 19.65it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 506it [29:12,  3.46s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 7it [00:00, 21.17it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 507it [29:12,  3.46s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 8it [00:00, 22.56it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 508it [29:12,  3.45s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 9it [00:00, 23.47it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 509it [29:12,  3.44s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 10it [00:00, 24.99it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 510it [29:12,  3.44s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 11it [00:00, 26.15it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 511it [29:12,  3.43s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 12it [00:00, 27.17it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 512it [29:13,  3.42s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 13it [00:00, 28.02it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 513it [29:13,  3.42s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 14it [00:00, 28.83it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 514it [29:13,  3.41s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 15it [00:00, 29.58it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 515it [29:13,  3.40s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 16it [00:00, 30.28it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 516it [29:13,  3.40s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 17it [00:00, 30.92it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 517it [29:13,  3.39s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 18it [00:00, 31.49it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 518it [29:13,  3.38s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 19it [00:00, 32.00it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 519it [29:13,  3.38s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 20it [00:00, 32.48it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 520it [29:13,  3.37s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 21it [00:00, 32.67it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 521it [29:13,  3.37s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 22it [00:00, 32.32it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 522it [29:13,  3.36s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 23it [00:00, 32.21it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 523it [29:13,  3.35s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 24it [00:00, 32.52it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 524it [29:13,  3.35s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 25it [00:00, 29.34it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 525it [29:13,  3.34s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 26it [00:00, 29.56it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 526it [29:13,  3.33s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 27it [00:00, 29.98it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 527it [29:13,  3.33s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 28it [00:00, 30.39it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 528it [29:13,  3.32s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 29it [00:00, 30.78it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 529it [29:13,  3.31s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 30it [00:00, 31.14it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 530it [29:13,  3.31s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 31it [00:00, 31.49it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 531it [29:13,  3.30s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 32it [00:01, 29.42it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 532it [29:13,  3.30s/it, loss=5.37, v_num=10]\u001B[0m\n",
      "\u001B[34mEpoch 0: : 532it [32:37,  3.68s/it, loss=5.37, v_num=10, val_loss=1.390]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 0: : 532it [32:37,  3.68s/it, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 0, global step 500: 'val_loss' reached 1.38819 (best 1.38819), saving model to '/opt/ml/code/lightning_logs/version_10/checkpoints/epoch=0-step=500.ckpt' as top 1\u001B[0m\n",
      "\u001B[34mEpoch 0: : 0it [00:00, ?it/s, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]      #015Epoch 1: : 0it [00:00, ?it/s, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 1it [00:00, 12.46it/s, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 1it [00:00, 12.36it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 2it [00:00,  6.58it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 2it [00:00,  6.57it/s, loss=5.43, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 3it [00:00,  7.84it/s, loss=5.43, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 3it [00:00,  7.83it/s, loss=5.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 4it [00:00,  8.67it/s, loss=5.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 4it [00:00,  8.66it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 5it [00:00,  9.26it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 5it [00:00,  9.25it/s, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 6it [00:00,  9.72it/s, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 6it [00:00,  9.71it/s, loss=5.4, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 7it [00:00, 10.06it/s, loss=5.4, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 7it [00:00, 10.06it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 8it [00:00, 10.26it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 8it [00:00, 10.25it/s, loss=5.46, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 9it [00:00, 10.42it/s, loss=5.46, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 9it [00:00, 10.41it/s, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 10it [00:00, 10.54it/s, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 10it [00:00, 10.53it/s, loss=5.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 11it [00:01, 10.64it/s, loss=5.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 11it [00:01, 10.64it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 12it [00:01, 10.67it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 12it [00:01, 10.67it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 13it [00:01, 10.74it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 13it [00:01, 10.74it/s, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 14it [00:01,  7.72it/s, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 14it [00:01,  7.72it/s, loss=5.18, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 15it [00:01,  7.84it/s, loss=5.18, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 15it [00:01,  7.83it/s, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 16it [00:02,  7.94it/s, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 16it [00:02,  7.94it/s, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 17it [00:02,  8.04it/s, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 17it [00:02,  8.04it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 18it [00:02,  8.07it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 18it [00:02,  8.07it/s, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 19it [00:02,  8.16it/s, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 19it [00:02,  8.16it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 20it [00:02,  8.23it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 20it [00:02,  8.23it/s, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 21it [00:02,  8.31it/s, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 21it [00:02,  8.31it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 22it [00:02,  8.37it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 22it [00:02,  8.37it/s, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 23it [00:03,  7.00it/s, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 23it [00:03,  7.00it/s, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 24it [00:03,  7.07it/s, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 24it [00:03,  7.06it/s, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 25it [00:03,  7.11it/s, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 25it [00:03,  7.11it/s, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 26it [00:03,  7.17it/s, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 26it [00:03,  7.16it/s, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 27it [00:03,  7.24it/s, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 27it [00:03,  7.24it/s, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 28it [00:03,  7.31it/s, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 28it [00:03,  7.31it/s, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 29it [00:03,  7.33it/s, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 29it [00:03,  7.33it/s, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 30it [00:04,  7.37it/s, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 30it [00:04,  7.37it/s, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 31it [00:04,  7.36it/s, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 31it [00:04,  7.35it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 32it [00:04,  7.37it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 32it [00:04,  7.36it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 33it [00:04,  7.38it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 33it [00:04,  7.37it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 34it [00:04,  6.86it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 34it [00:04,  6.84it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 35it [00:05,  6.85it/s, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 35it [00:05,  6.83it/s, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 36it [00:05,  6.73it/s, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 36it [00:05,  6.73it/s, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 37it [00:05,  6.72it/s, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 37it [00:05,  6.72it/s, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 38it [00:05,  6.74it/s, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 38it [00:05,  6.74it/s, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 39it [00:05,  6.72it/s, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 39it [00:05,  6.72it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 40it [00:05,  6.72it/s, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 40it [00:05,  6.72it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 41it [00:06,  6.68it/s, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 41it [00:06,  6.68it/s, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 42it [00:06,  6.60it/s, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 42it [00:06,  6.60it/s, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 43it [00:06,  6.57it/s, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 43it [00:06,  6.55it/s, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 44it [00:07,  6.00it/s, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 44it [00:07,  5.99it/s, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 45it [00:07,  5.98it/s, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 45it [00:07,  5.98it/s, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 46it [00:09,  4.79it/s, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 46it [00:09,  4.79it/s, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 47it [00:09,  4.80it/s, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 47it [00:09,  4.80it/s, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 48it [00:09,  4.82it/s, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 48it [00:09,  4.82it/s, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 49it [00:10,  4.85it/s, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 49it [00:10,  4.85it/s, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 50it [00:10,  4.88it/s, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 50it [00:10,  4.88it/s, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 51it [00:10,  4.89it/s, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 51it [00:10,  4.89it/s, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 52it [00:10,  4.90it/s, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 52it [00:10,  4.90it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 53it [00:10,  4.90it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 53it [00:10,  4.90it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 54it [00:10,  4.93it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 54it [00:10,  4.93it/s, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 55it [00:11,  4.77it/s, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 55it [00:11,  4.77it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 56it [00:11,  4.78it/s, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 56it [00:11,  4.78it/s, loss=5.38, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 57it [00:11,  4.80it/s, loss=5.38, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 57it [00:11,  4.80it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 58it [00:12,  4.82it/s, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 58it [00:12,  4.82it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 59it [00:12,  4.82it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 59it [00:12,  4.82it/s, loss=5.4, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 60it [00:12,  4.83it/s, loss=5.4, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 60it [00:12,  4.83it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 61it [00:12,  4.85it/s, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 61it [00:12,  4.84it/s, loss=5.36, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 62it [00:12,  4.85it/s, loss=5.36, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 62it [00:12,  4.85it/s, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 63it [00:12,  4.88it/s, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 63it [00:12,  4.88it/s, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 64it [01:58,  1.85s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 64it [01:58,  1.85s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 65it [02:21,  2.18s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 65it [02:21,  2.18s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 66it [02:21,  2.15s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 66it [02:21,  2.15s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 67it [02:22,  2.12s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 67it [02:22,  2.12s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 68it [02:22,  2.09s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 68it [02:22,  2.09s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 69it [02:32,  2.20s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 69it [02:32,  2.20s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 70it [02:32,  2.17s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 70it [02:32,  2.17s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 71it [02:32,  2.14s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 71it [02:32,  2.14s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 72it [02:32,  2.12s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 72it [02:32,  2.12s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 73it [02:32,  2.09s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 73it [02:32,  2.09s/it, loss=4.53, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 74it [02:32,  2.06s/it, loss=4.53, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 74it [02:32,  2.06s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 75it [02:32,  2.04s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 75it [02:32,  2.04s/it, loss=4.45, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 76it [02:32,  2.01s/it, loss=4.45, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 76it [02:32,  2.01s/it, loss=4.34, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 77it [02:32,  1.99s/it, loss=4.34, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 77it [02:32,  1.99s/it, loss=4.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 78it [02:33,  1.96s/it, loss=4.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 78it [02:33,  1.96s/it, loss=4.43, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 79it [02:33,  1.94s/it, loss=4.43, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 79it [02:33,  1.94s/it, loss=4.49, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 80it [02:33,  1.92s/it, loss=4.49, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 80it [02:33,  1.92s/it, loss=4.51, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 81it [02:33,  1.89s/it, loss=4.51, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 81it [02:33,  1.89s/it, loss=4.47, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 82it [02:45,  2.02s/it, loss=4.47, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 82it [02:45,  2.02s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 83it [02:45,  2.00s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 83it [02:45,  2.00s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 84it [02:46,  1.98s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 84it [02:46,  1.98s/it, loss=4.6, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 85it [02:46,  1.96s/it, loss=4.6, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 85it [02:46,  1.96s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 86it [02:46,  1.94s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 86it [02:46,  1.94s/it, loss=4.75, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 87it [02:46,  1.92s/it, loss=4.75, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 87it [02:46,  1.92s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 88it [02:46,  1.90s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 88it [02:46,  1.90s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 89it [02:47,  1.88s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 89it [02:47,  1.88s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 90it [02:47,  1.86s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 90it [02:47,  1.86s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 91it [02:47,  1.84s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 91it [02:47,  1.84s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 92it [02:47,  1.82s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 92it [02:47,  1.82s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 93it [02:47,  1.80s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 93it [02:47,  1.80s/it, loss=5.23, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 94it [02:47,  1.79s/it, loss=5.23, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 94it [02:47,  1.79s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 95it [02:47,  1.77s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 95it [02:48,  1.77s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 96it [03:34,  2.23s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 96it [03:34,  2.23s/it, loss=5.38, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 97it [04:16,  2.64s/it, loss=5.38, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 97it [04:16,  2.64s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 98it [04:16,  2.61s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 98it [04:16,  2.61s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 99it [04:16,  2.59s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 99it [04:16,  2.59s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 100it [04:16,  2.57s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 100it [04:16,  2.57s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 101it [04:24,  2.62s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 101it [04:24,  2.62s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 102it [04:25,  2.60s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 102it [04:25,  2.60s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 103it [04:25,  2.58s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 103it [04:25,  2.58s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 104it [04:25,  2.55s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 104it [04:25,  2.55s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 105it [04:25,  2.53s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 105it [04:25,  2.53s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 106it [04:25,  2.51s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 106it [04:25,  2.51s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 107it [04:25,  2.49s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 107it [04:25,  2.49s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 108it [04:26,  2.46s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 108it [04:26,  2.46s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 109it [04:26,  2.44s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 109it [04:26,  2.44s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 110it [04:26,  2.42s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 110it [04:26,  2.42s/it, loss=4.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 111it [04:26,  2.40s/it, loss=4.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 111it [04:26,  2.40s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 112it [04:26,  2.38s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 112it [04:26,  2.38s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 113it [04:26,  2.36s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 113it [04:26,  2.36s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 114it [04:39,  2.45s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 114it [04:39,  2.45s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 115it [04:39,  2.43s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 115it [04:39,  2.43s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 116it [04:39,  2.41s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 116it [04:39,  2.41s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 117it [04:39,  2.39s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 117it [04:39,  2.39s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 118it [04:39,  2.37s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 118it [04:39,  2.37s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 119it [04:39,  2.35s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 119it [04:39,  2.35s/it, loss=4.8, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 120it [04:40,  2.33s/it, loss=4.8, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 120it [04:40,  2.33s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 121it [04:40,  2.32s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 121it [04:40,  2.32s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 122it [04:40,  2.30s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 122it [04:40,  2.30s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 123it [04:40,  2.28s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 123it [04:40,  2.28s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 124it [04:40,  2.26s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 124it [04:40,  2.26s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 125it [04:40,  2.25s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 125it [04:40,  2.25s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 126it [04:41,  2.23s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 126it [04:41,  2.23s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 127it [04:41,  2.21s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 127it [04:41,  2.21s/it, loss=4.8, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 128it [05:18,  2.49s/it, loss=4.8, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 128it [05:18,  2.49s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 129it [05:54,  2.75s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 129it [05:54,  2.75s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 130it [05:54,  2.73s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 130it [05:54,  2.73s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 131it [05:54,  2.71s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 131it [05:54,  2.71s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 132it [05:54,  2.69s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 132it [05:54,  2.69s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 133it [06:35,  2.97s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 133it [06:35,  2.97s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 134it [06:35,  2.95s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 134it [06:35,  2.95s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 135it [06:35,  2.93s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 135it [06:35,  2.93s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 136it [06:35,  2.91s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 136it [06:35,  2.91s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 137it [06:36,  2.89s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 137it [06:36,  2.89s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 138it [06:36,  2.87s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 138it [06:36,  2.87s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 139it [06:36,  2.85s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 139it [06:36,  2.85s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 140it [06:36,  2.83s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 140it [06:36,  2.83s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 141it [06:36,  2.81s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 141it [06:36,  2.81s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 142it [06:36,  2.79s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 142it [06:36,  2.79s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 143it [06:36,  2.77s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 143it [06:36,  2.77s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 144it [06:36,  2.76s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 144it [06:36,  2.76s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 145it [06:37,  2.74s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 145it [06:37,  2.74s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 146it [06:52,  2.82s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 146it [06:52,  2.82s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 147it [06:52,  2.80s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 147it [06:52,  2.80s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 148it [06:52,  2.79s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 148it [06:52,  2.79s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 149it [06:52,  2.77s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 149it [06:52,  2.77s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 150it [06:52,  2.75s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 150it [06:52,  2.75s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 151it [06:52,  2.73s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 151it [06:52,  2.73s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 152it [06:53,  2.72s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 152it [06:53,  2.72s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 153it [06:53,  2.70s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 153it [06:53,  2.70s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 154it [06:53,  2.68s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 154it [06:53,  2.68s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 155it [06:53,  2.67s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 155it [06:53,  2.67s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 156it [06:53,  2.65s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 156it [06:53,  2.65s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 157it [07:01,  2.69s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 157it [07:01,  2.69s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 158it [07:01,  2.67s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 158it [07:01,  2.67s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 159it [07:02,  2.65s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 159it [07:02,  2.65s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 160it [07:05,  2.66s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 160it [07:05,  2.66s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 161it [07:43,  2.88s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 161it [07:43,  2.88s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 162it [07:57,  2.95s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 162it [07:57,  2.95s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 163it [07:58,  2.93s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 163it [07:58,  2.93s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 164it [07:58,  2.92s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 164it [07:58,  2.92s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 165it [08:42,  3.17s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 165it [08:42,  3.17s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 166it [08:43,  3.15s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 166it [08:43,  3.15s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 167it [08:43,  3.13s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 167it [08:43,  3.13s/it, loss=4.7, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 168it [08:43,  3.11s/it, loss=4.7, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 168it [08:43,  3.11s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 169it [08:43,  3.10s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 169it [08:43,  3.10s/it, loss=4.72, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 170it [08:43,  3.08s/it, loss=4.72, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 170it [08:43,  3.08s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 171it [08:43,  3.06s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 171it [08:43,  3.06s/it, loss=4.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 172it [08:43,  3.05s/it, loss=4.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 172it [08:43,  3.05s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 173it [08:43,  3.03s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 173it [08:43,  3.03s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 174it [08:43,  3.01s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 174it [08:43,  3.01s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 175it [08:44,  2.99s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 175it [08:44,  2.99s/it, loss=4.7, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 176it [08:44,  2.98s/it, loss=4.7, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 176it [08:44,  2.98s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 177it [08:44,  2.96s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 177it [08:44,  2.96s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 178it [08:44,  2.95s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 178it [08:44,  2.95s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 179it [08:44,  2.93s/it, loss=4.86, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 179it [08:44,  2.93s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 180it [08:44,  2.92s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 180it [08:44,  2.92s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 181it [08:44,  2.90s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 181it [08:44,  2.90s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 182it [08:44,  2.88s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 182it [08:44,  2.88s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 183it [08:45,  2.87s/it, loss=4.69, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 183it [08:45,  2.87s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 184it [08:45,  2.85s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 184it [08:45,  2.85s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 185it [08:45,  2.84s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 185it [08:45,  2.84s/it, loss=4.62, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 186it [08:45,  2.83s/it, loss=4.62, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 186it [08:45,  2.83s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 187it [08:45,  2.81s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 187it [08:45,  2.81s/it, loss=4.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 188it [08:45,  2.80s/it, loss=4.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 188it [08:45,  2.80s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 189it [09:01,  2.86s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 189it [09:01,  2.86s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 190it [09:01,  2.85s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 190it [09:01,  2.85s/it, loss=4.59, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 191it [09:01,  2.83s/it, loss=4.59, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 191it [09:01,  2.83s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 192it [09:28,  2.96s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 192it [09:28,  2.96s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 193it [09:37,  2.99s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 193it [09:37,  2.99s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 194it [09:57,  3.08s/it, loss=4.63, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 194it [09:57,  3.08s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 195it [09:57,  3.06s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 195it [09:57,  3.06s/it, loss=4.54, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 196it [09:57,  3.05s/it, loss=4.54, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 196it [09:57,  3.05s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 197it [10:47,  3.29s/it, loss=4.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 197it [10:47,  3.29s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 198it [10:47,  3.27s/it, loss=4.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 198it [10:47,  3.27s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 199it [10:47,  3.25s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 199it [10:47,  3.25s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 200it [10:47,  3.24s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 200it [10:47,  3.24s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 201it [10:47,  3.22s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 201it [10:47,  3.22s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 202it [10:47,  3.21s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 202it [10:47,  3.21s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 203it [10:47,  3.19s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 203it [10:47,  3.19s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 204it [10:48,  3.18s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 204it [10:48,  3.18s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 205it [10:48,  3.16s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 205it [10:48,  3.16s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 206it [10:48,  3.15s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 206it [10:48,  3.15s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 207it [10:48,  3.13s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 207it [10:48,  3.13s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 208it [10:48,  3.12s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 208it [10:48,  3.12s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 209it [10:49,  3.11s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 209it [10:49,  3.11s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 210it [10:49,  3.09s/it, loss=4.94, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 210it [10:49,  3.09s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 211it [10:49,  3.08s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 211it [10:49,  3.08s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 212it [10:49,  3.06s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 212it [10:49,  3.06s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 213it [10:49,  3.05s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 213it [10:49,  3.05s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 214it [10:49,  3.04s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 214it [10:49,  3.04s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 215it [10:50,  3.02s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 215it [10:50,  3.02s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 216it [10:50,  3.01s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 216it [10:50,  3.01s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 217it [10:50,  3.00s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 217it [10:50,  3.00s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 218it [10:50,  2.98s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 218it [10:50,  2.98s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 219it [10:50,  2.97s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 219it [10:50,  2.97s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 220it [10:50,  2.96s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 220it [10:50,  2.96s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 221it [10:51,  2.95s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 221it [10:51,  2.95s/it, loss=5.28, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 222it [10:51,  2.93s/it, loss=5.28, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 222it [10:51,  2.93s/it, loss=5.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 223it [10:51,  2.92s/it, loss=5.35, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 223it [10:51,  2.92s/it, loss=5.44, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 224it [11:06,  2.97s/it, loss=5.44, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 224it [11:06,  2.97s/it, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 225it [11:38,  3.11s/it, loss=5.37, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 225it [11:38,  3.11s/it, loss=5.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 226it [12:15,  3.25s/it, loss=5.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 226it [12:15,  3.25s/it, loss=5.42, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 227it [12:15,  3.24s/it, loss=5.42, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 227it [12:15,  3.24s/it, loss=5.47, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 228it [12:15,  3.23s/it, loss=5.47, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 228it [12:15,  3.23s/it, loss=5.48, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 229it [12:32,  3.29s/it, loss=5.48, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 229it [12:32,  3.29s/it, loss=5.55, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 230it [12:32,  3.27s/it, loss=5.55, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 230it [12:32,  3.27s/it, loss=5.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 231it [12:33,  3.26s/it, loss=5.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 231it [12:33,  3.26s/it, loss=5.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 232it [12:33,  3.25s/it, loss=5.65, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 232it [12:33,  3.25s/it, loss=5.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 233it [12:33,  3.23s/it, loss=5.5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 233it [12:33,  3.23s/it, loss=5.34, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 234it [12:33,  3.22s/it, loss=5.34, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 234it [12:33,  3.22s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 235it [12:33,  3.21s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 235it [12:33,  3.21s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 236it [12:33,  3.19s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 236it [12:33,  3.19s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 237it [12:33,  3.18s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 237it [12:33,  3.18s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 238it [12:33,  3.17s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 238it [12:33,  3.17s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 239it [12:34,  3.16s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 239it [12:34,  3.16s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 240it [12:34,  3.14s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 240it [12:34,  3.14s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 241it [12:34,  3.13s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 241it [12:34,  3.13s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 242it [12:34,  3.12s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 242it [12:34,  3.12s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 243it [12:34,  3.11s/it, loss=4.67, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 243it [12:34,  3.11s/it, loss=4.57, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 244it [12:34,  3.09s/it, loss=4.57, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 244it [12:34,  3.09s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 245it [12:34,  3.08s/it, loss=4.58, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 245it [12:34,  3.08s/it, loss=4.56, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 246it [12:34,  3.07s/it, loss=4.56, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 246it [12:34,  3.07s/it, loss=4.52, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 247it [12:34,  3.06s/it, loss=4.52, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 247it [12:34,  3.06s/it, loss=4.51, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 248it [12:35,  3.04s/it, loss=4.51, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 248it [12:35,  3.04s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 249it [12:35,  3.03s/it, loss=4.61, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 249it [12:35,  3.03s/it, loss=4.49, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 250it [12:35,  3.02s/it, loss=4.49, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 250it [12:35,  3.02s/it, loss=4.45, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 251it [12:35,  3.01s/it, loss=4.45, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 251it [12:35,  3.01s/it, loss=4.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 252it [12:35,  3.00s/it, loss=4.41, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 252it [12:35,  3.00s/it, loss=4.44, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 253it [12:47,  3.03s/it, loss=4.44, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 253it [12:47,  3.03s/it, loss=4.52, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 254it [12:47,  3.02s/it, loss=4.52, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 254it [12:47,  3.02s/it, loss=4.6, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 255it [12:47,  3.01s/it, loss=4.6, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 255it [12:47,  3.01s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 256it [12:51,  3.01s/it, loss=4.64, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 256it [12:51,  3.01s/it, loss=4.72, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 257it [13:47,  3.22s/it, loss=4.72, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 257it [13:47,  3.22s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 258it [14:10,  3.30s/it, loss=4.77, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 258it [14:10,  3.30s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 259it [14:11,  3.29s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 259it [14:11,  3.29s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 260it [14:11,  3.27s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 260it [14:11,  3.27s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 261it [14:47,  3.40s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 261it [14:47,  3.40s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 262it [14:47,  3.39s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 262it [14:47,  3.39s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 263it [14:47,  3.37s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 263it [14:47,  3.37s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 264it [14:47,  3.36s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 264it [14:47,  3.36s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 265it [14:47,  3.35s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 265it [14:47,  3.35s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 266it [14:47,  3.34s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 266it [14:47,  3.34s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 267it [14:47,  3.32s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 267it [14:47,  3.32s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 268it [14:47,  3.31s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 268it [14:47,  3.31s/it, loss=5.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 269it [14:47,  3.30s/it, loss=5.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 269it [14:47,  3.30s/it, loss=5.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 270it [14:48,  3.29s/it, loss=5.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 270it [14:48,  3.29s/it, loss=5.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 271it [14:48,  3.28s/it, loss=5.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 271it [14:48,  3.28s/it, loss=5.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 272it [14:48,  3.27s/it, loss=5.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 272it [14:48,  3.27s/it, loss=5.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 273it [14:48,  3.25s/it, loss=5.71, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 273it [14:48,  3.25s/it, loss=5.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 274it [14:48,  3.24s/it, loss=5.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 274it [14:48,  3.24s/it, loss=5.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 275it [14:48,  3.23s/it, loss=5.73, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 275it [14:48,  3.23s/it, loss=5.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 276it [14:48,  3.22s/it, loss=5.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 276it [14:48,  3.22s/it, loss=5.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 277it [14:48,  3.21s/it, loss=5.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 277it [14:48,  3.21s/it, loss=5.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 278it [14:49,  3.20s/it, loss=5.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 278it [14:49,  3.20s/it, loss=5.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 279it [14:49,  3.19s/it, loss=5.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 279it [14:49,  3.19s/it, loss=5.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 280it [14:49,  3.18s/it, loss=5.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 280it [14:49,  3.18s/it, loss=5.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 281it [14:49,  3.17s/it, loss=5.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 281it [14:49,  3.17s/it, loss=5.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 282it [14:49,  3.15s/it, loss=5.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 282it [14:49,  3.15s/it, loss=5.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 283it [14:49,  3.14s/it, loss=5.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 283it [14:49,  3.14s/it, loss=5.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 284it [14:49,  3.13s/it, loss=5.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 284it [14:49,  3.13s/it, loss=5.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 285it [14:49,  3.12s/it, loss=5.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 285it [14:49,  3.12s/it, loss=5.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 286it [14:50,  3.11s/it, loss=5.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 286it [14:50,  3.11s/it, loss=5.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 287it [14:50,  3.10s/it, loss=5.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 287it [14:50,  3.10s/it, loss=5.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 288it [14:56,  3.11s/it, loss=5.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 288it [14:56,  3.11s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 289it [16:00,  3.32s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 289it [16:00,  3.32s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 290it [16:15,  3.37s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 290it [16:15,  3.37s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 291it [16:16,  3.35s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 291it [16:16,  3.35s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 292it [16:16,  3.34s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 292it [16:16,  3.34s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 293it [16:38,  3.41s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 293it [16:38,  3.41s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 294it [16:38,  3.40s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 294it [16:38,  3.40s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 295it [16:39,  3.39s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 295it [16:39,  3.39s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 296it [16:39,  3.38s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 296it [16:39,  3.38s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 297it [16:39,  3.36s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 297it [16:39,  3.36s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 298it [16:39,  3.35s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 298it [16:39,  3.35s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 299it [16:39,  3.34s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 299it [16:39,  3.34s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 300it [16:39,  3.33s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 300it [16:39,  3.33s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 301it [16:39,  3.32s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 301it [16:39,  3.32s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 302it [16:39,  3.31s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 302it [16:39,  3.31s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 303it [16:40,  3.30s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 303it [16:40,  3.30s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 304it [16:40,  3.29s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 304it [16:40,  3.29s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 305it [16:40,  3.28s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 305it [16:40,  3.28s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 306it [16:40,  3.27s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 306it [16:40,  3.27s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 307it [16:40,  3.26s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 307it [16:40,  3.26s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 308it [16:40,  3.25s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 308it [16:40,  3.25s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 309it [16:40,  3.24s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 309it [16:40,  3.24s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 310it [16:41,  3.23s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 310it [16:41,  3.23s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 311it [16:41,  3.22s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 311it [16:41,  3.22s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 312it [16:41,  3.21s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 312it [16:41,  3.21s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 313it [16:41,  3.20s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 313it [16:41,  3.20s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 314it [16:41,  3.19s/it, loss=5.2, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 314it [16:41,  3.19s/it, loss=5.24, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 315it [16:41,  3.18s/it, loss=5.24, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 315it [16:41,  3.18s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 316it [16:42,  3.17s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 316it [16:42,  3.17s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 317it [16:42,  3.16s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 317it [16:42,  3.16s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 318it [16:54,  3.19s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 318it [16:54,  3.19s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 319it [16:54,  3.18s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 319it [16:54,  3.18s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 320it [16:54,  3.17s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 320it [16:54,  3.17s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 321it [18:00,  3.37s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 321it [18:00,  3.37s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 322it [18:24,  3.43s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 322it [18:24,  3.43s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 323it [18:24,  3.42s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 323it [18:24,  3.42s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 324it [18:24,  3.41s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 324it [18:24,  3.41s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 325it [18:55,  3.49s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 325it [18:55,  3.49s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 326it [18:55,  3.48s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 326it [18:55,  3.48s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 327it [18:55,  3.47s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 327it [18:55,  3.47s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 328it [18:55,  3.46s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 328it [18:55,  3.46s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 329it [18:55,  3.45s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 329it [18:55,  3.45s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 330it [18:55,  3.44s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 330it [18:55,  3.44s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 331it [18:56,  3.43s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 331it [18:56,  3.43s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 332it [18:56,  3.42s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 332it [18:56,  3.42s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 333it [18:56,  3.41s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 333it [18:56,  3.41s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 334it [18:56,  3.40s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 334it [18:56,  3.40s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 335it [18:56,  3.39s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 335it [18:56,  3.39s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 336it [18:56,  3.38s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 336it [18:56,  3.38s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 337it [18:56,  3.37s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 337it [18:56,  3.37s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 338it [18:56,  3.36s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 338it [18:56,  3.36s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 339it [18:56,  3.35s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 339it [18:56,  3.35s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 340it [18:57,  3.34s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 340it [18:57,  3.34s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 341it [18:57,  3.33s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 341it [18:57,  3.33s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 342it [18:57,  3.33s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 342it [18:57,  3.33s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 343it [18:57,  3.32s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 343it [18:57,  3.32s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 344it [18:57,  3.31s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 344it [18:57,  3.31s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 345it [18:57,  3.30s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 345it [18:57,  3.30s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 346it [18:57,  3.29s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 346it [18:57,  3.29s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 347it [18:57,  3.28s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 347it [18:57,  3.28s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 348it [18:58,  3.27s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 348it [18:58,  3.27s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 349it [18:58,  3.26s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 349it [18:58,  3.26s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 350it [19:01,  3.26s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 350it [19:01,  3.26s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 351it [19:01,  3.25s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 351it [19:01,  3.25s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 352it [19:01,  3.24s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 352it [19:01,  3.24s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 353it [19:34,  3.33s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 353it [19:34,  3.33s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 354it [19:55,  3.38s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 354it [19:55,  3.38s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 355it [19:55,  3.37s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 355it [19:55,  3.37s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 356it [19:55,  3.36s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 356it [19:55,  3.36s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 357it [20:28,  3.44s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 357it [20:28,  3.44s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 358it [20:28,  3.43s/it, loss=4.68, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 358it [20:28,  3.43s/it, loss=4.66, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 359it [20:28,  3.42s/it, loss=4.66, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 359it [20:28,  3.42s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 360it [20:28,  3.41s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 360it [20:28,  3.41s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 361it [20:28,  3.40s/it, loss=4.78, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 361it [20:28,  3.40s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 362it [20:29,  3.40s/it, loss=4.76, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 362it [20:29,  3.40s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 363it [20:29,  3.39s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 363it [20:29,  3.39s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 364it [20:29,  3.38s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 364it [20:29,  3.38s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 365it [20:29,  3.37s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 365it [20:29,  3.37s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 366it [20:29,  3.36s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 366it [20:29,  3.36s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 367it [20:29,  3.35s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 367it [20:29,  3.35s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 368it [20:30,  3.34s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 368it [20:30,  3.34s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 369it [20:30,  3.33s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 369it [20:30,  3.33s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 370it [20:30,  3.33s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 370it [20:30,  3.33s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 371it [20:30,  3.32s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 371it [20:30,  3.32s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 372it [20:30,  3.31s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 372it [20:30,  3.31s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 373it [20:30,  3.30s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 373it [20:30,  3.30s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 374it [20:31,  3.29s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 374it [20:31,  3.29s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 375it [20:31,  3.28s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 375it [20:31,  3.28s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 376it [20:31,  3.28s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 376it [20:31,  3.28s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 377it [20:31,  3.27s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 377it [20:31,  3.27s/it, loss=5.24, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 378it [20:31,  3.26s/it, loss=5.24, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 378it [20:31,  3.26s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 379it [20:31,  3.25s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 379it [20:31,  3.25s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 380it [20:32,  3.24s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 380it [20:32,  3.24s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 381it [21:09,  3.33s/it, loss=5.26, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 381it [21:09,  3.33s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 382it [21:19,  3.35s/it, loss=5.25, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 382it [21:19,  3.35s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 383it [21:19,  3.34s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 383it [21:19,  3.34s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 384it [21:20,  3.33s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 384it [21:20,  3.33s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 385it [21:20,  3.33s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 385it [21:20,  3.33s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 386it [21:34,  3.35s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 386it [21:34,  3.35s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 387it [21:34,  3.35s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 387it [21:34,  3.35s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 388it [21:52,  3.38s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 388it [21:52,  3.38s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 389it [22:13,  3.43s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 389it [22:13,  3.43s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 390it [22:21,  3.44s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 390it [22:21,  3.44s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 391it [22:21,  3.43s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 391it [22:21,  3.43s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 392it [22:22,  3.42s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 392it [22:22,  3.42s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 393it [22:22,  3.42s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 393it [22:22,  3.42s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 394it [22:22,  3.41s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 394it [22:22,  3.41s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 395it [22:22,  3.40s/it, loss=4.95, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 395it [22:22,  3.40s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 396it [22:22,  3.39s/it, loss=4.89, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 396it [22:22,  3.39s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 397it [22:23,  3.38s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 397it [22:23,  3.38s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 398it [22:23,  3.37s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 398it [22:23,  3.37s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 399it [22:23,  3.37s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 399it [22:23,  3.37s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 400it [22:23,  3.36s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 400it [22:23,  3.36s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 401it [22:23,  3.35s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 401it [22:23,  3.35s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 402it [22:23,  3.34s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 402it [22:23,  3.34s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 403it [22:23,  3.33s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 403it [22:23,  3.33s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 404it [22:23,  3.33s/it, loss=4.74, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 404it [22:23,  3.33s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 405it [22:23,  3.32s/it, loss=4.85, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 405it [22:23,  3.32s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 406it [22:24,  3.31s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 406it [22:24,  3.31s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 407it [22:24,  3.30s/it, loss=4.79, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 407it [22:24,  3.30s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 408it [22:24,  3.29s/it, loss=4.84, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 408it [22:24,  3.29s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 409it [22:24,  3.29s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 409it [22:24,  3.29s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 410it [22:24,  3.28s/it, loss=4.9, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 410it [22:24,  3.28s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 411it [22:24,  3.27s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 411it [22:24,  3.27s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 412it [22:24,  3.26s/it, loss=4.83, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 412it [22:24,  3.26s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 413it [22:44,  3.30s/it, loss=4.91, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 413it [22:44,  3.30s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 414it [23:04,  3.34s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 414it [23:04,  3.34s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 415it [23:04,  3.34s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 415it [23:04,  3.34s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 416it [23:04,  3.33s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 416it [23:04,  3.33s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 417it [23:13,  3.34s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 417it [23:13,  3.34s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 418it [23:47,  3.41s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 418it [23:47,  3.41s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 419it [23:47,  3.41s/it, loss=5.01, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 419it [23:47,  3.41s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 420it [24:14,  3.46s/it, loss=5.03, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 420it [24:14,  3.46s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 421it [24:23,  3.48s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 421it [24:23,  3.48s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 422it [24:26,  3.48s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 422it [24:26,  3.48s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 423it [24:26,  3.47s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 423it [24:26,  3.47s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 424it [24:27,  3.46s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 424it [24:27,  3.46s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 425it [24:27,  3.45s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 425it [24:27,  3.45s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 426it [24:27,  3.44s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 426it [24:27,  3.44s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 427it [24:27,  3.44s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 427it [24:27,  3.44s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 428it [24:27,  3.43s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 428it [24:27,  3.43s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 429it [24:27,  3.42s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 429it [24:27,  3.42s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 430it [24:27,  3.41s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 430it [24:27,  3.41s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 431it [24:27,  3.41s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 431it [24:27,  3.41s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 432it [24:27,  3.40s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 432it [24:27,  3.40s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 433it [24:27,  3.39s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 433it [24:27,  3.39s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 434it [24:28,  3.38s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 434it [24:28,  3.38s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 435it [24:28,  3.38s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 435it [24:28,  3.38s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 436it [24:28,  3.37s/it, loss=5.27, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 436it [24:28,  3.37s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 437it [24:28,  3.36s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 437it [24:28,  3.36s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 438it [24:28,  3.35s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 438it [24:28,  3.35s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 439it [24:28,  3.35s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 439it [24:28,  3.35s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 440it [24:28,  3.34s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 440it [24:28,  3.34s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 441it [24:28,  3.33s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 441it [24:28,  3.33s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 442it [24:29,  3.32s/it, loss=4.82, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 442it [24:29,  3.32s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 443it [24:29,  3.32s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 443it [24:29,  3.32s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 444it [24:29,  3.31s/it, loss=4.92, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 444it [24:29,  3.31s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 445it [24:41,  3.33s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 445it [24:41,  3.33s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 446it [24:59,  3.36s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 446it [24:59,  3.36s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 447it [24:59,  3.36s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 447it [24:59,  3.36s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 448it [25:00,  3.35s/it, loss=4.97, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 448it [25:00,  3.35s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 449it [25:21,  3.39s/it, loss=4.81, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 449it [25:21,  3.39s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 450it [26:06,  3.48s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 450it [26:06,  3.48s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 451it [26:06,  3.47s/it, loss=4.88, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 451it [26:06,  3.47s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 452it [26:37,  3.53s/it, loss=4.93, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 452it [26:37,  3.53s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 453it [26:37,  3.53s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 453it [26:37,  3.53s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 454it [26:37,  3.52s/it, loss=4.87, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 454it [26:37,  3.52s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 455it [26:37,  3.51s/it, loss=5.04, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 455it [26:37,  3.51s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 456it [26:37,  3.50s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 456it [26:37,  3.50s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 457it [26:38,  3.50s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 457it [26:38,  3.50s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 458it [26:38,  3.49s/it, loss=4.96, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 458it [26:38,  3.49s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 459it [26:38,  3.48s/it, loss=5.09, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 459it [26:38,  3.48s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 460it [26:38,  3.47s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 460it [26:38,  3.47s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 461it [26:38,  3.47s/it, loss=5.02, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 461it [26:38,  3.47s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 462it [26:38,  3.46s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 462it [26:38,  3.46s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 463it [26:38,  3.45s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 463it [26:38,  3.45s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 464it [26:38,  3.45s/it, loss=4.99, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 464it [26:38,  3.45s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 465it [26:38,  3.44s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 465it [26:38,  3.44s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 466it [26:39,  3.43s/it, loss=5.07, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 466it [26:39,  3.43s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 467it [26:39,  3.42s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 467it [26:39,  3.42s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 468it [26:39,  3.42s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 468it [26:39,  3.42s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 469it [26:39,  3.41s/it, loss=5.12, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 469it [26:39,  3.41s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 470it [26:39,  3.40s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 470it [26:39,  3.40s/it, loss=5.18, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 471it [26:39,  3.40s/it, loss=5.18, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 471it [26:39,  3.40s/it, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 472it [26:39,  3.39s/it, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 472it [26:39,  3.39s/it, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 473it [26:39,  3.38s/it, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 473it [26:39,  3.38s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 474it [26:39,  3.38s/it, loss=5.31, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 474it [26:39,  3.38s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 475it [26:40,  3.37s/it, loss=5.1, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 475it [26:40,  3.37s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 476it [26:40,  3.36s/it, loss=5.05, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 476it [26:40,  3.36s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 477it [26:50,  3.38s/it, loss=5.11, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 477it [26:50,  3.38s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 478it [26:57,  3.38s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 478it [26:57,  3.38s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 479it [26:57,  3.38s/it, loss=5.14, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 479it [26:57,  3.38s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 480it [26:57,  3.37s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 480it [26:57,  3.37s/it, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 481it [27:45,  3.46s/it, loss=5.22, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 481it [27:45,  3.46s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 482it [28:14,  3.52s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 482it [28:14,  3.52s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 483it [28:14,  3.51s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 483it [28:14,  3.51s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 484it [28:38,  3.55s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 484it [28:38,  3.55s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 485it [28:38,  3.54s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 485it [28:38,  3.54s/it, loss=5.3, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 486it [28:38,  3.54s/it, loss=5.3, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 486it [28:38,  3.54s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 487it [28:38,  3.53s/it, loss=5.32, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 487it [28:38,  3.53s/it, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 488it [28:38,  3.52s/it, loss=5.29, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 488it [28:38,  3.52s/it, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 489it [28:38,  3.52s/it, loss=5.39, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 489it [28:38,  3.52s/it, loss=5.3, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 490it [28:39,  3.51s/it, loss=5.3, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 490it [28:39,  3.51s/it, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 491it [28:39,  3.50s/it, loss=5.33, v_num=10, val_loss=1.390, train_loss=5.620]#015Epoch 1: : 491it [28:39,  3.50s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 492it [28:39,  3.49s/it, loss=5.19, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 492it [28:39,  3.49s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 493it [28:39,  3.49s/it, loss=5.15, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 493it [28:39,  3.49s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 494it [28:39,  3.48s/it, loss=5.16, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 494it [28:39,  3.48s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 495it [28:39,  3.47s/it, loss=5.21, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 495it [28:39,  3.47s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 496it [28:39,  3.47s/it, loss=5.17, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 496it [28:39,  3.47s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 497it [28:39,  3.46s/it, loss=5.06, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 497it [28:39,  3.46s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 498it [28:39,  3.45s/it, loss=5.13, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 498it [28:39,  3.45s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 499it [28:40,  3.45s/it, loss=5.08, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 499it [28:40,  3.45s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 500it [28:40,  3.44s/it, loss=4.98, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 500it [28:40,  3.44s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34m#015Validation DataLoader 0: : 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 1it [00:00, 23.10it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 501it [28:46,  3.45s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 2it [00:00, 25.58it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 502it [28:46,  3.44s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 3it [00:00, 29.89it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 503it [28:46,  3.43s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 4it [00:00, 32.27it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 504it [28:46,  3.43s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 5it [00:00, 33.80it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 505it [28:46,  3.42s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 6it [00:00, 35.04it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 506it [28:46,  3.41s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 7it [00:00, 35.86it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 507it [28:46,  3.41s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 8it [00:00, 37.24it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 508it [28:46,  3.40s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 9it [00:00, 38.44it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 509it [28:46,  3.39s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 10it [00:00, 39.48it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 510it [28:46,  3.39s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 11it [00:00, 40.29it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 511it [28:46,  3.38s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 12it [00:00, 40.97it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 512it [28:46,  3.37s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 13it [00:00, 41.66it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 513it [28:46,  3.37s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 14it [00:00, 37.90it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 514it [28:46,  3.36s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 15it [00:00, 38.52it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 515it [28:46,  3.35s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 16it [00:00, 39.08it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 516it [28:46,  3.35s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 17it [00:00, 32.03it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 517it [28:46,  3.34s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 18it [00:00, 32.53it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 518it [28:46,  3.33s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 19it [00:00, 33.00it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 519it [28:46,  3.33s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 20it [00:00, 33.46it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 520it [28:46,  3.32s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 21it [00:00, 33.86it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 521it [28:46,  3.31s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 22it [00:00, 31.63it/s]#033[A#015Epoch 1: : 522it [28:46,  3.31s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 23it [00:00, 31.85it/s]#033[A#015Epoch 1: : 523it [28:47,  3.30s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 24it [00:00, 32.47it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 524it [28:47,  3.30s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 25it [00:00, 33.05it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 525it [28:47,  3.29s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 26it [00:00, 33.61it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 526it [28:47,  3.28s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 27it [00:00, 34.13it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 527it [28:47,  3.28s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 28it [00:00, 34.47it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 528it [28:47,  3.27s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 29it [00:00, 34.91it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 529it [28:47,  3.26s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 30it [00:00, 35.42it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 530it [28:47,  3.26s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 31it [00:00, 35.98it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 531it [28:47,  3.25s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 32it [00:00, 34.42it/s]#033[A#015Epoch 1: : 532it [28:47,  3.25s/it, loss=5, v_num=10, val_loss=1.390, train_loss=5.620]\u001B[0m\n",
      "\u001B[34mEpoch 1: : 532it [32:35,  3.68s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.620]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 1: : 532it [32:35,  3.68s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 1, global step 1000: 'val_loss' reached 0.77254 (best 0.77254), saving model to '/opt/ml/code/lightning_logs/version_10/checkpoints/epoch=1-step=1000.ckpt' as top 1\u001B[0m\n",
      "\u001B[34mEpoch 1: : 0it [00:00, ?it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]      #015Epoch 2: : 0it [00:00, ?it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 1it [00:00, 12.62it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 1it [00:00, 12.56it/s, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 2it [00:00, 12.71it/s, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 2it [00:00, 12.66it/s, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 3it [00:00, 12.70it/s, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 3it [00:00, 12.67it/s, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 4it [00:00, 12.67it/s, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 4it [00:00, 12.64it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 5it [00:00, 12.68it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 5it [00:00, 12.66it/s, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 6it [00:00, 12.68it/s, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 6it [00:00, 12.66it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 7it [00:00, 12.68it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 7it [00:00, 12.67it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 8it [00:00, 12.68it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 8it [00:00, 12.67it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 9it [00:01,  8.72it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 9it [00:01,  8.71it/s, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 10it [00:01,  8.99it/s, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 10it [00:01,  8.98it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 11it [00:01,  9.20it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 11it [00:01,  9.20it/s, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 12it [00:01,  9.38it/s, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 12it [00:01,  9.38it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 13it [00:01,  8.62it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 13it [00:01,  8.62it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 14it [00:02,  6.17it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 14it [00:02,  6.16it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 15it [00:02,  6.34it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 15it [00:02,  6.33it/s, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 16it [00:02,  5.37it/s, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 16it [00:02,  5.36it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 17it [00:03,  5.51it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 17it [00:03,  5.51it/s, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 18it [00:03,  5.65it/s, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 18it [00:03,  5.65it/s, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 19it [00:03,  5.78it/s, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 19it [00:03,  5.78it/s, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 20it [00:03,  5.88it/s, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 20it [00:03,  5.88it/s, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 21it [00:03,  5.99it/s, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 21it [00:03,  5.99it/s, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 22it [00:03,  6.10it/s, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 22it [00:03,  6.10it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 23it [00:03,  6.21it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 23it [00:03,  6.20it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 24it [00:03,  6.31it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 24it [00:03,  6.30it/s, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 25it [00:03,  6.40it/s, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 25it [00:03,  6.40it/s, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 26it [00:04,  6.49it/s, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 26it [00:04,  6.48it/s, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 27it [00:04,  6.57it/s, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 27it [00:04,  6.57it/s, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 28it [00:04,  6.63it/s, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 28it [00:04,  6.63it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 29it [00:04,  6.67it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 29it [00:04,  6.67it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 30it [00:04,  6.68it/s, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 30it [00:04,  6.67it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 31it [00:04,  6.63it/s, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 31it [00:04,  6.63it/s, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 32it [00:04,  6.61it/s, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 32it [00:04,  6.59it/s, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 33it [00:05,  6.52it/s, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 33it [00:05,  6.52it/s, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 34it [00:05,  6.45it/s, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 34it [00:05,  6.45it/s, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 35it [00:05,  6.43it/s, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 35it [00:05,  6.43it/s, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 36it [00:05,  6.40it/s, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 36it [00:05,  6.40it/s, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 37it [00:05,  6.46it/s, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 37it [00:05,  6.44it/s, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 38it [00:05,  6.43it/s, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 38it [00:05,  6.43it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 39it [00:06,  6.43it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 39it [00:06,  6.43it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 40it [00:06,  6.39it/s, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 40it [00:06,  6.39it/s, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 41it [00:06,  6.40it/s, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 41it [00:06,  6.39it/s, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 42it [00:06,  6.38it/s, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 42it [00:06,  6.38it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 43it [00:06,  6.38it/s, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 43it [00:06,  6.38it/s, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 44it [00:06,  6.41it/s, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 44it [00:06,  6.41it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 45it [00:08,  5.47it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 45it [00:08,  5.47it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 46it [00:10,  4.55it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 46it [00:10,  4.55it/s, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 47it [00:10,  4.57it/s, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 47it [00:10,  4.56it/s, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 48it [00:10,  4.52it/s, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 48it [00:10,  4.52it/s, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 49it [00:10,  4.56it/s, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 49it [00:10,  4.56it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 50it [00:11,  4.46it/s, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 50it [00:11,  4.46it/s, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 51it [00:11,  4.46it/s, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 51it [00:11,  4.46it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 52it [00:11,  4.48it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 52it [00:11,  4.48it/s, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 53it [00:11,  4.48it/s, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 53it [00:11,  4.48it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 54it [00:11,  4.50it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 54it [00:11,  4.50it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 55it [00:12,  4.53it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 55it [00:12,  4.53it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 56it [00:12,  4.55it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 56it [00:12,  4.54it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 57it [00:12,  4.55it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 57it [00:12,  4.55it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 58it [00:12,  4.55it/s, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 58it [00:12,  4.55it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 59it [00:12,  4.57it/s, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 59it [00:12,  4.57it/s, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 60it [00:13,  4.60it/s, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 60it [00:13,  4.60it/s, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 61it [00:13,  4.63it/s, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 61it [00:13,  4.63it/s, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 62it [00:13,  4.66it/s, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 62it [00:13,  4.66it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 63it [00:13,  4.68it/s, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 63it [00:13,  4.68it/s, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 64it [01:55,  1.81s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 64it [01:55,  1.81s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 65it [02:07,  1.95s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 65it [02:07,  1.95s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 66it [02:07,  1.93s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 66it [02:07,  1.93s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 67it [02:07,  1.90s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 67it [02:07,  1.90s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 68it [02:07,  1.88s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 68it [02:07,  1.88s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 69it [02:07,  1.85s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 69it [02:07,  1.85s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 70it [02:08,  1.83s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 70it [02:08,  1.83s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 71it [02:24,  2.04s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 71it [02:24,  2.04s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 72it [02:26,  2.04s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 72it [02:26,  2.04s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 73it [02:26,  2.01s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 73it [02:26,  2.01s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 74it [02:27,  1.99s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 74it [02:27,  1.99s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 75it [02:27,  1.96s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 75it [02:27,  1.96s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 76it [02:27,  1.94s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 76it [02:27,  1.94s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 77it [02:27,  1.92s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 77it [02:27,  1.92s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 78it [02:27,  1.89s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 78it [02:27,  1.89s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 79it [02:27,  1.87s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 79it [02:27,  1.87s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 80it [02:28,  1.85s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 80it [02:28,  1.85s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 81it [02:28,  1.83s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 81it [02:28,  1.83s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 82it [02:28,  1.81s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 82it [02:28,  1.81s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 83it [02:28,  1.79s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 83it [02:28,  1.79s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 84it [02:28,  1.77s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 84it [02:28,  1.77s/it, loss=4.65, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 85it [02:28,  1.75s/it, loss=4.65, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 85it [02:28,  1.75s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 86it [02:29,  1.73s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 86it [02:29,  1.73s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 87it [02:29,  1.72s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 87it [02:29,  1.72s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 88it [02:29,  1.70s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 88it [02:29,  1.70s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 89it [02:29,  1.68s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 89it [02:29,  1.68s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 90it [02:29,  1.66s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 90it [02:29,  1.66s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 91it [02:30,  1.65s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 91it [02:30,  1.65s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 92it [02:30,  1.63s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 92it [02:30,  1.63s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 93it [02:30,  1.62s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 93it [02:30,  1.62s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 94it [02:30,  1.60s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 94it [02:30,  1.60s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 95it [02:30,  1.59s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 95it [02:30,  1.59s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 96it [03:39,  2.29s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 96it [03:39,  2.29s/it, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 97it [04:34,  2.83s/it, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 97it [04:34,  2.83s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 98it [04:35,  2.81s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 98it [04:35,  2.81s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 99it [04:35,  2.78s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 99it [04:35,  2.78s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 100it [04:35,  2.75s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 100it [04:35,  2.75s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 101it [04:35,  2.73s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 101it [04:35,  2.73s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 102it [04:35,  2.70s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 102it [04:35,  2.70s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 103it [04:35,  2.68s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 103it [04:35,  2.68s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 104it [04:35,  2.65s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 104it [04:35,  2.65s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 105it [04:36,  2.63s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 105it [04:36,  2.63s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 106it [04:36,  2.61s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 106it [04:36,  2.61s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 107it [04:36,  2.58s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 107it [04:36,  2.58s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 108it [04:36,  2.56s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 108it [04:36,  2.56s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 109it [04:36,  2.54s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 109it [04:36,  2.54s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 110it [04:36,  2.52s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 110it [04:36,  2.52s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 111it [04:37,  2.50s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 111it [04:37,  2.50s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 112it [04:37,  2.48s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 112it [04:37,  2.48s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 113it [04:37,  2.46s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 113it [04:37,  2.46s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 114it [04:37,  2.44s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 114it [04:37,  2.44s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 115it [04:37,  2.41s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 115it [04:37,  2.41s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 116it [04:37,  2.40s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 116it [04:37,  2.40s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 117it [04:38,  2.38s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 117it [04:38,  2.38s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 118it [04:38,  2.36s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 118it [04:38,  2.36s/it, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 119it [04:47,  2.42s/it, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 119it [04:47,  2.42s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 120it [04:47,  2.40s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 120it [04:47,  2.40s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 121it [04:48,  2.38s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 121it [04:48,  2.38s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 122it [04:48,  2.36s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 122it [04:48,  2.36s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 123it [04:48,  2.34s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 123it [04:48,  2.34s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 124it [04:48,  2.33s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 124it [04:48,  2.33s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 125it [04:48,  2.31s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 125it [04:48,  2.31s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 126it [04:48,  2.29s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 126it [04:48,  2.29s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 127it [04:49,  2.28s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 127it [04:49,  2.28s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 128it [05:01,  2.35s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 128it [05:01,  2.35s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 129it [06:50,  3.18s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 129it [06:50,  3.18s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 130it [06:50,  3.16s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 130it [06:50,  3.16s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 131it [06:50,  3.13s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 131it [06:50,  3.13s/it, loss=4.38, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 132it [06:50,  3.11s/it, loss=4.38, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 132it [06:50,  3.11s/it, loss=4.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 133it [06:50,  3.09s/it, loss=4.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 133it [06:50,  3.09s/it, loss=4.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 134it [06:50,  3.07s/it, loss=4.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 134it [06:50,  3.07s/it, loss=4.34, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 135it [06:51,  3.04s/it, loss=4.34, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 135it [06:51,  3.05s/it, loss=4.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 136it [06:51,  3.02s/it, loss=4.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 136it [06:51,  3.02s/it, loss=4.38, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 137it [06:51,  3.00s/it, loss=4.38, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 137it [06:51,  3.00s/it, loss=4.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 138it [06:51,  2.98s/it, loss=4.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 138it [06:51,  2.98s/it, loss=4.37, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 139it [06:51,  2.96s/it, loss=4.37, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 139it [06:51,  2.96s/it, loss=4.4, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 140it [06:51,  2.94s/it, loss=4.4, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 140it [06:51,  2.94s/it, loss=4.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 141it [06:52,  2.92s/it, loss=4.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 141it [06:52,  2.92s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 142it [06:52,  2.90s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 142it [06:52,  2.90s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 143it [06:52,  2.88s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 143it [06:52,  2.88s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 144it [06:52,  2.87s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 144it [06:52,  2.87s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 145it [06:52,  2.85s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 145it [06:52,  2.85s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 146it [06:52,  2.83s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 146it [06:52,  2.83s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 147it [06:53,  2.81s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 147it [06:53,  2.81s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 148it [06:53,  2.79s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 148it [06:53,  2.79s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 149it [06:53,  2.77s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 149it [06:53,  2.77s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 150it [06:53,  2.76s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 150it [06:53,  2.76s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 151it [06:53,  2.74s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 151it [06:53,  2.74s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 152it [06:54,  2.72s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 152it [06:54,  2.72s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 153it [06:54,  2.71s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 153it [06:54,  2.71s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 154it [06:54,  2.69s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 154it [06:54,  2.69s/it, loss=5.32, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 155it [06:54,  2.67s/it, loss=5.32, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 155it [06:54,  2.67s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 156it [06:54,  2.66s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 156it [06:54,  2.66s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 157it [06:54,  2.64s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 157it [06:54,  2.64s/it, loss=5.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 158it [06:54,  2.63s/it, loss=5.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 158it [06:54,  2.63s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 159it [06:55,  2.61s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 159it [06:55,  2.61s/it, loss=5.51, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 160it [06:55,  2.60s/it, loss=5.51, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 160it [06:55,  2.60s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 161it [08:40,  3.24s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 161it [08:40,  3.24s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 162it [08:41,  3.22s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 162it [08:41,  3.22s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 163it [08:41,  3.20s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 163it [08:41,  3.20s/it, loss=5.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 164it [08:41,  3.18s/it, loss=5.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 164it [08:41,  3.18s/it, loss=5.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 165it [08:41,  3.16s/it, loss=5.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 165it [08:41,  3.16s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 166it [08:41,  3.14s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 166it [08:41,  3.14s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 167it [08:42,  3.13s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 167it [08:42,  3.13s/it, loss=5.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 168it [08:42,  3.11s/it, loss=5.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 168it [08:42,  3.11s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 169it [08:42,  3.09s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 169it [08:42,  3.09s/it, loss=5.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 170it [08:42,  3.07s/it, loss=5.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 170it [08:42,  3.07s/it, loss=5.4, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 171it [08:42,  3.06s/it, loss=5.4, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 171it [08:42,  3.06s/it, loss=5.34, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 172it [08:42,  3.04s/it, loss=5.34, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 172it [08:42,  3.04s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 173it [08:43,  3.02s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 173it [08:43,  3.02s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 174it [08:43,  3.01s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 174it [08:43,  3.01s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 175it [08:43,  2.99s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 175it [08:43,  2.99s/it, loss=5.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 176it [08:43,  2.98s/it, loss=5.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 176it [08:43,  2.98s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 177it [08:43,  2.96s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 177it [08:43,  2.96s/it, loss=5.23, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 178it [08:43,  2.94s/it, loss=5.23, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 178it [08:43,  2.94s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 179it [08:44,  2.93s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 179it [08:44,  2.93s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 180it [08:44,  2.91s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 180it [08:44,  2.91s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 181it [08:44,  2.90s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 181it [08:44,  2.90s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 182it [08:44,  2.88s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 182it [08:44,  2.88s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 183it [08:44,  2.87s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 183it [08:44,  2.87s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 184it [08:44,  2.85s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 184it [08:44,  2.85s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 185it [08:45,  2.84s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 185it [08:45,  2.84s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 186it [08:45,  2.82s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 186it [08:45,  2.82s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 187it [08:45,  2.81s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 187it [08:45,  2.81s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 188it [08:45,  2.80s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 188it [08:45,  2.80s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 189it [08:45,  2.78s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 189it [08:45,  2.78s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 190it [08:46,  2.77s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 190it [08:46,  2.77s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 191it [08:46,  2.75s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 191it [08:46,  2.75s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 192it [08:47,  2.75s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 192it [08:47,  2.75s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 193it [10:11,  3.17s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 193it [10:11,  3.17s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 194it [10:11,  3.15s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 194it [10:11,  3.15s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 195it [10:11,  3.14s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 195it [10:11,  3.14s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 196it [10:11,  3.12s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 196it [10:11,  3.12s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 197it [10:25,  3.18s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 197it [10:25,  3.18s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 198it [10:25,  3.16s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 198it [10:25,  3.16s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 199it [10:26,  3.15s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 199it [10:26,  3.15s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 200it [10:26,  3.13s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 200it [10:26,  3.13s/it, loss=5.24, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 201it [10:26,  3.12s/it, loss=5.24, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 201it [10:26,  3.12s/it, loss=5.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 202it [11:04,  3.29s/it, loss=5.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 202it [11:04,  3.29s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 203it [11:04,  3.27s/it, loss=5.44, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 203it [11:04,  3.27s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 204it [11:04,  3.26s/it, loss=5.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 204it [11:04,  3.26s/it, loss=5.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 205it [11:04,  3.24s/it, loss=5.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 205it [11:04,  3.24s/it, loss=5.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 206it [11:05,  3.23s/it, loss=5.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 206it [11:05,  3.23s/it, loss=5.51, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 207it [11:05,  3.21s/it, loss=5.51, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 207it [11:05,  3.21s/it, loss=5.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 208it [11:05,  3.20s/it, loss=5.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 208it [11:05,  3.20s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 209it [11:05,  3.18s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 209it [11:05,  3.18s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 210it [11:05,  3.17s/it, loss=5.45, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 210it [11:05,  3.17s/it, loss=5.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 211it [11:05,  3.15s/it, loss=5.36, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 211it [11:05,  3.15s/it, loss=5.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 212it [11:05,  3.14s/it, loss=5.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 212it [11:05,  3.14s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 213it [11:05,  3.13s/it, loss=5.43, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 213it [11:05,  3.13s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 214it [11:05,  3.11s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 214it [11:05,  3.11s/it, loss=5.28, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 215it [11:06,  3.10s/it, loss=5.28, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 215it [11:06,  3.10s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 216it [11:06,  3.08s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 216it [11:06,  3.08s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 217it [11:06,  3.07s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 217it [11:06,  3.07s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 218it [11:06,  3.06s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 218it [11:06,  3.06s/it, loss=5.29, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 219it [11:06,  3.04s/it, loss=5.29, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 219it [11:06,  3.04s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 220it [11:06,  3.03s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 220it [11:06,  3.03s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 221it [11:06,  3.02s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 221it [11:06,  3.02s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 222it [11:06,  3.00s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 222it [11:06,  3.00s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 223it [11:07,  2.99s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 223it [11:07,  2.99s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 224it [11:07,  2.98s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 224it [11:07,  2.98s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 225it [12:05,  3.22s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 225it [12:05,  3.22s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 226it [12:05,  3.21s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 226it [12:05,  3.21s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 227it [12:05,  3.20s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 227it [12:05,  3.20s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 228it [12:06,  3.18s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 228it [12:06,  3.18s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 229it [12:06,  3.17s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 229it [12:06,  3.17s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 230it [12:06,  3.16s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 230it [12:06,  3.16s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 231it [12:15,  3.18s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 231it [12:15,  3.18s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 232it [12:15,  3.17s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 232it [12:15,  3.17s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 233it [12:15,  3.16s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 233it [12:15,  3.16s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 234it [12:52,  3.30s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 234it [12:52,  3.30s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 235it [12:52,  3.29s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 235it [12:52,  3.29s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 236it [12:52,  3.27s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 236it [12:52,  3.27s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 237it [12:52,  3.26s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 237it [12:52,  3.26s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 238it [12:53,  3.25s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 238it [12:53,  3.25s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 239it [12:53,  3.24s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 239it [12:53,  3.24s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 240it [12:53,  3.22s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 240it [12:53,  3.22s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 241it [12:53,  3.21s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 241it [12:53,  3.21s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 242it [12:53,  3.20s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 242it [12:53,  3.20s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 243it [12:53,  3.18s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 243it [12:53,  3.18s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 244it [12:53,  3.17s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 244it [12:53,  3.17s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 245it [12:54,  3.16s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 245it [12:54,  3.16s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 246it [12:54,  3.15s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 246it [12:54,  3.15s/it, loss=4.41, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 247it [12:54,  3.14s/it, loss=4.41, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 247it [12:54,  3.14s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 248it [12:54,  3.12s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 248it [12:54,  3.12s/it, loss=4.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 249it [12:54,  3.11s/it, loss=4.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 249it [12:54,  3.11s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 250it [12:55,  3.10s/it, loss=4.49, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 250it [12:55,  3.10s/it, loss=4.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 251it [12:55,  3.09s/it, loss=4.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 251it [12:55,  3.09s/it, loss=4.41, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 252it [12:55,  3.08s/it, loss=4.41, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 252it [12:55,  3.08s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 253it [12:55,  3.07s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 253it [12:55,  3.07s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 254it [12:55,  3.05s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 254it [12:55,  3.05s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 255it [12:56,  3.04s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 255it [12:56,  3.04s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 256it [12:56,  3.03s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 256it [12:56,  3.03s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 257it [14:10,  3.31s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 257it [14:10,  3.31s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 258it [14:10,  3.30s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 258it [14:10,  3.30s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 259it [14:10,  3.28s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 259it [14:10,  3.28s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 260it [14:10,  3.27s/it, loss=4.79, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 260it [14:10,  3.27s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 261it [14:10,  3.26s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 261it [14:10,  3.26s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 262it [14:10,  3.25s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 262it [14:10,  3.25s/it, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 263it [14:11,  3.24s/it, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 263it [14:11,  3.24s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 264it [14:11,  3.22s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 264it [14:11,  3.22s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 265it [14:11,  3.21s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 265it [14:11,  3.21s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 266it [14:59,  3.38s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 266it [14:59,  3.38s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 267it [14:59,  3.37s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 267it [14:59,  3.37s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 268it [14:59,  3.36s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 268it [14:59,  3.36s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 269it [14:59,  3.35s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 269it [14:59,  3.35s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 270it [14:59,  3.33s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 270it [14:59,  3.33s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 271it [15:00,  3.32s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 271it [15:00,  3.32s/it, loss=5.26, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 272it [15:00,  3.31s/it, loss=5.26, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 272it [15:00,  3.31s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 273it [15:00,  3.30s/it, loss=5.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 273it [15:00,  3.30s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 274it [15:00,  3.29s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 274it [15:00,  3.29s/it, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 275it [15:00,  3.27s/it, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 275it [15:00,  3.27s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 276it [15:00,  3.26s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 276it [15:00,  3.26s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 277it [15:00,  3.25s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 277it [15:00,  3.25s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 278it [15:00,  3.24s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 278it [15:00,  3.24s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 279it [15:00,  3.23s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 279it [15:00,  3.23s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 280it [15:01,  3.22s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 280it [15:01,  3.22s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 281it [15:01,  3.21s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 281it [15:01,  3.21s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 282it [15:01,  3.20s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 282it [15:01,  3.20s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 283it [15:01,  3.19s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 283it [15:01,  3.19s/it, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 284it [15:01,  3.17s/it, loss=4.74, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 284it [15:01,  3.17s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 285it [15:01,  3.16s/it, loss=4.72, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 285it [15:01,  3.16s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 286it [15:01,  3.15s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 286it [15:01,  3.15s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 287it [15:07,  3.16s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 287it [15:07,  3.16s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 288it [15:07,  3.15s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 288it [15:07,  3.15s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 289it [15:42,  3.26s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 289it [15:42,  3.26s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 290it [15:47,  3.27s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 290it [15:47,  3.27s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 291it [15:51,  3.27s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 291it [15:51,  3.27s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 292it [16:07,  3.31s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 292it [16:07,  3.31s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 293it [16:12,  3.32s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 293it [16:12,  3.32s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 294it [16:12,  3.31s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 294it [16:12,  3.31s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 295it [16:13,  3.30s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 295it [16:13,  3.30s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 296it [16:13,  3.29s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 296it [16:13,  3.29s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 297it [16:13,  3.28s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 297it [16:13,  3.28s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 298it [16:41,  3.36s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 298it [16:41,  3.36s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 299it [16:41,  3.35s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 299it [16:41,  3.35s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 300it [16:41,  3.34s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 300it [16:41,  3.34s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 301it [17:00,  3.39s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 301it [17:00,  3.39s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 302it [17:00,  3.38s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 302it [17:00,  3.38s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 303it [17:00,  3.37s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 303it [17:00,  3.37s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 304it [17:00,  3.36s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 304it [17:00,  3.36s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 305it [17:00,  3.35s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 305it [17:00,  3.35s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 306it [17:00,  3.34s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 306it [17:00,  3.34s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 307it [17:00,  3.33s/it, loss=5.15, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 307it [17:00,  3.33s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 308it [17:00,  3.31s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 308it [17:00,  3.31s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 309it [17:01,  3.30s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 309it [17:01,  3.30s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 310it [17:01,  3.29s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 310it [17:01,  3.29s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 311it [17:01,  3.28s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 311it [17:01,  3.28s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 312it [17:01,  3.27s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 312it [17:01,  3.27s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 313it [17:01,  3.26s/it, loss=4.52, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 313it [17:01,  3.26s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 314it [17:01,  3.25s/it, loss=4.5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 314it [17:01,  3.25s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 315it [17:01,  3.24s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 315it [17:01,  3.24s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 316it [17:01,  3.23s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 316it [17:01,  3.23s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 317it [17:02,  3.22s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 317it [17:02,  3.22s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 318it [17:02,  3.21s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 318it [17:02,  3.21s/it, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 319it [17:06,  3.22s/it, loss=4.91, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 319it [17:06,  3.22s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 320it [17:06,  3.21s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 320it [17:06,  3.21s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 321it [17:38,  3.30s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 321it [17:38,  3.30s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 322it [17:48,  3.32s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 322it [17:48,  3.32s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 323it [17:48,  3.31s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 323it [17:48,  3.31s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 324it [17:57,  3.33s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 324it [17:57,  3.33s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 325it [18:18,  3.38s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 325it [18:18,  3.38s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 326it [18:19,  3.37s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 326it [18:19,  3.37s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 327it [18:19,  3.36s/it, loss=5.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 327it [18:19,  3.36s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 328it [18:19,  3.35s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 328it [18:19,  3.35s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 329it [18:19,  3.34s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 329it [18:19,  3.34s/it, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 330it [18:43,  3.40s/it, loss=5.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 330it [18:43,  3.40s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 331it [18:43,  3.39s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 331it [18:43,  3.39s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 332it [18:43,  3.38s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 332it [18:43,  3.38s/it, loss=5.29, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 333it [18:50,  3.40s/it, loss=5.29, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 333it [18:50,  3.40s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 334it [18:50,  3.39s/it, loss=5.19, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 334it [18:50,  3.39s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 335it [18:51,  3.38s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 335it [18:51,  3.38s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 336it [18:51,  3.37s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 336it [18:51,  3.37s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 337it [18:51,  3.36s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 337it [18:51,  3.36s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 338it [18:51,  3.35s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 338it [18:51,  3.35s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 339it [18:51,  3.34s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 339it [18:51,  3.34s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 340it [18:51,  3.33s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 340it [18:51,  3.33s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 341it [18:51,  3.32s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 341it [18:51,  3.32s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 342it [18:51,  3.31s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 342it [18:51,  3.31s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 343it [18:51,  3.30s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 343it [18:51,  3.30s/it, loss=4.65, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 344it [18:52,  3.29s/it, loss=4.65, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 344it [18:52,  3.29s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 345it [18:52,  3.28s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 345it [18:52,  3.28s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 346it [18:52,  3.27s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 346it [18:52,  3.27s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 347it [18:52,  3.26s/it, loss=4.69, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 347it [18:52,  3.26s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 348it [18:52,  3.25s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 348it [18:52,  3.25s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 349it [18:52,  3.25s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 349it [18:52,  3.25s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 350it [18:52,  3.24s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 350it [18:52,  3.24s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 351it [18:52,  3.23s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 351it [18:52,  3.23s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 352it [18:53,  3.22s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 352it [18:53,  3.22s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 353it [19:22,  3.29s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 353it [19:22,  3.29s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 354it [19:46,  3.35s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 354it [19:46,  3.35s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 355it [19:46,  3.34s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 355it [19:46,  3.34s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 356it [19:51,  3.35s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 356it [19:51,  3.35s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 357it [20:09,  3.39s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 357it [20:09,  3.39s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 358it [20:09,  3.38s/it, loss=4.75, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 358it [20:09,  3.38s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 359it [20:09,  3.37s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 359it [20:09,  3.37s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 360it [20:09,  3.36s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 360it [20:09,  3.36s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 361it [20:09,  3.35s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 361it [20:09,  3.35s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 362it [20:40,  3.43s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 362it [20:40,  3.43s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 363it [20:40,  3.42s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 363it [20:40,  3.42s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 364it [20:40,  3.41s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 364it [20:40,  3.41s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 365it [20:41,  3.40s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 365it [20:41,  3.40s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 366it [20:41,  3.39s/it, loss=5.01, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 366it [20:41,  3.39s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 367it [20:41,  3.38s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 367it [20:41,  3.38s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 368it [20:41,  3.37s/it, loss=5.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 368it [20:41,  3.37s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 369it [20:41,  3.36s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 369it [20:41,  3.36s/it, loss=5.11, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 370it [20:41,  3.36s/it, loss=5.11, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 370it [20:41,  3.36s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 371it [20:41,  3.35s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 371it [20:41,  3.35s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 372it [20:41,  3.34s/it, loss=5.08, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 372it [20:41,  3.34s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 373it [20:42,  3.33s/it, loss=5.09, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 373it [20:42,  3.33s/it, loss=5.11, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 374it [20:42,  3.32s/it, loss=5.11, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 374it [20:42,  3.32s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 375it [20:42,  3.31s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 375it [20:42,  3.31s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 376it [20:42,  3.31s/it, loss=5.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 376it [20:42,  3.31s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 377it [20:42,  3.30s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 377it [20:42,  3.30s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 378it [20:43,  3.29s/it, loss=5.05, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 378it [20:43,  3.29s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 379it [20:43,  3.28s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 379it [20:43,  3.28s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 380it [20:43,  3.27s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 380it [20:43,  3.27s/it, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 381it [20:43,  3.26s/it, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 381it [20:43,  3.26s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 382it [20:43,  3.26s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 382it [20:43,  3.26s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 383it [20:43,  3.25s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 383it [20:43,  3.25s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 384it [20:43,  3.24s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 384it [20:43,  3.24s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 385it [21:23,  3.33s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 385it [21:23,  3.33s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 386it [22:19,  3.47s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 386it [22:19,  3.47s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 387it [22:19,  3.46s/it, loss=4.78, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 387it [22:19,  3.46s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 388it [22:19,  3.45s/it, loss=4.57, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 388it [22:19,  3.45s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 389it [22:19,  3.44s/it, loss=4.58, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 389it [22:19,  3.44s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 390it [22:19,  3.44s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 390it [22:19,  3.44s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 391it [22:19,  3.43s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 391it [22:19,  3.43s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 392it [22:20,  3.42s/it, loss=4.53, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 392it [22:20,  3.42s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 393it [22:20,  3.41s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 393it [22:20,  3.41s/it, loss=4.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 394it [22:57,  3.50s/it, loss=4.47, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 394it [22:57,  3.50s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 395it [22:57,  3.49s/it, loss=4.48, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 395it [22:57,  3.49s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 396it [22:57,  3.48s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 396it [22:57,  3.48s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 397it [22:57,  3.47s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 397it [22:57,  3.47s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 398it [22:57,  3.46s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 398it [22:57,  3.46s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 399it [22:57,  3.45s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 399it [22:57,  3.45s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 400it [22:57,  3.44s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 400it [22:57,  3.44s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 401it [22:58,  3.44s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 401it [22:58,  3.44s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 402it [22:58,  3.43s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 402it [22:58,  3.43s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 403it [22:58,  3.42s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 403it [22:58,  3.42s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 404it [22:58,  3.41s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 404it [22:58,  3.41s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 405it [22:58,  3.40s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 405it [22:58,  3.40s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 406it [22:58,  3.40s/it, loss=4.84, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 406it [22:58,  3.40s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 407it [22:58,  3.39s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 407it [22:58,  3.39s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 408it [22:58,  3.38s/it, loss=4.9, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 408it [22:58,  3.38s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 409it [22:59,  3.37s/it, loss=4.93, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 409it [22:59,  3.37s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 410it [22:59,  3.36s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 410it [22:59,  3.36s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 411it [22:59,  3.36s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 411it [22:59,  3.36s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 412it [22:59,  3.35s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 412it [22:59,  3.35s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 413it [22:59,  3.34s/it, loss=5.12, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 413it [22:59,  3.34s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 414it [22:59,  3.33s/it, loss=5.06, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 414it [22:59,  3.33s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 415it [22:59,  3.32s/it, loss=4.99, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 415it [22:59,  3.32s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 416it [22:59,  3.32s/it, loss=5.07, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 416it [23:00,  3.32s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 417it [23:05,  3.32s/it, loss=4.96, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 417it [23:05,  3.32s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 418it [24:06,  3.46s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 418it [24:06,  3.46s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 419it [24:06,  3.45s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 419it [24:06,  3.45s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 420it [24:06,  3.44s/it, loss=5.04, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 420it [24:06,  3.44s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 421it [24:06,  3.44s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 421it [24:06,  3.44s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 422it [24:06,  3.43s/it, loss=5, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 422it [24:06,  3.43s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 423it [24:07,  3.42s/it, loss=4.88, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 423it [24:07,  3.42s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 424it [24:07,  3.41s/it, loss=4.86, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 424it [24:07,  3.41s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 425it [24:07,  3.41s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 425it [24:07,  3.41s/it, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 426it [24:57,  3.51s/it, loss=4.83, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 426it [24:57,  3.51s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 427it [24:57,  3.51s/it, loss=4.81, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 427it [24:57,  3.51s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 428it [24:57,  3.50s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 428it [24:57,  3.50s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 429it [24:57,  3.49s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 429it [24:57,  3.49s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 430it [24:57,  3.48s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 430it [24:57,  3.48s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 431it [24:57,  3.48s/it, loss=4.67, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 431it [24:57,  3.48s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 432it [24:58,  3.47s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 432it [24:58,  3.47s/it, loss=4.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 433it [24:58,  3.46s/it, loss=4.42, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 433it [24:58,  3.46s/it, loss=4.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 434it [24:58,  3.45s/it, loss=4.39, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 434it [24:58,  3.45s/it, loss=4.37, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 435it [24:58,  3.44s/it, loss=4.37, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 435it [24:58,  3.44s/it, loss=4.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 436it [24:58,  3.44s/it, loss=4.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 436it [24:58,  3.44s/it, loss=4.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 437it [24:58,  3.43s/it, loss=4.25, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 437it [24:58,  3.43s/it, loss=4.27, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 438it [24:58,  3.42s/it, loss=4.27, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 438it [24:58,  3.42s/it, loss=4.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 439it [24:58,  3.41s/it, loss=4.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 439it [24:58,  3.41s/it, loss=4.22, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 440it [24:58,  3.41s/it, loss=4.22, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 440it [24:58,  3.41s/it, loss=4.21, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 441it [24:59,  3.40s/it, loss=4.21, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 441it [24:59,  3.40s/it, loss=4.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 442it [24:59,  3.39s/it, loss=4.1, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 442it [24:59,  3.39s/it, loss=4.26, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 443it [24:59,  3.38s/it, loss=4.26, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 443it [24:59,  3.38s/it, loss=4.22, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 444it [24:59,  3.38s/it, loss=4.22, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 444it [24:59,  3.38s/it, loss=4.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 445it [24:59,  3.37s/it, loss=4.18, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 445it [24:59,  3.37s/it, loss=4.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 446it [24:59,  3.36s/it, loss=4.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 446it [24:59,  3.36s/it, loss=4.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 447it [24:59,  3.36s/it, loss=4.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 447it [24:59,  3.36s/it, loss=4.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 448it [24:59,  3.35s/it, loss=4.03, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 448it [24:59,  3.35s/it, loss=4.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 449it [25:32,  3.41s/it, loss=4.13, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 449it [25:32,  3.41s/it, loss=4.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 450it [26:09,  3.49s/it, loss=4.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 450it [26:09,  3.49s/it, loss=4.24, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 451it [26:10,  3.48s/it, loss=4.24, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 451it [26:10,  3.48s/it, loss=4.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 452it [26:10,  3.47s/it, loss=4.3, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 452it [26:10,  3.47s/it, loss=4.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 453it [26:10,  3.47s/it, loss=4.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 453it [26:10,  3.47s/it, loss=4.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 454it [26:10,  3.46s/it, loss=4.46, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 454it [26:10,  3.46s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 455it [26:19,  3.47s/it, loss=4.54, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 455it [26:19,  3.47s/it, loss=4.56, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 456it [26:19,  3.46s/it, loss=4.56, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 456it [26:19,  3.46s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 457it [26:20,  3.46s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 457it [26:20,  3.46s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 458it [26:57,  3.53s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 458it [26:57,  3.53s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 459it [26:57,  3.52s/it, loss=4.6, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 459it [26:57,  3.52s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 460it [26:57,  3.52s/it, loss=4.62, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 460it [26:57,  3.52s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 461it [26:57,  3.51s/it, loss=4.59, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 461it [26:57,  3.51s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 462it [26:57,  3.50s/it, loss=4.66, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 462it [26:57,  3.50s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 463it [26:57,  3.49s/it, loss=4.64, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 463it [26:57,  3.49s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 464it [26:57,  3.49s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 464it [26:57,  3.49s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 465it [26:58,  3.48s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 465it [26:58,  3.48s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 466it [26:58,  3.47s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 466it [26:58,  3.47s/it, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 467it [26:58,  3.47s/it, loss=4.82, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 467it [26:58,  3.47s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 468it [26:58,  3.46s/it, loss=4.85, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 468it [26:58,  3.46s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 469it [26:58,  3.45s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 469it [26:58,  3.45s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 470it [26:58,  3.44s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 470it [26:58,  3.44s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 471it [26:58,  3.44s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 471it [26:58,  3.44s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 472it [26:58,  3.43s/it, loss=4.73, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 472it [26:58,  3.43s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 473it [26:59,  3.42s/it, loss=4.71, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 473it [26:59,  3.42s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 474it [26:59,  3.42s/it, loss=4.68, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 474it [26:59,  3.42s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 475it [26:59,  3.41s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 475it [26:59,  3.41s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 476it [26:59,  3.40s/it, loss=4.63, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 476it [26:59,  3.40s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 477it [26:59,  3.40s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 477it [26:59,  3.40s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 478it [27:00,  3.39s/it, loss=4.55, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 478it [27:00,  3.39s/it, loss=4.61, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 479it [27:00,  3.38s/it, loss=4.61, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 479it [27:00,  3.38s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 480it [27:15,  3.41s/it, loss=4.7, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 480it [27:15,  3.41s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 481it [27:34,  3.44s/it, loss=4.77, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 481it [27:34,  3.44s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 482it [28:27,  3.54s/it, loss=4.8, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 482it [28:27,  3.54s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 483it [28:27,  3.54s/it, loss=4.76, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 483it [28:27,  3.54s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 484it [28:28,  3.53s/it, loss=4.87, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 484it [28:28,  3.53s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 485it [28:28,  3.52s/it, loss=4.89, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 485it [28:28,  3.52s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 486it [28:28,  3.52s/it, loss=4.92, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 486it [28:28,  3.52s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 487it [28:28,  3.51s/it, loss=4.95, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 487it [28:28,  3.51s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 488it [28:28,  3.50s/it, loss=5.02, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 488it [28:28,  3.50s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 489it [28:28,  3.49s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 489it [28:28,  3.49s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 490it [29:04,  3.56s/it, loss=4.94, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 490it [29:04,  3.56s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 491it [29:04,  3.55s/it, loss=4.97, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 491it [29:04,  3.55s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 492it [29:04,  3.55s/it, loss=4.98, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 492it [29:04,  3.55s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 493it [29:04,  3.54s/it, loss=5.16, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 493it [29:04,  3.54s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 494it [29:04,  3.53s/it, loss=5.17, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 494it [29:04,  3.53s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 495it [29:04,  3.52s/it, loss=5.2, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 495it [29:04,  3.52s/it, loss=5.27, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 496it [29:04,  3.52s/it, loss=5.27, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 496it [29:04,  3.52s/it, loss=5.33, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 497it [29:04,  3.51s/it, loss=5.33, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 497it [29:04,  3.51s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 498it [29:05,  3.50s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 498it [29:05,  3.50s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 499it [29:05,  3.50s/it, loss=5.35, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 499it [29:05,  3.50s/it, loss=5.21, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 500it [29:05,  3.49s/it, loss=5.21, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 500it [29:05,  3.49s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34mValidation: 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 0it [00:00, ?it/s]#033[A\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 1it [00:00, 41.37it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 501it [29:10,  3.49s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 2it [00:00, 49.05it/s]#033[A#015Epoch 2: : 502it [29:10,  3.49s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 3it [00:00, 52.99it/s]#033[A#015Epoch 2: : 503it [29:10,  3.48s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 4it [00:00, 54.96it/s]#033[A#015Epoch 2: : 504it [29:10,  3.47s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 5it [00:00, 53.89it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 505it [29:10,  3.47s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 6it [00:00, 54.70it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 506it [29:10,  3.46s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 7it [00:00, 55.37it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 507it [29:10,  3.45s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 8it [00:00, 56.07it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 508it [29:10,  3.45s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 9it [00:00, 56.78it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 509it [29:10,  3.44s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 10it [00:00, 57.63it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 510it [29:10,  3.43s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 11it [00:00, 55.61it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 511it [29:11,  3.43s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 12it [00:00, 54.96it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 512it [29:11,  3.42s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 13it [00:00, 54.81it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 513it [29:11,  3.41s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 14it [00:00, 55.22it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 514it [29:11,  3.41s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 15it [00:00, 55.83it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 515it [29:11,  3.40s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 16it [00:00, 53.36it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 516it [29:11,  3.39s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 17it [00:00, 52.79it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 517it [29:11,  3.39s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 18it [00:00, 53.66it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 518it [29:11,  3.38s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 19it [00:00, 54.62it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 519it [29:11,  3.37s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 20it [00:00, 54.25it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 520it [29:11,  3.37s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 21it [00:00, 54.38it/s]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 521it [29:11,  3.36s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 22it [00:00, 54.47it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 522it [29:11,  3.35s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 23it [00:00, 55.19it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 523it [29:11,  3.35s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 24it [00:00, 55.86it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 524it [29:11,  3.34s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 25it [00:00, 56.52it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 525it [29:11,  3.34s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 26it [00:00, 57.19it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 526it [29:11,  3.33s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 27it [00:00, 57.81it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 527it [29:11,  3.32s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 28it [00:00, 58.41it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 528it [29:11,  3.32s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 29it [00:00, 59.00it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 529it [29:11,  3.31s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 30it [00:00, 59.57it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 530it [29:11,  3.30s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 31it [00:00, 60.10it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 531it [29:11,  3.30s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mValidation DataLoader 0: : 32it [00:00, 54.78it/s]#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 532it [29:11,  3.29s/it, loss=5.14, v_num=10, val_loss=0.773, train_loss=5.010]\u001B[0m\n",
      "\u001B[34mEpoch 2: : 532it [32:08,  3.63s/it, loss=5.14, v_num=10, val_loss=1.110, train_loss=5.010]\u001B[0m\n",
      "\u001B[34m#033[A\u001B[0m\n",
      "\u001B[34mEpoch 2: : 532it [32:08,  3.63s/it, loss=5.14, v_num=10, val_loss=1.110, train_loss=4.870]\u001B[0m\n",
      "\u001B[34mEpoch 2, global step 1500: 'val_loss' was not in top 1\u001B[0m\n",
      "\u001B[34m`Trainer.fit` stopped: `max_epochs=3` reached.\u001B[0m\n",
      "\u001B[34mEpoch 2: : 532it [35:02,  3.95s/it, loss=5.14, v_num=10, val_loss=1.110, train_loss=4.870]\u001B[0m\n",
      "\u001B[34mINFO:gluonts.torch.model.estimator:Loading best model from /opt/ml/code/lightning_logs/version_10/checkpoints/epoch=1-step=1000.ckpt\u001B[0m\n",
      "\u001B[34m0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m100%|██████████| 1/1 [00:00<00:00, 16.83it/s]\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[test_dataset_stats]: DatasetStatistics(integer_dataset=False, max_target=4863.81005859375, mean_abs_target=1681.1037052654287, mean_target=1681.1037052654287, mean_target_length=1631624.0, max_target_length=1631624, min_target=87.56999969482422, feat_static_real=[], feat_static_cat=[], num_past_feat_dynamic_real=0, num_feat_dynamic_real=3, num_feat_dynamic_cat=0, num_missing_values=0, num_time_observations=1631624, num_time_series=1, scale_histogram=gluonts.dataset.stat.ScaleHistogram(base=2.0, bin_counts=defaultdict(<class 'int'>, {10: 1}), empty_target_count=0))\u001B[0m\n",
      "\u001B[34mRunning evaluation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mINFO:gluonts.model.forecast_generator:Forecast is not sample based. Ignoring parameter `num_samples` from predict method.\u001B[0m\n",
      "\u001B[34mRunning evaluation: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-MSE]: 0.06662052273750305\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-abs_error]: 1.1826171875\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-abs_target_sum]: 8123.56005859375\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-abs_target_mean]: 1624.71201171875\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-seasonal_error]: 58.812378272570065\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-MASE]: 0.004021660821193383\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-MAPE]: 0.00014558049151673912\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-sMAPE]: 0.0001455678604543209\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-MSIS]: 0.08482184134146226\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-QuantileLoss[0.1]]: 0.8765380859375\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-Coverage[0.1]]: 0.0\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-QuantileLoss[0.5]]: 1.1826171875\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-Coverage[0.5]]: 1.0\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-QuantileLoss[0.9]]: 1.8035400390624994\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-Coverage[0.9]]: 1.0\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-RMSE]: 0.2581095169448485\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-NRMSE]: 0.00015886478039378786\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-ND]: 0.00014557868458779142\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-wQuantileLoss[0.1]]: 0.00010790073312872576\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-wQuantileLoss[0.5]]: 0.00014557868458779142\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-wQuantileLoss[0.9]]: 0.00022201350467700067\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-mean_absolute_QuantileLoss]: 1.2875651041666665\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-mean_wQuantileLoss]: 0.00015849764079783928\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-MAE_Coverage]: 0.5\u001B[0m\n",
      "\u001B[34mINFO:gluonts.evaluation.backtest:gluonts[metric-OWA]: nan\u001B[0m\n",
      "\u001B[34mAggregated performance\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"MSE\": 0.06662052273750305,\n",
      "    \"abs_error\": 1.1826171875,\n",
      "    \"abs_target_sum\": 8123.56005859375,\n",
      "    \"abs_target_mean\": 1624.71201171875,\n",
      "    \"seasonal_error\": 58.812378272570065,\n",
      "    \"MASE\": 0.004021660821193383,\n",
      "    \"MAPE\": 0.00014558049151673912,\n",
      "    \"sMAPE\": 0.0001455678604543209,\n",
      "    \"MSIS\": 0.08482184134146226,\n",
      "    \"QuantileLoss[0.1]\": 0.8765380859375,\n",
      "    \"Coverage[0.1]\": 0.0,\n",
      "    \"QuantileLoss[0.5]\": 1.1826171875,\n",
      "    \"Coverage[0.5]\": 1.0,\n",
      "    \"QuantileLoss[0.9]\": 1.8035400390624994,\n",
      "    \"Coverage[0.9]\": 1.0,\n",
      "    \"RMSE\": 0.2581095169448485,\n",
      "    \"NRMSE\": 0.00015886478039378786,\n",
      "    \"ND\": 0.00014557868458779142,\n",
      "    \"wQuantileLoss[0.1]\": 0.00010790073312872576,\n",
      "    \"wQuantileLoss[0.5]\": 0.00014557868458779142,\n",
      "    \"wQuantileLoss[0.9]\": 0.00022201350467700067,\n",
      "    \"mean_absolute_QuantileLoss\": 1.2875651041666665,\n",
      "    \"mean_wQuantileLoss\": 0.00015849764079783928,\n",
      "    \"MAE_Coverage\": 0.5,\n",
      "    \"OWA\": NaN\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mCompleted training\u001B[0m\n",
      "\u001B[34m2023-03-05 21:20:45,606 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2023-03-05 21:20:45,607 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2023-03-05 21:20:45,607 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001B[0m\n",
      "\n",
      "2023-03-05 21:21:07 Uploading - Uploading generated training model\n",
      "2023-03-05 21:21:07 Completed - Training job completed\n",
      "Training seconds: 6962\n",
      "Billable seconds: 2089\n",
      "Managed Spot Training savings: 70.0%\n",
      "2023-03-05 16:21:23.551645 Training job name: pytorch-training-2023-03-05-19-21-31-227\n",
      "2023-03-05 16:21:23.621907 Model is saved in: /giia-tft_torch-1.1.2/models/pytorch-training-2023-03-05-19-21-31-227/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Configure sagemaker and estimator, then train\n",
    "#\n",
    "\n",
    "# from ml.train import Train\n",
    "# TRAIN = Train(LOGGER)\n",
    "\n",
    "from ml.train_torch import TrainTorch\n",
    "TRAIN = TrainTorch(LOGGER)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    train_kwargs = {}\n",
    "else:\n",
    "    train_kwargs = {\n",
    "        # 'checkpoint_s3_uri': model_output_dir_uri,\n",
    "        'output_path': model_output_dir_uri,\n",
    "        'code_location': model_output_dir_uri,\n",
    "        'use_spot_instances': True,\n",
    "        'max_wait': 18 * 60 * 60, # 18 hours\n",
    "        'max_run': 18 * 60 * 60, # 18 hours\n",
    "    }\n",
    "\n",
    "estimator = TRAIN.create_model(config.SM_ROLE, INSTANCE_TYPE, sagemaker_session, train_kwargs)\n",
    "TRAIN.fit_model(estimator, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load model\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from gluonts.model.predictor import Predictor\n",
    "\n",
    "# Uncomment if you want to quickly compare AWS model with local model\n",
    "IS_LOCAL = False\n",
    "\n",
    "if IS_LOCAL:\n",
    "    # model_output_dir_path is basically the same path as it was before, though sagemaker appends a random temp\n",
    "    # directory to the path. The path from TRAIN includes that random temp directory\n",
    "    model_dir_path = TRAIN.model_data_path.parent.parent / \"model\"\n",
    "    # model_dir_path = local_artifact_dir / \"local_cli\" / \"model\"\n",
    "else:\n",
    "    model_dir_path = AWS_HANDLER.download_model_from_s3(str(TRAIN.model_data_path), local_artifact_dir)\n",
    "\n",
    "LOGGER.log(f\"Model dir is [{model_dir_path}]\")\n",
    "predictor = Predictor.deserialize(model_dir_path)\n",
    "LOGGER.log(f\"Predictor metadata [{predictor.__dict__}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define test data\n",
    "#\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.common import load_datasets\n",
    "from gluonts.dataset.stat import calculate_dataset_statistics\n",
    "from gluonts.dataset.split import split\n",
    "\n",
    "import data_processing.gluonts_helper as gh\n",
    "\n",
    "test_dates = [\"2021-05-27 12:50:00\", \"2021-05-27 15:55:00\", \"2021-05-28 16:00:00\", \"2021-05-28 17:00:00\"]\n",
    "# test_dates = [\"2021-05-22 17:00:00\"]\n",
    "test_datasets = []\n",
    "\n",
    "if FILEDATASET_BASED:\n",
    "    datasets = load_datasets(\n",
    "        metadata=(dataset_dir_path / config.METADATA_DATASET_FILENAME).parent,\n",
    "        train=(dataset_dir_path / config.TRAIN_DATASET_FILENAME).parent,\n",
    "        test=(dataset_dir_path / config.TEST_DATASET_FILENAME).parent,\n",
    "        one_dim_target=ONE_DIM_TARGET,\n",
    "        cache=True\n",
    "    )\n",
    "\n",
    "    train_stats = calculate_dataset_statistics(datasets.train)\n",
    "    feature_columns = train_stats.feat_static_real\n",
    "\n",
    "    for idx, date in enumerate(test_dates):\n",
    "        train_dataset, test_template = split(datasets.test, date=pd.Period(date, freq='T'))\n",
    "        test_dataset = test_template.generate_instances(\n",
    "            prediction_length=config.HYPER_PARAMETERS[\"prediction_length\"],\n",
    "            max_history=config.FREQTRADE_MAX_CONTEXT,\n",
    "        )\n",
    "        test_datasets.append(train_dataset)\n",
    "else:\n",
    "    test_dataset_filename = dataset_dir_path / config.TEST_CSV_FILENAME\n",
    "    test_df = pd.read_csv(filepath_or_buffer=test_dataset_filename, header=0, index_col=0)\n",
    "\n",
    "    feature_columns = gh.get_feature_columns(test_df, exclude_close=False)\n",
    "\n",
    "    for idx, date in enumerate(test_dates):\n",
    "        split_df = test_df[:date].tail(config.FREQTRADE_MAX_CONTEXT)\n",
    "        test_dataset = gh.df_to_multivariate_target_dataset(split_df, feature_columns)\n",
    "\n",
    "        LOGGER.log(f\"Test dataset [{idx}] stats: {calculate_dataset_statistics(test_dataset)}\")\n",
    "        test_datasets.append(test_dataset)\n",
    "\n",
    "print(f\"feature_columns are [{feature_columns}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate and visualize the prediction\n",
    "#\n",
    "import json\n",
    "\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "\n",
    "def plot_prob_forecasts(ts_list, forecast_list, plot_length=100):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "        ax = target[-plot_length:].plot(figsize=(10, 7), linewidth=2)\n",
    "        forecast.plot(keys=['0.5', '0.9'])\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "def plot_prob_forecasts_multi(ts_list, forecast_list, close_index, plot_length=60):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        quantiles = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in quantiles][::-1]\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "        target[close_index][-plot_length:].plot(ax=ax)  # plot the time series\n",
    "        forecast.copy_dim(close_index).plot(quantiles=quantiles, color='g')\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "for test_dataset in test_datasets:\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_dataset,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    forecast_entry = forecasts[0]\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    # LOGGER.log(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "    # LOGGER.log(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "    # LOGGER.log(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "    # LOGGER.log(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "\n",
    "    if ONE_DIM_TARGET:\n",
    "        evaluator = Evaluator(quantiles=[0.1])\n",
    "    else:\n",
    "        evaluator = MultivariateEvaluator(quantiles=[0.1])\n",
    "\n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_dataset))\n",
    "\n",
    "    for key in list(agg_metrics.keys()):\n",
    "        if key[0].isdigit():\n",
    "            del agg_metrics[key]\n",
    "    LOGGER.log(\"Aggregated performance\")\n",
    "    LOGGER.log(json.dumps(agg_metrics, indent=4))\n",
    "\n",
    "    if ONE_DIM_TARGET:\n",
    "        plot_prob_forecasts(tss, forecasts)\n",
    "    else:\n",
    "        close_index = feature_columns.index(\"close\")\n",
    "        # close_index = feature_columns.index(\"log_return_close\")\n",
    "        LOGGER.log(\"'close' performance\")\n",
    "        LOGGER.log(item_metrics.iloc[close_index])\n",
    "\n",
    "        plot_prob_forecasts_multi(tss, forecasts, close_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "import numpy as np\n",
    "\n",
    "def count_model_params(net: nn.HybridBlock) -> int:\n",
    "    params = net.collect_params()\n",
    "    num_params = 0\n",
    "    for p in params:\n",
    "        v = params[p]\n",
    "        num_params += np.prod(v.shape)\n",
    "    return num_params\n",
    "\n",
    "net_name = type(predictor.prediction_net).__name__\n",
    "num_model_param = count_model_params(predictor.prediction_net)\n",
    "print(f\"Number of parameters in {net_name}: {num_model_param}\")\n",
    "\n",
    "def plot_prob_forecasts_multi2(ts_list, forecast_list, close_index, plot_length=60):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        print(forecast)\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "        target[close_index][-plot_length:].plot(ax=ax)  # plot the time series\n",
    "        forecast.copy_dim(close_index).plot(prediction_intervals=prediction_intervals, color='g')\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "# close_index = feature_columns.index(\"close\")\n",
    "# LOGGER.log(\"'close' performance\")\n",
    "# LOGGER.log(item_metrics.iloc[close_index])\n",
    "#\n",
    "# plot_prob_forecasts_multi2(tss, forecasts, close_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: FURTHER CELLS ARE COMPATIBLE WITH AWS SAGEMAKER ONLY, LOCAL MODE WILL NOT WORK\n",
    "# Hyperparameter tune the model\n",
    "#\n",
    "\n",
    "from ml.tune import Tune\n",
    "\n",
    "TUNE = Tune(UTILS, LOGGER)\n",
    "\n",
    "train_dataset_uri = f\"{dataset_dir_uri}/{config.TRAIN_DATASET_FILENAME}\"\n",
    "test_dataset_uri = f\"{dataset_dir_uri}/{config.TEST_DATASET_FILENAME}\"\n",
    "\n",
    "# Note: Feel free to tune the tuner, i.e. update max number of jobs and hyperparameters. Default is 10 jobs, but you\n",
    "# may want to change this as you refine the model. Additionally, if you find the best model has a parameter at the\n",
    "# end of the range you gave it, then you should look to move that range to determine if the model performs better\n",
    "# along that vector\n",
    "tuner = TUNE.create_tuner(estimator)\n",
    "TUNE.fit_tuner(tuner, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get updates for Hyperparameter tune job. Ensure this is completed before going to the next cell\n",
    "#\n",
    "\n",
    "TUNE.get_tune_job_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the metrics of the tune job\n",
    "#\n",
    "\n",
    "TUNE.report_job_analytics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giia (venv)",
   "language": "python",
   "name": "giia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
