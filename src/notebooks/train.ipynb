{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory [/Users/jbeckman/projects/capia/src]\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "MODULE_PATH = ipynbname.path().parent.parent\n",
    "sys.path.append(str(MODULE_PATH))\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# This should always be `./src`\n",
    "print(f\"Current working directory [{os.getcwd()}]\")\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-16 07:16:46.332314 The model id is [giia-0.5.8]\n",
      "2021-04-16 07:16:46.333394 The MXNet version is [1.7.0]\n",
      "2021-04-16 07:16:46.333779 The GPU count is [0]\n",
      "2021-04-16 07:16:46.342938 The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU computation\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup utils\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "from utils import config\n",
    "\n",
    "LOGGER = LoggerUtil(config.MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()\n",
    "\n",
    "# AWS instance specs can be found here https://aws.amazon.com/sagemaker/pricing/\n",
    "AWS_INSTANCE = 'ml.m5.large' # 2 vCPU, 0 GPU, 8 GB memory, $0.134/hour\n",
    "AWS_INSTANCE_2 = 'ml.m5.4xlarge' # 8 vCPU, 0 GPU, 32 GB memory, $0.538/hour\n",
    "AWS_GPU_INSTANCE = 'ml.g4dn.xlarge' # 4 vCPU, 1 GPU, 16 GB memory, $0.736/hour\n",
    "AWS_GPU_INSTANCE_2 = 'ml.g4dn.2xlarge' # 8 vCPU, 1 GPU, 32 GB memory, $1.053/hour\n",
    "LOCAL_INSTANCE = 'local'\n",
    "try:\n",
    "    if subprocess.call('nvidia-smi') == 0:\n",
    "        LOCAL_INSTANCE = 'local_gpu'\n",
    "except:\n",
    "    LOGGER.log(\"The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU \"\n",
    "               \"computation\")\n",
    "\n",
    "# Change this to your desired instance type\n",
    "INSTANCE_TYPE = LOCAL_INSTANCE\n",
    "IS_LOCAL = LOCAL_INSTANCE == INSTANCE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-16 07:16:57.810654 First sample:\n",
      "2021-04-16 07:16:57.813693 \n",
      "                        open     high      low    close    volume\n",
      "date                                                             \n",
      "2017-08-17 04:00:00  4261.48  4280.56  4261.48  4261.48  2.189061\n",
      "2021-04-16 07:16:57.814085 Last sample:\n",
      "2021-04-16 07:16:57.816821 \n",
      "                         open      high       low     close     volume\n",
      "date                                                                  \n",
      "2021-03-26 01:25:00  52151.39  52178.81  52108.76  52122.84  93.536227\n",
      "2021-04-16 07:16:58.134366 Parsed train and test datasets can be found in [/Users/jbeckman/projects/capia/out/datasets]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Parse dataset\n",
    "#\n",
    "\n",
    "from data_processing.parse import Parse\n",
    "\n",
    "PARSE = Parse(LOGGER)\n",
    "\n",
    "dataset_dir_path = local_artifact_dir / \"datasets\"\n",
    "\n",
    "# Creates train and test dataset CSVs\n",
    "PARSE.split_train_test_dataset(dataset_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup local/aws environment. If aws, upload the datasets to S3\n",
    "#\n",
    "\n",
    "from data_processing.aws_handler import AWSHandler\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "AWS_HANDLER = AWSHandler(LOGGER, config.MODEL_ID)\n",
    "\n",
    "sagemaker_session = None\n",
    "\n",
    "model_output_dir_path = local_artifact_dir / config.MODEL_ID / \"models\"\n",
    "model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    LOGGER.log(\"Notebook is set to local mode, not uploading to S3\")\n",
    "\n",
    "    dataset_dir_uri = f\"file://{dataset_dir_path}\"\n",
    "    model_output_dir_uri = f\"file://{model_output_dir_path}\"\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {\n",
    "        'local': {\n",
    "            'local_code': True,\n",
    "            'container_root': str(model_output_dir_path)\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    sagemaker_session = AWS_HANDLER.sagemaker_session\n",
    "\n",
    "    AWS_HANDLER.upload_train_datasets(dataset_dir_path)\n",
    "    dataset_dir_uri = AWS_HANDLER.s3_dataset_dir_uri\n",
    "\n",
    "    model_output_dir_uri = AWS_HANDLER.s3_model_output_uri\n",
    "\n",
    "LOGGER.log(f\"Model output dir is [{model_output_dir_uri}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Configure sagemaker and estimator\n",
    "#\n",
    "\n",
    "from ml.train import Train\n",
    "\n",
    "TRAIN = Train(LOGGER)\n",
    "\n",
    "if IS_LOCAL:\n",
    "    train_kwargs = {}\n",
    "else:\n",
    "    train_kwargs = {\n",
    "        # 'checkpoint_s3_uri': model_output_dir_uri,\n",
    "        'output_path': model_output_dir_uri,\n",
    "        'code_location': model_output_dir_uri,\n",
    "        'use_spot_instances': True,\n",
    "        'max_wait': 18 * 60 * 60, # 18 hours\n",
    "        'max_run': 18 * 60 * 60, # 18 hours\n",
    "    }\n",
    "\n",
    "estimator = TRAIN.create_model(config.SM_ROLE, INSTANCE_TYPE, sagemaker_session, train_kwargs)\n",
    "TRAIN.fit_model(estimator, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-16 07:17:15.143892 Model dir is [/Users/jbeckman/projects/capia/out/local_cli/model]\n",
      "2021-04-16 07:17:15.412521 Predictor metadata [{'prediction_length': 12, 'freq': '5min', 'lead_time': 0, 'input_names': ['past_target', 'past_observed_values'], 'prediction_net': gluonts.model.lstnet._network.LSTNetPredict(ar_window=18, channels=90, context_length=24, dropout_rate=0.2, dtype=numpy.float32, kernel_size=6, lead_time=0, num_series=5, output_activation=None, prediction_length=12, rnn_cell_type=\"gru\", rnn_num_cells=100, rnn_num_layers=90, scaling=True, skip_rnn_cell_type=\"gru\", skip_rnn_num_cells=10, skip_rnn_num_layers=9, skip_size=9), 'batch_size': 16, 'input_transform': gluonts.transform._base.Chain(trans=[gluonts.transform._base.Chain(trans=[gluonts.transform.convert.AsNumpyArray(dtype=numpy.float32, expected_ndim=2, field=\"target\"), gluonts.transform.feature.AddObservedValuesIndicator(dtype=numpy.float32, imputation_method=gluonts.transform.feature.DummyValueImputation(dummy_value=0.0), output_field=\"observed_values\", target_field=\"target\")]), gluonts.transform.split.InstanceSplitter(dummy_value=0.0, forecast_start_field=\"forecast_start\", future_length=12, instance_sampler=gluonts.transform.sampler.PredictionSplitSampler(axis=-1, min_past=0, min_future=0, allow_empty_interval=False), is_pad_field=\"is_pad\", lead_time=0, output_NTC=False, past_length=24, start_field=\"start\", target_field=\"target\", time_series_fields=[\"observed_values\"])]), 'forecast_generator': gluonts.model.forecast_generator.SampleForecastGenerator(), 'output_transform': None, 'ctx': cpu(0), 'dtype': <class 'numpy.float32'>}]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load model\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from gluonts.model.predictor import Predictor\n",
    "\n",
    "if IS_LOCAL:\n",
    "    # model_output_dir_path is basically the same path as it was before, though sagemaker appends a random temp\n",
    "    # directory to the path. The path from TRAIN includes that random temp directory\n",
    "    # model_dir_path = TRAIN.model_data_path.parent.parent / \"model\"\n",
    "    model_dir_path = local_artifact_dir / \"local_cli\" / \"model\"\n",
    "else:\n",
    "    model_dir_path = AWS_HANDLER.download_model_from_s3(str(TRAIN.model_data_path), local_artifact_dir)\n",
    "\n",
    "LOGGER.log(f\"Model dir is [{model_dir_path}]\")\n",
    "predictor = Predictor.deserialize(model_dir_path)\n",
    "LOGGER.log(f\"Predictor metadata [{predictor.__dict__}]\")\n",
    "\n",
    "\n",
    "def plot_prob_forecasts(ts_list, forecast_list, plot_length=100):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        prediction_intervals = (50.0, 90.0)\n",
    "        legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "        ax = target[-plot_length:].plot(figsize=(10, 7), linewidth=2)\n",
    "        forecast.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "        plt.grid(which=\"both\")\n",
    "        plt.legend(legend, loc=\"upper left\")\n",
    "        plt.show()\n",
    "    \n",
    "def plot_prob_forecasts_multi(ts_list, forecast_list, plot_length=100):\n",
    "    for target, forecast in islice(zip(ts_list, forecast_list), len(forecast_list)):\n",
    "        for i in range(5):\n",
    "            prediction_intervals = (50.0, 90.0)\n",
    "            legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "            target[i][-plot_length:].plot(ax=ax)  # plot the time series\n",
    "            forecast.copy_dim(i).plot(prediction_intervals=prediction_intervals, color='g')\n",
    "            plt.grid(which=\"both\")\n",
    "            plt.legend(legend, loc=\"upper left\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-e141a4ffe086>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;31m# max_history=config.HYPER_PARAMETERS[\"past_length\"] + config.HYPER_PARAMETERS[\"prediction_length\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     )\n\u001B[0;32m---> 28\u001B[0;31m     \u001B[0;34m(\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_dataset\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplitter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_file_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;31m# # 2) Remove other time-series as we only want to predict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capia/venv/lib/python3.8/site-packages/gluonts/dataset/split/splitter.py\u001B[0m in \u001B[0;36msplit\u001B[0;34m(self, items)\u001B[0m\n\u001B[1;32m    217\u001B[0m         \u001B[0msplit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainTestSplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 219\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTimeSeriesSlice\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_data_entry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitems\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    220\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m             \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_train_slice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capia/venv/lib/python3.8/site-packages/gluonts/dataset/split/splitter.py\u001B[0m in \u001B[0;36mfrom_data_entry\u001B[0;34m(cls, item, freq)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m         return TimeSeriesSlice(\n\u001B[0;32m---> 97\u001B[0;31m             \u001B[0mtarget\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"target\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     98\u001B[0m             \u001B[0mitem\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mFieldName\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mITEM_ID\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0mfeat_static_cat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfeat_static_cat\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capia/venv/lib/python3.8/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    325\u001B[0m                     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 327\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msanitize_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraise_cast_failure\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSingleBlockManager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capia/venv/lib/python3.8/site-packages/pandas/core/construction.py\u001B[0m in \u001B[0;36msanitize_array\u001B[0;34m(data, index, dtype, copy, raise_cast_failure)\u001B[0m\n\u001B[1;32m    494\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0msubarr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    495\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 496\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Data must be 1-dimensional\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    497\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    498\u001B[0m             \u001B[0msubarr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray_tuplesafe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mException\u001B[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define test data and make a prediction\n",
    "#\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.common import ListDataset, FileDataset\n",
    "from utils.splitter import DateSplitter\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.stat import calculate_dataset_statistics\n",
    "\n",
    "# test_file_dataset = FileDataset(path=(dataset_dir_path / config.TEST_DATASET_FILENAME).parent, freq=\"min\")\n",
    "test_file_dataset = FileDataset(path=(dataset_dir_path / config.TEST_DATASET_FILENAME).parent, freq=\"5min\")\n",
    "\n",
    "# pd.Timedelta(config.HYPER_PARAMETERS[\"prediction_length\"], unit=\"5min\")\n",
    "# for data in iter(test_file_dataset):\n",
    "#     data.start.freqstr = \"min\"\n",
    "\n",
    "test_datasets = []\n",
    "test_dates = [\"2020-11-20 12:50:00\", \"2021-01-20 15:55:00\", \"2021-01-20 17:10:00\"]\n",
    "for idx, date in enumerate(test_dates):\n",
    "    # 1) Get splice of dataset for different dates with ample history\n",
    "    splitter = DateSplitter(\n",
    "        prediction_length=-config.HYPER_PARAMETERS[\"prediction_length\"],\n",
    "        split_date=date,\n",
    "        max_history=config.HYPER_PARAMETERS[\"past_length\"]\n",
    "        # max_history=config.HYPER_PARAMETERS[\"past_length\"] + config.HYPER_PARAMETERS[\"prediction_length\"]\n",
    "    )\n",
    "    (_, train_dataset), (_, test_dataset) = splitter.split(test_file_dataset)\n",
    "\n",
    "    # # 2) Remove other time-series as we only want to predict\n",
    "    # for data in iter(test_dataset):\n",
    "    #     if data['item_id'] == \"close\":\n",
    "    #         test_dataset = ListDataset([{\n",
    "    #             FieldName.START: data[FieldName.START],\n",
    "    #             FieldName.TARGET: data[FieldName.TARGET],\n",
    "    #             FieldName.FEAT_STATIC_CAT: data[FieldName.FEAT_STATIC_CAT],\n",
    "    #             FieldName.ITEM_ID: data[FieldName.ITEM_ID],\n",
    "    #         }], freq=config.DATASET_FREQ)\n",
    "    #         break\n",
    "\n",
    "    LOGGER.log(f\"Test dataset [{idx}] stats: {calculate_dataset_statistics(test_dataset)}\")\n",
    "    test_datasets.append(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate and visualize the prediction\n",
    "#\n",
    "import json\n",
    "\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "\n",
    "for test_dataset in test_datasets:\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_dataset,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    forecast_entry = forecasts[0]\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    # LOGGER.log(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "    # LOGGER.log(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "    # LOGGER.log(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "    # LOGGER.log(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "\n",
    "    evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_dataset))\n",
    "\n",
    "    LOGGER.log(json.dumps(agg_metrics, indent=4))\n",
    "    item_metrics.head()\n",
    "\n",
    "    plot_prob_forecasts(tss, forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: FURTHER CELLS ARE COMPATIBLE WITH AWS SAGEMAKER ONLY, LOCAL MODE WILL NOT WORK\n",
    "# Hyperparameter tune the model\n",
    "#\n",
    "\n",
    "from ml.tune import Tune\n",
    "\n",
    "TUNE = Tune(UTILS, LOGGER)\n",
    "\n",
    "train_dataset_uri = f\"{dataset_dir_uri}/{config.TRAIN_DATASET_FILENAME}\"\n",
    "test_dataset_uri = f\"{dataset_dir_uri}/{config.TEST_DATASET_FILENAME}\"\n",
    "\n",
    "# Note: Feel free to tune the tuner, i.e. update max number of jobs and hyperparameters. Default is 10 jobs, but you\n",
    "# may want to change this as you refine the model. Additionally, if you find the best model has a parameter at the\n",
    "# end of the range you gave it, then you should look to move that range to determine if the model performs better\n",
    "# along that vector\n",
    "tuner = TUNE.create_tuner(estimator)\n",
    "TUNE.fit_tuner(tuner, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get updates for Hyperparameter tune job. Ensure this is completed before going to the next cell\n",
    "#\n",
    "\n",
    "TUNE.get_tune_job_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the metrics of the tune job\n",
    "#\n",
    "\n",
    "TUNE.report_job_analytics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capia (venv)",
   "language": "python",
   "name": "capia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}