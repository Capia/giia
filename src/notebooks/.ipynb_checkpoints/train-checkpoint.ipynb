{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giia-0.3.3\n",
      "The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU computation\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Setup constants\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "MODEL_NAME = \"giia\"\n",
    "MODEL_VERSION = \"0.3.3\"\n",
    "MODEL_ID = f\"{MODEL_NAME}-{MODEL_VERSION}\"\n",
    "print(MODEL_ID)\n",
    "\n",
    "DATASET = \"../freqtrade/user_data/data/binance/ETH_BTC-5m.json\"\n",
    "SRC_DATASET_DIR = \"../freqtrade/user_data/data/binance\"\n",
    "SM_ROLE ='arn:aws:iam::941048668662:role/service-role/AmazonSageMaker-ExecutionRole-20191206T145896'\n",
    "\n",
    "AWS_INSTANCE = 'ml.m5.large'\n",
    "LOCAL_INSTANCE = 'local'\n",
    "try:\n",
    "    if subprocess.call('nvidia-smi') == 0:\n",
    "        LOCAL_INSTANCE = 'local_gpu'\n",
    "except:\n",
    "    print(\"The nvidia-smi binary was not found and thus GPU computation is not supported. Using the default CPU \"\n",
    "          \"computation\")\n",
    "\n",
    "# Change this to your desired instance type\n",
    "INSTANCE_TYPE = LOCAL_INSTANCE\n",
    "IS_LOCAL = LOCAL_INSTANCE==INSTANCE_TYPE\n",
    "\n",
    "MODULE_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%cache magic is now registered in ipython\n",
      "loading cached value for variable 'MODULE_PATH'. Time since pickling  51 days, 9:10:31.054788\n",
      "2021-03-14 21:21:36.548124 Background logger started\n",
      "2021-03-14 21:21:36.548711 Current working directory [/Users/jbeckman/projects/capia/src]\n",
      "2021-03-14 21:21:36.548886 The MXNet version is [1.6.0]\n",
      "2021-03-14 21:21:36.549064 The GPU count is [0]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialization\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cache_magic\n",
    "from pathlib import Path\n",
    "\n",
    "# Set notebook's src module path. Note that you may have to update your IDE's project settings to do the same for the\n",
    "#  local library imports to work the same\n",
    "%cache MODULE_PATH = os.path.dirname(Path().resolve())\n",
    "sys.path.append(MODULE_PATH)\n",
    "\n",
    "# Keep paths consistent throughout notebook\n",
    "os.chdir(MODULE_PATH)\n",
    "\n",
    "# Place all local artifacts in a disposable, git-ignored directory\n",
    "local_artifact_dir = Path(os.getcwd()).parent / \"out\"\n",
    "local_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Autoreload imports at the beginning of cell execution.\n",
    "#  https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.logger_util import LoggerUtil\n",
    "from utils.utils import Utils\n",
    "\n",
    "LOGGER = LoggerUtil(MODEL_ID, local_artifact_dir / \"logs\")\n",
    "UTILS = Utils(LOGGER)\n",
    "\n",
    "UTILS.describe_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-14 21:30:26.320264 First sample:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c70412c2bc55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Creates train and test dataset CSVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mPARSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_train_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_DATASET_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/capia/src/data_processing/parse.py\u001b[0m in \u001b[0;36msplit_train_test_dataset\u001b[0;34m(self, src_dataset_dir_path, dataset_dir_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m                              'downloaded to the configured location')\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First sample:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLast sample:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/capia/src/utils/logger_util.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, message, log_level, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Outputs to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_level\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'info'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlog_level\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'warning'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Parse dataset\n",
    "#\n",
    "\n",
    "from data_processing.parse import Parse\n",
    "\n",
    "PARSE = Parse(LOGGER)\n",
    "\n",
    "dataset_dir_path = local_artifact_dir / \"datasets\"\n",
    "\n",
    "# Creates train and test dataset CSVs\n",
    "PARSE.split_train_test_dataset(Path(SRC_DATASET_DIR), dataset_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup local/aws environment. If aws, upload the datasets to S3\n",
    "#\n",
    "\n",
    "from data_processing.upload import Upload\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "UPLOAD = Upload(LOGGER, MODEL_ID)\n",
    "\n",
    "sagemaker_session = None\n",
    "\n",
    "if IS_LOCAL:\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "    LOGGER.log(\"Notebook is set to local mode, not uploading to S3\")\n",
    "    dataset_dir_uri = f\"file://{dataset_dir_path}\"\n",
    "\n",
    "    model_output_dir_path = local_artifact_dir / \"models\"\n",
    "    model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_output_dir_uri = f\"file://{model_output_dir_path}\"\n",
    "else:\n",
    "    sagemaker_session = UPLOAD.sagemaker_session\n",
    "\n",
    "    UPLOAD.upload_to_sagemaker_s3_bucket(dataset_dir_path, PARSE.TRAIN_DATASET_FILENAME)\n",
    "    UPLOAD.upload_to_sagemaker_s3_bucket(dataset_dir_path, PARSE.TEST_DATASET_FILENAME)\n",
    "    dataset_dir_uri = UPLOAD.s3_dataset_dir_uri\n",
    "\n",
    "    model_output_dir_uri = UPLOAD.s3_model_output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Configure sagemaker and estimator\n",
    "#\n",
    "\n",
    "from ml.train import Train\n",
    "\n",
    "TRAIN = Train(LOGGER)\n",
    "\n",
    "estimator = TRAIN.create_model(SM_ROLE, INSTANCE_TYPE, model_output_dir_uri, sagemaker_session)\n",
    "TRAIN.fit_model(estimator, dataset_dir_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Graph some prediction test results\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gluonts.model.predictor import Predictor\n",
    "\n",
    "#TODO: Should really use deepar.model_fn\n",
    "model_path = model_output_dir_path\n",
    "print(\"Adsf\")\n",
    "print(model_path)\n",
    "print(\"qwer\")\n",
    "predictor = Predictor.deserialize(Path(model_output_dir_uri))\n",
    "\n",
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "    plot_length = 150\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_prob_forecasts(tss[0], forecasts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: FURTHER CELLS ARE COMPATIBLE WITH AWS SAGEMAKER ONLY, LOCAL MODE WILL NOT WORK\n",
    "# Hyperparameter tune the model\n",
    "#\n",
    "\n",
    "from ml.tune import Tune\n",
    "\n",
    "TUNE = Tune(LOGGER)\n",
    "\n",
    "train_dataset_uri = f\"{dataset_dir_uri}/{PARSE.TRAIN_DATASET_FILENAME}\"\n",
    "test_dataset_uri = f\"{dataset_dir_uri}/{PARSE.TEST_DATASET_FILENAME}\"\n",
    "\n",
    "tuner = TUNE.create_tuner(estimator)\n",
    "TUNE.fit_tuner(tuner, train_dataset_uri, test_dataset_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get updates for Hyperparameter tune job. Ensure this is completed before going to the next cell\n",
    "#\n",
    "\n",
    "TUNE.get_tune_job_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the metrics of the tune job\n",
    "#\n",
    "\n",
    "TUNE.report_job_analytics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
